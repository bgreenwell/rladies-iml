<<parent-template, include=FALSE>>=
set_parent("book.Rnw")
@

<<interpret-setup, include=FALSE, cache=TRUE>>=
# Load required packages
library(ggplot2)
library(randomForest)

# Set the plotting theme
theme_set(theme_bw())

# Load the Ames housing data
ames <- as.data.frame(AmesHousing::make_ames())
ames$Sale_Price <- ames$Sale_Price / 1000  # rescale response
set.seed(4919)
id <- sample.int(nrow(ames), size = floor(0.7 * nrow(ames)))
ames.trn <- ames[id, ]
ames.tst <- ames[-id, ]

# Reproduce bagged tree ensemble from previous chapter
ames.bag <- readRDS("../data/chap-ensembles-ames-bag.rds")
@

% Peter Buehlmann and Bin Yu (2002), Analyzing Bagging. The Annals of Statistics 30(4), 927â€“961.

% It's worth noting a few pitfalls of ML interpretability.
% 
% \FIXME{Reference \url{https://arxiv.org/pdf/2007.04131.pdf}}.
%  % * Need reference for \pkg{PDPbox} package in \Python{}.

\chapter{Peeking inside the "black box"\label{chap:peeking}}

\FIXME{Missing a quote!}

% One of the nice aspects of single trees (and rule sets) is that they can be easy to interpret---I stress the word "can" because complex trees (e.g., trees with many splits or trees that fit complex models in the terminal nodes) quickly lose that simplicity. This is especially true when we start combining several trees into an ensemble. While ensembles of trees may still retain many of the benefits of a single tree (e.g., robustness to outliers in the features and variable selection), there are many aspects that are hard to interpret. For example, which features and feature values had the most impact on a single prediction? How does a particular feature, or subset thereof, functionally relate to the predicted outcome of interest? These questions, of course, are important to ask for non-tree-based \FIXME{Spelling?} models as well. Fortunately, the techniques discussed in this chapter apply to any supervised learning model.
% 
% Talk about other ways trees can be useful.
% 
% Fantastic resource for IML: \url{https://github.com/jphall663/awesome-machine-learning-interpretability}. Includes \R{}/\Python{} resources, as well as papers, (free) books---including Molnar book and H2O book.
% 
% \FIXME{Discuss identifying interaction effects (e.g., $H$-statistic, PDP variance, clustered ICE plots, etc. Incorporate into examples.)}
% 
% Complex nonparametric models---like neural networks, support vector machines, and tree-based ensembles---are more common than ever in predictive analytics, especially when dealing with large observational databases that don't adhere to the strict assumptions imposed by traditional statistical techniques (e.g., multiple linear regression which assumes linearity, homoscedasticity, and normality). Unfortunately, it can be challenging to understand the results of such models and explain them to management. 
% 
% Too often machine learning (ML) models are summarized using a single metric (e.g., cross-validated accuracy) and then put into production. Although we often care about the predictions from these models, it is becoming routine (and good practice) to also better understand the predictions! Understanding how an ML model makes its predictions helps build trust in the model and is the fundamental idea of the emerging
% field of \emph{interpretable machine learning} (IML) \index{Interpretable machine learning}.\footnote{Although ``interpretability'' is difficult to formally define in the context of ML, we follow \citet{doshivelez-2017-rigorous} and describe ``interpretable'' as the ``\ldots{}ability to explain or to present in understandable terms to a human.''} For an in-depth discussion on IML, see \citet{molnar-2019-iml}. 
% 
% While many of the procedures discussed in this section apply to any model that makes predictions, it should be noted that these methods heavily depend on the accuracy and importance of the fitted model; hence, unimportant features may appear relatively important (albeit not predictive) in comparison to the other included features. For this reason, we stress the usefulness of understanding the scale on which VI scores are calculated and take that into account when assessing the importance of each feature and communicating the results to others. Also, we should point out that this work focuses mostly on \emph{post-hoc interpretability} where a trained model is given and the goal is to understand what features are driving the model's predictions. Consequently, our work focuses on functional understanding of the model in contrast to the lower-level mechanistic understanding \citep{montavon-2018-methods}. That is, we seek to explain the relationship between the model's prediction behavior and features without explaining the full internal representation of the model.\footnote{We refer the reader to \citet{poulin-2006-visual}, \citet{caruana-2015-intelligible}, \citet{bibal-2016-intterpretability}, and \citet{bau-2017-network}, for discussions around model structure interpretation.}

% \citet{harrison-1978-hedonic} were among the first to analyze the well-known Boston housing data. One of their goals was to find a housing value equation using data on median home values from $n = 506$ census tracts in the suburbs of Boston from the 1970 census; see \citet[Table IV]{harrison-1978-hedonic} for a description of each variable. The data violate many classical assumptions like linearity, normality, and constant variance. Nonetheless, \citeauthor{harrison-1978-hedonic}---using a combination of transformations, significance testing, and grid searches---were able to find a reasonable fitting model ($R^2 = 0.81$). Part of the payoff for there time and efforts was a supposedly interpretable prediction equation which is reproduced in Equation~\eqref{eqn:boston}.
% \begin{equation}
% \label{eqn:boston}
% \begin{split}
% \widehat{\log\left(MV\right)} &= 9.76 + 0.0063 RM^2 + 8.98\times10^{-5} AGE - 0.19\log\left(DIS\right) \\
%   &\quad + 0.096\log\left(RAD\right) - 4.20\times10^{-4} TAX - 0.031 PTRATIO \\
%   &\quad + 0.36\left(B - 0.63\right)^2 - 0.37\log\left(LSTAT\right) - 0.012 CRIM \\
%   &\quad + 8.03\times10^{-5} ZN + 2.41\times10^{-4} INDUS + 0.088 CHAS \\
%   &\quad - 0.0064 NOX^2
% \end{split}.
% \end{equation}
% But how interpretable is this housing value equation? Certainly you could tease out some of the effects, but the nonlinear transformations make this more difficult, even in the absence of higher order interaction effects. I would argue that this model isn't much more interpretable than a purely nonparametric model, like a bagged tree-based ensemble, when given the right interpretation tools.

This chapter is dedicated to select topics from the increasingly popular field of \emph{interpretable machine learning} (IML), which easily deserves its own book-length treatment, and it has; see, for example, \citet{molnar-2019-iml} and \citet{biecek-2021-explanatory} (both of which are freely available online). These methods can be categorized into whether they help interpret a "black box" model at the global or local (e.g., individual row or prediction) level. To be honest, I don't really like the term "black box", especially when we now have access to a rich ecosystem of interpretability tools. For example, linear regression models are often hailed as interpretable models. Sure, but this is really only true when the model has a simple form. Once you start including transformations and interaction effects---which are often required to boost accuracy and meet assumptions---the coefficients become much less interpretable.

Tree-based ensembles, especially the ones discussed in the next two chapters, can provide state-of-the-art performance, and are quite competitive with other popular supervised learning algorithms, especially on tabular data sets. Even when tree-based ensembles perform as advertised, there's a price to be paid in terms of parsimony, as we lose the ability to summarize the model using a simple tree diagram. Luckily, there exist a number of post-hoc techniques that allow us to tease the same information out of an ensemble of trees that we would ordinarily be able to glean from looking at a simple tree diagram (e.g., which variables seem to be the most important, the effect of each predictor, and potential interactions). Note that the techniques discussed in this chapter are \emph{model-agnostic}, meaning they can be applied to any type of supervised learning algorithm, not just tree-based ensembles. For example, they can also be used to help interpret neural networks, or a more complicated tree structure that uses linear splits or non-constant models in the terminal nodes.

% Are linear models really that interpretable? What about a single decision tree? My first experience with a deicision tree model occurred during my time as a graduate assistant in the Wright State University Statistical Consulting Center while working on a project to help identify anomalous credit card transactions for another University department. At one point we were using decision trees and I remember one of the trees being so large that we had to print it on several sheets of paper and tape it together just so we could have a good look at it. Clearly, such a tree was not very easy to interpret. But shallow decision trees and simple additive model are usually straightforward to interpret.

% Nowadays, many supervised learning algorithms can fit the data automatically in seconds---typically with higher accuracy. The downfall, however, is some loss of interpretation since these algorithms typically do not produce simple prediction formulas like Equation~\eqref{eqn:boston}. These models can still provide insight into the data, but it is not in the form of simple equations. For example, quantifying predictor importance has become an essential task in the analysis of "big data", and many supervised learning algorithms, like tree-based methods, can naturally assign variable importance scores to all of the predictors in the training data.

\FIXME{Add a brief subsection on feature interactions; reference your archive paper.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Feature importance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% From scikit-learn docs: Warning The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training data set and therefore do not necessarily inform us on which features are most important to make good predictions on held-out dataset. Secondly, they favor high cardinality features, that is features with many unique values. Permutation feature importance is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: Permutation Importance vs Random Forest Feature Importance (MDI).

For the purposes of this chapter, we can think of variable importance (VI) as the extent to which a feature has a "meaningful" impact on the predicted outcome. A more formal definition and treatment can be found in \citet{laan-2006-statistical}. Given that point of view, a natural way to assess the impact of an arbitrary feature $X_j$ is to remove it from the training data and examine the drop in performance that occurs after refitting the model without it. This procedure is referred to as \emph{leave-one-covariate-out} (LOCO) importance; see \citet{hooker-2019-stop} and the references therein. 

Obviously, the LOCO importance method is computationally prohibitive for larger data sets and complex fitting procedures because it requires retraining the model once more for each dropped feature. In the next section, we discuss an approximate approach based on reassessing performance after randomly permuting each feature (one at a time). This procedure is referred to as \emph{permutation importance} \index{Permutation importance}.


%-------------------------------------------------------------------------------
\subsection{Permutation importance\label{sec:interpret-permute}}
%-------------------------------------------------------------------------------

\FIXME{Check out \url{https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance}}

While some algorithms, like tree-based models, have a natural way of quantifying the importance of each predictor, it is useful to have a \emph{model-agnostic} procedure that can be used for any type of supervised learning algorithm. This also makes it possible to directly compare the importance of features across different types of models. In this section, we'll discuss a popular method for measuring the importance of predictors in any supervised learning model called \emph{permutation importance}.

The permutation-based VI scores exists in various forms and was made popular in \citet{random-2001-breiman} for random forests (Chapter~\ref{chap:rf}), before being generalized and extended in \citet{fisher-2018-model}. A general permutation-based VI procedure is outlined in Algorithm \ref{alg:interpret-permute} below. The idea is that if we randomly permute the values of an important feature in the training data, the training performance would degrade (since permuting the values of a feature effectively destroys any relationship between that feature and the target variable). This of course assumes that the model has been properly tuned (e.g., using cross-validation) and is not over fitting. The permutation approach uses the difference between some baseline performance measure (e.g., training $R^2$, AUC, or RMSE) and the same performance measure obtained after permuting the values of a particular feature in the training data (\strong{Note:} the model is NOT refit to the training data after randomly permuting the values of a feature). It is also important to note that this method may not be appropriate when you have, for example, highly correlated features (since permuting one feature at a time may lead to unlikely data instances). 

\begin{algo}
Let $X_1, X_2, \dots, X_j$ be the features of interest and let $\mathcal{M}_{orig}$ be the baseline performance metric for the trained model; for brevity, we'll assume smaller is better (e.g., classification error or RMSE). The permutation-based importance scores can be computed as follows:
\begin{enumerate}
  \item For $i = 1, 2, \dots, j$:
  \begin{enumerate}
    \item Permute the values of feature $X_i$ in the training data.
    \item Recompute the performance metric on the permuted data $\mathcal{M}_{perm}$.
    \item Record the difference from baseline using $imp\left(X_i\right) = \mathcal{M}_{perm} - \mathcal{M}_{orig}$.
  \end{enumerate}
  \item Return the VI scores $imp\left(X_1\right), imp\left(X_2\right), \dots, imp\left(X_j\right)$.
\end{enumerate}
\caption{A simple algorithm for constructing permutation-based VI scores. \label{alg:interpret-permute}}
\end{algo}

Algorithm \ref{alg:interpret-permute} can be improved or modified in a number of ways. For instance, the process can be repeated several times and the results averaged together. This helps to provide more stable VI scores, and also the opportunity to measure their variability. Rather than taking the difference in step (c), \citet[sec. 5.5.4]{molnar-2019-iml} argues that using the ratio $\mathcal{M}_{perm} / \mathcal{M}_{orig}$ makes the importance scores more comparable across different problems. It's also possible to assign importance scores to groups of features (e.g., by permuting more than one feature at a time); this would be useful if features can be categorized into mutually exclusive groups, for instance, categorical features that have been \emph{one-hot-encoded}.


%-------------------------------------------------------------------------------
\subsection{Software}
%-------------------------------------------------------------------------------

The permutation approach to variable importance is implemented in several \R{} packages, including: \pkg{vip} \citep{R-vip}, \pkg{iml} \citet{R-iml}, \pkg{ingredients} \citet{R-ingredients}, and \pkg{mmpf} \citep{R-mmpf}. Further details, some comparisons, and an in-depth explanation of \pkg{vip} are provided in \citet{vip2020}. Starting with version 0.22.0, scikit-learn's \pkg{inspection} modules provides an implementation of permutation importance for any fitted model.


%-------------------------------------------------------------------------------
\subsection{Example: Ames housing}
%-------------------------------------------------------------------------------

To illustrate the computations, let's compute permutation importance score for the Ames housing bagged tree ensemble (\code{ames.bag}) from the previous chapter; (Recall that \code{Sale\_Price} was rescaled by dividing by 1000.) Let's start by writing an RMSE function, our performance metric of interest, and use it to obtain a baseline for computing the permutation-based importance scores.

<<interpret-bagging-ames, cache=TRUE>>=
RMSE <- function(predicted, actual) {
  sqrt(mean((predicted - actual) ^ 2))
}
(baseline.rmse <- RMSE(predict(ames.bag, newdata = ames.trn), 
                       actual = ames.trn$Sale_Price))
@

To get a more stable importance measure, we'll use 30 permutations for each predictor. This is done using a nested \code{for} loop in the next code chunk:

<<interpret-permutation-ames-vi, cache=TRUE>>=
nperm <- 30  # number of permutation to use per feature
xnames <- names(subset(ames.trn, select = -Sale_Price))
vi <- matrix(nrow = nperm, ncol = length(xnames))
colnames(vi) <- xnames
for (j in colnames(vi)) {
  for (i in seq_len(nrow(vi))) {
    temp <- ames.trn  # temporary copy of training data
    temp[[j]] <- sample(temp[[j]])  # permute feature values
    pred <- predict(ames.bag, newdata = temp)  # score permuted data
    permuted.rmse <- RMSE(pred, actual = temp$Sale_Price) ^ 2
    vi[i, j] <- permuted.rmse - baseline.rmse  # smaller is better 
  }
}

# Average VI scores across all permutations
head(vi.avg <- sort(colMeans(vi), decreasing = TRUE))
@

Note that the individual permutation importance scores are computed independently of each other, making it relatively straightforward to parallelize the whole procedure; in fact, many \R{} implementations of Algorithm~\ref{alg:interpret-permute}, like \pkg{vip} and \pkg{iml}, have options to do this in parallel using a number of different parallel backends.

A boxplot of the unaggregated permutation scores for the top ten features, as measured by the average across all 30 permutations, is displayed in Figure~\ref{fig:interpret-permutation-ames-boxplot}. Here you can see that the overall quality rating of the home and its above ground square footage are two of the most important predictors of sale price, followed by neighborhood. A simple dotchart of the average permutation scores would suffice, but fails to show the variability in the individual importance values.

<<interpret-permutation-ames-boxplot, echo=FALSE, fig.cap="Permutation-based variable importance scores for the top ten features in the Ames housing example, as measured by the average across all 30 permutations.">>=
top10 <- names(vi.avg)[1L:10L]
vi <- as.data.frame(vi)  # coerce to data frame
vi <- reshape(vi, times = names(vi), timevar = "feature",
              varying = list(names(vi)), direction = "long",
              v.names = "importance")  # wide to long conversion
vi.top10 <- vi[vi$feature %in% top10, ]

# Display raw permutation importance scores
par(mar = c(4.1, 6, 0.1, 0.1))
boxplot(importance ~ feature, data = vi.top10, las = 1, col = 2,
        cex.axis = 0.7, horizontal = TRUE,
        xlab = "Permutation importance", ylab = "")
@

Although permutation importance is most naturally computed on the training data, it may also be useful to do the shuffling and measure performance on new data. This is discussed in depth in \citet[sec. 5.2]{molnar-2019-iml}.%; see Exercise~\ref{ex:tbd}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Feature effects}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While determining predictor importance is a crucial task in any supervised learning problem, ranking variables is only part of the story and once a subset of "important" features is identified it is often necessary to assess the relationship between them (or subset thereof) and the response. This can be done in many ways, but in statistical and machine learning it is often accomplished by constructing plots of \emph{partial dependence} \index{Partial dependence plots} or \emph{individual conditional expectation} \index{Individual conditional expectation plots}; see \citet{friedman-2001-greedy} and \citep{goldstein-peeking-2015}, respectively, for details.

%-------------------------------------------------------------------------------
\subsection{Partial dependence}
%-------------------------------------------------------------------------------

Partial dependence plots (PDPs) help visualize the relationship between a subset of the features (typically 1--3) and the response while accounting for the average effect of the other predictors in the model. They are particularly effective with "black box" models like random forests and support vector machines.

Let $\boldsymbol{x} = \left\{x_1, x_2, \dots, x_p\right\}$ represent the predictors in a model whose prediction function is $\widehat{f}\left(\boldsymbol{x}\right)$. If we partition $\boldsymbol{x}$ into an interest set, $\boldsymbol{z}_s$, and its compliment, $\boldsymbol{z}_c = \boldsymbol{x} \setminus \boldsymbol{z}_s$, then the "partial dependence" of the response on $\boldsymbol{z}_s$ is defined as
\begin{equation}
\label{eqn:avg_fun}
  f_s\left(\boldsymbol{z}_s\right) = E_{\boldsymbol{z}_c}\left[\widehat{f}\left(\boldsymbol{z}_s, \boldsymbol{z}_c\right)\right] = \int \widehat{f}\left(\boldsymbol{z}_s, \boldsymbol{z}_c\right)p_{c}\left(\boldsymbol{z}_c\right)d\boldsymbol{z}_c,
\end{equation}
where $p_{c}\left(\boldsymbol{z}_c\right)$ is the marginal probability density of $\boldsymbol{z}_c$: $p_{c}\left(\boldsymbol{z}_c\right) = \int p\left(\boldsymbol{x}\right)d\boldsymbol{z}_s$.
Equation~\eqref{eqn:avg_fun} can be estimated from a set of training data by
\begin{equation}
\label{eqn:pdf}
\bar{f}_s\left(\boldsymbol{z}_s\right) = \frac{1}{n}\sum_{i = 1}^n\widehat{f}\left(\boldsymbol{z}_s,\boldsymbol{z}_{i, c}\right),
\end{equation}
where $\boldsymbol{z}_{i, c}$ $\left(i = 1, 2, \dots, n\right)$ are the values of $\boldsymbol{z}_c$ that occur in the training sample; that is, we average out the effects of all the other predictors in the model.

Mathematical gibberish aside, computing partial dependence \eqref{eqn:pdf} in practice is rather straightforward. To simplify, let $\boldsymbol{z}_s = x_1$ be the predictor variable of interest with unique values $\left\{x_{11}, x_{12}, \dots, x_{1k}\right\}$. The partial dependence of the response on $x_1$ can be constructed as follows:

\begin{algo}
\begin{enumerate}
  \item For $i \in \left\{1, 2, \dots, k\right\}$:
  \begin{enumerate}
    \item Copy the training data and replace the original values of $x_1$ with the constant $x_{1i}$.
    \item Compute the vector of predicted values from the modified copy of the training data.
    \item Compute the average prediction to obtain $\bar{f}_1\left(x_{1i}\right)$.
  \end{enumerate}
  \item Plot the pairs $\left\{x_{1i}, \bar{f}_1\left(x_{1i}\right)\right\}$ for $i = 1, 2, \dotsc, k$.
\end{enumerate}
\caption{A simple algorithm for constructing the partial dependence of the response on a single predictor $x_1$. \label{alg:pdp}}
\end{algo}
Algorithm~\ref{alg:pdp} can be quite computationally intensive since it involves $k$ passes over the training records. Fortunately, the algorithm can be parallelized quite easily (more on this in Section~\ref{sec:computational}). It can also be easily extended to larger subsets of two or more features as well.


%-------------------------------------------------------------------------------
\subsubsection{Classification problems}
%-------------------------------------------------------------------------------

Traditionally, for classification problems, partial dependence functions are on a scale similar to the logit; see, for example, \citet[pp. 369---370]{hastie-2009-elements}. Suppose the response is categorical with $K$ levels, then for each class we compute

\begin{equation}
\label{eqn:avg-logit}
f_k\left(x\right) = \log\left[p_k\left(x\right)\right] - \frac{1}{K}\sum_{k = 1}^K\log\left[p_k\left(x\right)\right], \quad k = 1, 2, \dots, K,
\end{equation}

where $p_k\left(x\right)$ is the predicted probability for the $k$-th class. Plotting $f_k\left(x\right)$ helps us understand how the log-odds for the $k$-th class depends on different subsets of the predictor variables. Nonetheless, there's no reason partial dependence can't be displayed on the raw probability scale. Same goes for ICE plots (Section~\ref{sec:interpret-ice}). A multiclass classification example of PDPs on the probability scale is given in Section~\ref{sec:interpret-pdp-iris}.


%-------------------------------------------------------------------------------
\subsection{Individual conditional expectations \label{sec:interpret-ice}}
%-------------------------------------------------------------------------------

\FIXME{Switch to a real example; \url{file:///Users/bgreenwell/Desktop/iml-intro/docs/iml-intro.html#43}.}

PDPs can be misleading in the presence of strong interaction effects \citep{goldstein-peeking-2015} (akin to interpreting a main effects in a linear model that's also involved in an interaction term). To overcome this issue \citeauthor*{goldstein-peeking-2015} developed the concept of \emph{individual conditional expectation} (ICE) plots \index{Individual conditional expectation plots}. ICE plots display the estimated relationship between the response and a predictor of interest for each observation. Consequently, the PDP for a predictor of interest can be obtained by averaging the corresponding ICE plots across all observations. 

As described in \citep{goldstein-peeking-2015}, when the individual curves have a wide range of intercepts and consequently overlay each other, heterogeneity in the model can be difficult to discern. For that reason, \citeauthor*{goldstein-peeking-2015} suggest centering the ICE curves to produce a \emph{centered ICE plot} or \emph{c-ICE plot}. They also suggest other modifications, like \emph{derivative plots}, to further explore the presence of interaction effects. Centered ICE curves are obtained after shifting the ICE curves up or down by subtracting off the first value from each curve, effectively pinching them together at the beginning.

The next code example shows how to obtain c-ICE plots using the \pkg{pdp} package. Note that \pkg{pdp} defines its own \code{autoplot()} method for automatically constructing \pkg{ggplot2}-based PDPs and ICE plots; but keep in mind that plotting the results manually will always be more flexible. 


%-------------------------------------------------------------------------------
\subsection{Software}
%-------------------------------------------------------------------------------

PDPs and ICE plots (and many variants thereof) are implemented in several \R{} packages as well. Historically, PDPs were only implemented in specific tree-based ensemble packages, like \pkg{randomForest} and \pkg{gbm} \citep{R-gbm}. However, they were made generally available in package \pkg{pdp}, which was soon followed by \pkg{iml} and \pkg{ingredients}, among others; these packages also support ICE plots; the \R{} package \pkg{ICEbox} \citep{R-ICEbox} provides the original implementation of ICE plots and several variants thereof, like \emph{derivative ice plots} \citep{goldstein-peeking-2015}. Partial dependence and ICE plots were also made available in scikit-learn's \pkg{inspection} module, starting with version 0.22.0 and 0.24.0, respectively.


%-------------------------------------------------------------------------------
\subsection{Example: Ames housing \label{sec:interpret-pd-ames}}
%-------------------------------------------------------------------------------

Continuing with the Ames housing bagged tree ensemble, we'll see how to construct PDPs and ICE curves by hand and using the \pkg{pdp} package. To start, let's construct a PDP for above ground square footage (\code{Gr\_Liv\_Area}), one of the top predictors according to permutation importance. 

The first step is to create a grid of points over which to construct the plot. For continuous variables it is sufficient to use a fine enough grid of percentiles, as is done in the example below (\code{x.grid}). Then, we simply loop through each grid point and 1) copy the training data, 2) replace all the values of \code{Gr\_Liv\_Area} in the copy with the current grid value, and 3) score the modified copy and average the predictions together. Then we simply plot the grid points against the averaged predictions obtained from the \code{for} loop, as shown in the example below. The results are displayed in Figure~\ref{interpret-pdp-ames-gr-liv-area} and shows a relatively monotonic increasing relationship between above ground square footage and predicted sale price.

<<interpret-pdp-ames-gr-liv-area, par=TRUE, cache=TRUE, fig.cap="Partial dependence of sale price on above ground square footage for the bagged tree ensemble.">>=
x.grid <- quantile(ames.trn$Gr_Liv_Area, prob = 1:19 / 20)
pd <- numeric(length(x.grid))
for (i in seq_along(x.grid)) {
  temp <- ames.trn  # temporary copy of data 
  temp[["Gr_Liv_Area"]] <- x.grid[i]
  pd[i] <- mean(predict(ames.bag, newdata = temp))
}

# PDP for above ground square footage
plot(x.grid, pd, type = "l", xlab = "Above ground square footage",
     ylab = "Partial dependence", las = 1)
rug(x.grid)  # add rug plot to x-axis 
@

While looking at the partial dependence of the response on a single feature (i.e., main effect) is informative, it is often useful to look at the dependence on two or three predictors simultaneously\footnote{Theoretically, we can look at any order of interaction effect of interest, however, the computational complexity usually prohibits going beyond just two- or three-way interactions.} (i.e., interactions effects). Fortunately, it is rather straightforward to modify the above \code{for} loop to accommodate multiple predictors, however, this is a good opportunity to show an alternative method to computing partial dependence that can be more efficient if your working in SQL or Spark. 

In essence, we can generate all of the modified training copies in a single stacked data frame using a \emph{cross-join}, score it once, then aggregate the predictions into the partial dependence values. Below is an example using base \R{}, but it should be rather straightforward to translate to \pkg{data.table} \citep{R-data.table}, \pkg{dplyr} \citep{R-dplyr}, \pkg{SparkR} \citep{R-SparkR}, \pkg{sparklyr} \citep{R-sparklyr}, or any other language that can perform simple cross-joins---provided you can hold the resulting Cartesian product in memory! (An example in Spark will be given in Section~\ref{sec:rf-spark}).

To illustrate, we'll construct the partial dependence of sale price on both above ground and first floor square footage. To start, we still need to construct a grid of points over which to construct the plot; here, we'll use a Cartesian product between the percentiles of each feature using \code{expand.grid()}. In the next step, we need to perform a cross-join between the 2-dimensional grid and the original training data with those features removed. Then, we simply need to score the data, and aggregate by computing the average within each grid point, as shown in the example below. A \emph{false color level plot} of the results with contour lines is displayed in Figure~\ref{fig:interpret-pdp-ames-gr-liv-area-first-flr-sf}. Here, we can see the joint effect of both features on the predicted sale price.

<<interpret-pdp-ames-gr-liv-area-first-flr-sf, cache=TRUE, fig.cap="Partial dependence of sale price on above ground and first floor square footage for the bagged tree ensemble.">>=
x1.grid <- quantile(ames.trn$Gr_Liv_Area, prob = 1:20 / 20)
x2.grid <- quantile(ames.trn$First_Flr_SF, prob = 1:20 / 20)
df1 <- expand.grid("Gr_Liv_Area" = x1.grid, 
                   "First_Flr_SF" = x2.grid)  # Cartesian product

# Training data with plotting features removed
df2 <- subset(ames.trn, select = -c(Gr_Liv_Area, First_Flr_SF))

# Perform a cross-join between the two data sets; BE CAREFUL
# as the resulting data set can be quite large!
pd <- merge(df1, df2, all = TRUE)  # Cartesian product

# Score and then aggregate predictions
pd$yhat <- predict(ames.bag, newdata = pd)  # might take a few minutes!
pd <- aggregate(yhat ~ Gr_Liv_Area + First_Flr_SF, 
                data = pd, FUN = mean)

# PDP for above ground and first floor square footage
library(lattice)
levelplot(yhat ~ Gr_Liv_Area * First_Flr_SF, data = pd, 
          contour = TRUE, col = "white",
          col.regions = hcl.colors(100, palette = "Inferno"))
@

It is not wise to draw conclusions from PDPs in regions outside the area of the training data. \citet{pdp2017} describes two ways to mitigate the risk of extrapolation in PDPs: rug displays, like the one we used in Figure~\ref{fig:interpret-pdp-ames-gr-liv-area}, and \emph{convex hulls}. 

Constructing ICE curves is just as easy, we just skip the aggregation step and plot each individual curves. In the example below, we'll use the \pkg{pdp} package to construct a c-ICE plot showing the partial dependence of above ground square footage on sale price for each observation in the learning sample. We'll use the same percentiles to construct the plot as we did for the PDP in Figure~\ref{fig:interpret-pdp-ames-gr-liv-area} by invoking the \code{quantiles} and \code{probs} arguments in the call to \code{partial()}; note that \code{partial()}'s default is to use an evenly spaced grid of points across the range of predictor values. 

The results are displayed in Figure~\ref{fig:interpret-cice-ames-gr-liv-area}; the red line shows the average c-ICE value at each above ground square footage (i.e., the centered partial dependence). The heterogeneity in the c-ICE curves indicates a potential interaction between effect between \code{Gr\_Liv\_Area} and at least one other feature. The c-ICE curves indicate a relatively monotonic increasing relationship for the majority of houses in the training set, but you can see a few of the curves at the bottom deviate from this overall pattern. 

% \strong{WARNING:} The \code{partial()} function in \pkg{pdp} will not work correctly if package \pkg{purrr} is also loaded, as the latter also has a function called \code{partial()} that will cause a conflict. If you have the \R{} package \pkg{purrr} loaded, either unload it or call \code{pdp::partial()} directly.

<<interpret-cice-ames-gr-liv-area, cache=TRUE, fig.cap="c-ICE plot of above ground square footage for the Ames housing bagged tree ensemble, indicating a relatively monotonic increasing relationship for the majority of houses in the training set.">>=
ice <- partial(ames.bag, pred.var = "Gr_Liv_Area", ice = TRUE, 
               center = TRUE, quantiles = TRUE, probs = 1:19 / 20)
autoplot(ice, alpha = 0.1) + 
  ylab("Conditional expectation")
@


%-------------------------------------------------------------------------------
\subsection{Example: Edgar Anderson's iris data \label{sec:interpret-pdp-iris}}
%-------------------------------------------------------------------------------

For a classification example, we consider Edgar Anderson's iris data from the \pkg{datasets} package in \R{}. The \code{iris} data frame contains the sepal length, sepal width, petal length, and petal width (in centimeters) for 50 flowers from each of three species of iris: setosa, versicolor, and virginica. Below, we fit a bagged tree ensemble to the data using the \pkg{randomForest} package:

<<interpret-iris-rf>>=
library(randomForest)

# Fit a bagged tree ensemble
set.seed(1452)  # for reproducibility
(iris.bag <- randomForest(Species ~ ., data = iris, mtry = 4))
@

Next, we plot the partial dependence of \code{Species} on \code{Petal.Width} for each of the three classes using the \pkg{pdp} package. The result is displayed in Figure~\ref{fig:interpret-pdp-classification-iris-plott}. Note that without the aid of a user-supplied prediction function (via the \code{pred.fun} argument), \pkg{pdp}'s \code{partial()} function can only compute partial dependence in regards to a single class; see \citet{pdp2017} for more details on the use of this package. The next code chunk illustrates a simple trick to computing partial dependence with \code{partial()} for several classes simultaneously. Here we can clearly see the average effect petal width has on the probability of belonging to each species.

\FIXME{Still waiting on a fix for \code{partial()} to use names for \code{yhat.id} whenever available, as in this example.}

<<interpret-pdp-iris-petal-width, fig.cap="Partial dependence of species probability on petal width for each of the three iris species using a bagged tree ensemble.">>=
library(pdp)
library(ggplot2)

# Prediction wrapper that returns average prediction for each class
pfun <- function(object, newdata) {
  colMeans(predict(object, newdata = newdata, type = "prob"))
}

# Partial dependence of probability for each class on petal width
p <- partial(iris.bag, pred.var = "Petal.Width", pred.fun = pfun)
ggplot(p, aes(Petal.Width, yhat, color = as.factor(yhat.id))) +
  geom_line() +
  theme(legend.title = element_blank())
@

% Warning For GradientBoostingClassifier and GradientBoostingRegressor, the 'recursion' method (used by default) will not account for the init predictor of the boosting process. In practice, this will produce the same values as 'brute' up to a constant offset in the target response, provided that init is a constant estimator (which is the default). However, if init is not a constant estimator, the partial dependence values are incorrect for 'recursion' because the offset will be sample-dependent. It is preferable to use the 'brute' method. Note that this only applies to GradientBoostingClassifier and GradientBoostingRegressor, not to HistGradientBoostingClassifier and HistGradientBoostingRegressor.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Feature contributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In general, a \emph{local feature contribution} is the worth assigned to a feature's value that is proportional to the feature's share in the model's prediction for a particular observation. You can think of feature contributions as a directional variable importance at the individual prediction level. While there are a wide variety of feature contribution methodologies, the next section briefly covers one of the most popular methods in current use: \emph{Shapley values} (or \emph{Shapley explanations}). Shapley values necessarily involve a lot of mathematical notation, but I will try to avoid as much as possible while trying to convey the main concepts. For more details, start with \citet{strumbelj-2014-explaining} and \citet{lundberg-2017-shap}.


%-------------------------------------------------------------------------------
\subsection{Shapley values \label{sec:interpret-shapley}}
%-------------------------------------------------------------------------------

The Shapley value \citep{shapley-2016-value} is an idea from coalitional/cooperative game theory. In a coalitional game, assume there are $p$ players that form a grand coalition ($S$) worth a certain payout ($\Delta_S$). Suppose it is also known how much any smaller coalition ($Q \subseteq S$) (i.e., any subset of $p$ players) is worth ($\Delta_Q$). The goal is to distribute the total payout $\Delta_S$ to the individual $p$ players in a "fair" way; that is, so that each player receives their "fair" share. The Shapley value is one such solution and the only one that uniquely satisfies a particular set of "fairness properties".

Let $v$ be a \emph{characteristic function} that assigns a value to each subset of players; in particular, $v : 2^p \rightarrow \mathbb{R}$, where $v\left(S\right) = \Delta_S$ and $v\left(\emptyset\right) = 0$, with $\emptyset$ denoting the empty set (i.e., zero players). Let $\phi_i\left(v\right)$ be the contribution (or portion of the total payout) attributed to player $i$ in a particular game with total payout $v\left(S\right) = \Delta_S$. The Shapley value satisfies the following properties:

\begin{itemize}

  \item efficiency: $\sum_{i = 1} ^ p \phi_i\left(v\right) = \Delta_S$;
  
  \item null player: $\forall W \subseteq S \setminus \left\{i\right\}: \Delta_W = \Delta_{W \cup \left\{i\right\}} \implies \phi_i\left(v\right) = 0$;
  
  \item symmetry: $\forall W \subseteq S \setminus \left\{i, j\right\}: \Delta_{W \cup \left\{i\right\}} = \Delta_{W \cup \left\{j\right\}} \implies \phi_i\left(v\right) = \phi_j\left(v\right)$;
  
  \item linearity: If $v$ and $w$ are functions describing two coalitional games, then $\phi_i\left(v + w\right) = \phi_i\left(v\right) + \phi_i\left(w\right)$.

\end{itemize}

The above properties can be interpreted as follows: 1) the individual player contributions sum to the total payout, hence, are implicitly normalized; 2) if a player does not contribute to the coalition, they receive a payout of zero; 3) if two players have the same impact across all coalitions, they receive equal payout; and 4) the local contributions are additive across different games.

\citet{shapley-2016-value} showed that the unique solution satisfying the above properties is given by

\begin{equation}
\label{eqn:shapley-value}
\phi_i\left(v\right) = \frac{1}{p!} \sum_{\mathcal{O} \in \pi\left(p\right)} \left[v\left(S^\mathcal{O} \cup i\right) - v\left(S^\mathcal{O}\right)\right], \quad i = 1, 2, \dots, p,
\end{equation}

where $\mathcal{O}$ is a specific permutation of the player indices $\left\{1, 2, \dots, p\right\}$, $\pi\left(p\right)$ is the set of all such permutations of size $p$, and $S^\mathcal{O}$ is the set of players joining the coalition before player $i$.

In other words, the Shapley value is the average marginal contribution of a player across all possible coalitions in a game. Another way to interpret \eqref{eqn:shapley-value} is as follows. Imagine the coalitions (subsets of players) being formed one player at a time (which can happen in different orders), with the $i$-th player demanding a fair contribution/payout of $v\left(S^\mathcal{O} \cup i\right) - v\left(S^\mathcal{O}\right)$. The Shapley value for player $i$ is given by the average of this contribution over all possible permutations in which the coalition can be formed.

A simple example may help clarify the main ideas. Suppose three friends (players)---Alex, Brad, and Brandon---decide to go out for drinks after work (the game). They shared a few pitchers of beer, but nobody payed attention to how much each person drank (collaborated). What's a fair way to split the tab (total payout)? Suppose we knew the follow information, perhaps based on historical happy hours:

\begin{itemize}

  \item if Alex drank alone, he'd only pay \$10;
  
  \item if Brad drank alone, he'd only pay \$20;
  
  \item if Brandon drank alone, he'd only pay \$10;
  
  \item if Alex and Brad drank together, they'd only pay \$25;
  
  \item if Alex and Brandon drank together, they'd only pay \$15;
  
  \item if Brad and Brandon drank together, they'd only pay \$13;
  
  \item if Alex, Brad, and Brandon drank together, they'd only pay \$30.

\end{itemize}

With only three players, we can enumerate all possible coalitions. In Table \ref{tab:bar}, we list out all possible permutations of the three players and list the marginal contribution of each. Take the first row, for example. In this particular permutation, we start with Alex. We know that if Alex drinks alone, he'd spend \$10, so his marginal contribution by entering first is $10$. Next, we assume Brad enters the coalition. We know that if Alex and Brad drank together, they'd pay a total of $25$, leaving $15$ left over for Brad's marginal contribution. Similarly, if Brandon joins the party last, his marginal contribution would be only $5$ (the difference between $30$ and $25$). The Shapley value for each player is the average across all six possible permutations (these are the column averages reported in the last row). In this case, Brandon would get away with the smallest payout (i.e., have to pay the smallest portion of the total tab). The next time the bartender asks how you want to split the tab, whip out a pencil and do the math!

\begin{table}[!htb]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
                      & \multicolumn{3}{l}{Marginal contribution} \\ \midrule
Permutation/order of players           & Alex        & Brad         & Brandon      \\ \midrule
Alex, Brad, Brandon   & \$10        & \$15         & \$5          \\ 
Alex, Brandon, Brad   & \$10        & \$15         & \$5          \\
Brad, Alex, Brandon   & \$5         & \$20         & \$5          \\
Brad, Brandon, Alex   & \$10        & \$20         & \$0          \\
Brandon, Alex, Brad   & \$5         & \$15         & \$10         \\
Brandon, Brad, Alex   & \$17        & \$3          & \$10         \\ \midrule
Shapley contribution: & \$9.50      & \$14.67      & \$5.83       \\ \bottomrule
\end{tabular}
\caption{Marginal contribution for each permutation of the players/beer drinkers \{Alex, Brad, Brandon\} (i.e., the order in which they arrive). The Shapley contribution is the average marginal contribution across all permutations. (Notice how each row sums to the total bill of \$30.) \label{tab:bar}}
\end{table}


%-------------------------------------------------------------------------------
\subsection{Explaining predictions with Shapley values}
%-------------------------------------------------------------------------------

From this section forward, let $\left\{x_i\right\}_{i = 1} ^ p$ represent the $p$ feature values comprising $\boldsymbol{x} ^ \star$, the observation whose prediction we want to try to explain. \citet{strumbelj-2014-explaining} suggested using the Shapley value \eqref{eqn:shapley-value} to help explain predictions from a supervised learning model. In the context of statistical and machine learning:

\begin{itemize}

  \item a game is represented by the prediction task for a single observation $\boldsymbol{x} ^ \star$;
  
  \item the total payout/worth ($\Delta_S$) for $\boldsymbol{x} ^ \star$ is the prediction for $\boldsymbol{x} ^ \star$ minus the average prediction for all training observations (call this the baseline prediction): $f\left(\boldsymbol{x} ^ \star\right) - \bar{f}$;
  
  \item the players are the individual feature values of $\boldsymbol{x} ^ \star$ that collaborate to receive the payout $\Delta_S$ (i.e., predict a certain value).
  
\end{itemize}

The second point, combined with the efficiency property stated in the previous section, imply that the $p$ Shapley explanations (or feature contributions) for an observation of interest $\boldsymbol{x} ^ \star$, $\left\{\phi_i\left(\boldsymbol{x} ^ \star\right)\right\}_{i = 1} ^ p$, are inherently standardized since $\sum_{j = 1} ^ p \phi_i\left(\boldsymbol{x} ^ \star\right) = f\left(\boldsymbol{x} ^ \star\right) - \bar{f}$. 

Except in certain circumstances, computing the exact Shapley values is computationally infeasible in most applications; hence, approximations are often employed. To that end, several methods exist for estimating Shapley values in practice. The most common is arguably TreeSHAP \citep{lundberg-2020-local}, an efficient implementation of exact Shapley values for decision trees and ensembles thereof; while the details of TreeSHAP are beyond the scope of this book, we'll see an example of it in action in Section~\ref{sec:gbm-ex-titanic}. In the following section, we'll discuss a general way to compute Shapley values for any supervised learning model using a simple Monte Carlo approach.


%-------------------------------------------------------------------------------
\subsection{TreeSHAP \label{sec:TreeSHAP}}
%-------------------------------------------------------------------------------

TreeSHAP is a fast and exact method to estimate Shapley values for tree-based models (including tree ensembles), under several different possible assumptions about feature dependence. The specifics of TreeSHAP are beyond the scope of this book, so we'll defer to \citep{lundberg-2020-local} for the details. It's implemented in the \Python{} \pypkg{shap} module, and embedded in several tree-based modeling packages across several \opensource{} languages; we'll discuss one such implementation in Section~\ref{sec:gbm-ex-titanic}.


%-------------------------------------------------------------------------------
\subsection{Monte Carlo-based Shapley explanations \label{sec:interpret-SampleSHAP}}
%-------------------------------------------------------------------------------

Except in special circumstances, like TreeSHAP, computing the exact Shapley value is computationally infeasible in most applications. To that end, \citet{strumbelj-2014-explaining} suggest a Monte Carlo approximation, which we'll call SampleSHAP for short, that assumes independent features\footnote{While SampleSHAP, along with many other common Shapley value procedures, assumes independent features, several arguments can be made in favor of this assumption; see, for example, \citet{chen-2020-true} and the references therein.}. Their approach is described in Algorithm \ref{alg:interpret-SampleSHAP} below.

Here, A single estimate of the contribution of feature $x_i$ to $f\left(\boldsymbol{x} ^ \star\right) - \bar{f}$ is nothing the more than the difference between two predictions, where each prediction is based on a set of "Frankenstein instances"\^{}\footnote{The terminology used here takes inspiration from \citet[p. 231]{molnar-2019-iml}.} that are constructed by swapping out values between the instance being explained ($x$) and an instance selected at random from the training data. To help stabilize the results, the procedure is repeated a large number, say, $R$, times, and the results averaged together.

\begin{algo}
\begin{enumerate}
  \item For $j = 1, 2, \dots, R$:
  \begin{enumerate}
    \item Select a random permutation $\mathcal{O}$ of the sequence $1, 2, \dots, p$.
    \item Select a random instance $\boldsymbol{w}$ from the set of training observations $\boldsymbol{X}$.
    \item Construct two new instances as follows:
    \begin{itemize}
      \item $\boldsymbol{b}_1 = \boldsymbol{x} ^ \star$, but all the features in $\mathcal{O}$ that appear after feature $x_i$ get their values swapped with the corresponding values in $\boldsymbol{w}$.
      \item $\boldsymbol{b}_2 = \boldsymbol{x} ^ \star$, but feature $x_j$, as well as all the features in $\mathcal{O}$ that appear after $x_j$, get their values swapped with the corresponding values in $\boldsymbol{w}$.
    \end{itemize}
    \item $\phi_{ij}\left(\boldsymbol{x} ^ \star\right) = f\left(\boldsymbol{b}_1\right) - f\left(\boldsymbol{b}_2\right)$.
  \end{enumerate}
  \item $\phi_i\left(\boldsymbol{x} ^ \star\right) = \sum_{j = 1} ^ R \phi_{ij}\left(x\right) / R$.
\end{enumerate}
\caption{Approximating the $i$-th feature's contribution to $f\left(x\right)$. \label{alg:interpret-SampleSHAP}}
\end{algo}

A simple \R{} implementation of Algorithm~\ref{alg:interpret-SampleSHAP} is given below. Here, \code{obj} is a fitted model with scoring function \code{f} (e.g., \code{predict()}), \code{nsim} is the number of Monte Carlo repetitions to perform, \code{feature} gives the name of the corresponding feature in \code{x} to be explained, and \code{X} is the training set of features.

<<interpret-SampleSHAP>>=
sample.shap <- function(f, obj, R, x, feature, X) {
  phi <- numeric(R)  # to store Shapley values
  N <- nrow(X)  # sample size
  p <- ncol(X)  # number of features
  b1 <- b2 <- x
  for (m in seq_len(R)) {
    w <- X[sample(N, size = 1), ]
    ord <- sample(names(w))  # random permutation of features
    swap <- ord[seq_len(which(ord == feature) - 1)]
    b1[swap] <- w[swap]
    b2[c(swap, feature)] <- w[c(swap, feature)]
    phi[m] <- f(obj, newdata = b1) - f(obj, newdata = b2)
  }
  mean(phi)
}
@

To illustrate, let's continue with the Ames housing example (\code{ames.bag}). Below we use the \code{sample.shap()} function to estimate the contribution of the value of \code{Gr\_Liv\_Area} to the prediction of the first observation in the learning sample (\code{ames.trn}):

<<interpret-SampleSHAP-ames, cache=TRUE>>=
X <- subset(ames.trn, select = -Sale_Price)  # features only
set.seed(2207)  # for reproducibility
sample.shap(predict, obj = ames.bag, R = 100, x = X[1, ], 
            feature = "Gr_Liv_Area", X = X)
@

So, having \code{Gr\_Liv\_Area} $=$ \Sexpr{X[1, "Gr_Liv_Area"]} helped push the predicted sale price down toward the baseline average; in this case, the baseline average is just the average predicted sale price across the entire training set: $\bar{f} =$ \Sexpr{scales::dollar(mean(predict(ames.bag, newdata = X)), prefix = "\\$")} (don't forget that we rescaled the response in this example).

If there are $p$ features and $m$ instanced to be explained, this requires $2 \times R \times p \times m$ predictions (or calls to the scoring function $f$). In practice, this can be quite computationally demanding, especially since $R$ needs to be large enough to produce good approximations to each $\phi_i\left(x\right)$. How large does $R$ need to be to produce accurate explanations? It depends on the variance of each feature in the observed training data, but typically $R \in \left[30, 100\right]$ will suffice. The \R{} package \pkg{fastshap} \citep{R-fastshap} provides an optimized implementation of Algorithm \ref{alg:interpret-SampleSHAP} that only requires $2mp$ calls to $f$; see the package documentation for details.

SampleSHAP can be computationally prohibitive if you need to explain large data sets (optimized or not). Fortunately, you often only need to explain a handful of predictions, the most extreme ones, for example. However, generating explanations for the entire training set, or a large enough sample thereof, can be useful for generating aggregated global model summaries. For example, Shapley-based dependence plots \citep{lundberg-2020-local} show how a feature's value impacts the prediction of every observation in a data set of interest. 


%-------------------------------------------------------------------------------
\subsection{Software}
%-------------------------------------------------------------------------------

Various flavors of Shapley values are starting to become widely available in \R{}. Implementations of SampleSHAP, for example, are provided in \pkg{fastshap}, \pkg{iml}, and \pkg{iBreakDown} \citep{R-iBreakDown}. \citet{aksymiuk-2021-landscape} discuss several others.

The \pypkg{shap} module in \Python{} is arguably one of the first and most well known implementation of Shapley values for statistical and machine learning. It offers several different flavors of Shapley explanations, including: TreeSHAP, KernelSHAP \citep{lundberg-2017-shap}, SampleSHAP, and many more specific to different applications, like \emph{deep learning}.


%-------------------------------------------------------------------------------
\subsection{Example: Ames housing}
%-------------------------------------------------------------------------------

In the example below, we use \pkg{fastshap} to estimate feature contributions for the record in the test set with the highest predicted sale price using $R = 100$ Monte Carlo repetitions. Note that \pkg{fastshap}'s \code{explain()} function includes an adjustment argument to ensure the efficiency property; see \url{https://github.com/bgreenwell/fastshap/issues/6} for details.

As with \pkg{pdp}, \pkg{fastshap} defines its own \code{autoplot()} method for automatically producing various \pkg{ggplot2}-based Shapley plots. In the code chunk below, we use \code{explain()} and \code{autoplot()} to produce a bar plot of the feature contributions for the training observation with the highest predicted sale price:

<<interpret-shap-highest-explain, cache=TRUE, fig.cap="Top ten (Shapley-based) feature contributions to the highest training prediction from the Ames housing bagged tree ensemble.">>=
library(fastshap)
library(ggplot2)

# Find observation with highest predicted sale price
pred <- predict(ames.bag, newdata = ames.tst)
highest <- which.max(pred)
pred[highest]

# fastshap needs to know how to compute predictions from your model
pfun <- function(object, newdata) predict(object, newdata = newdata)

# Need to supply feature columns only in fastshap::explain()
X <- subset(ames.trn, select = -Sale_Price)  # feature columns only
newx <- ames.tst[highest, names(X)]

# Compute feature contributions for observation with highest prediction
set.seed(1434)  # for reproducibility
ex <- explain(ames.bag, X = X, nsim = 100, newdata = newx, 
              pred_wrapper = pfun, adjust = TRUE)
ex[1, 1:5]  # peek at a few
autoplot(ex, type = "contribution", num_features = 10, feature_values = newx)
@

Below, we construct a Shapley dependence plot for \code{Gr\_Liv\_Area} using \pkg{fastshap} with $R = 50$ Monte Carlo repetitions. The results are displayed in Figure~\ref{fig:interpret-pdp-ames-gr-liv-area}. As with Figures~\ref{fig:interpret-pdp-ames-gr-liv-area} and \ref{fig:interpret-cice-ames-gr-liv-area}, the predicted sale price tends to increase with above ground square footage. As with the c-ICE curves in Figure~\ref{fig:interpret-cice-ames-gr-liv-area}, the increasing dispersion in the plot indicates a potential interaction with at least one other feature. Coloring the Shapley dependence plot by the values of another feature can help visualize such an interaction, if you know what you're looking for.

<<interpret-ames-shap-dependence, cache=TRUE, fig.cap="Shapley dependence of above ground square footage on predicted sale price.">>=
ex <- explain(ames.bag, feature_names = "Gr_Liv_Area", X = X, 
              nsim = 50, pred_wrapper = pfun)

# Shapley dependence plot
autoplot(ex, type = "dependence", X = X, alpha = 0.3)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Drawbacks of existing methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As discussed in \citet{hooker-2019-stop}, \emph{permute-and-predict} methods---like PDPs, ICE plots, and permutation importance---can produce results that are highly misleading.\footnote{It's been argued that approximate Shapley values share the same drawback, however, \citet{janzing-2019-feature} makes a compelling case against those arguments.} For example, the standard approach to computing permutation-based VI scores involves independently permuting individual features. This implicitly makes the assumption that the observed features are statistically independent. In practice, however, features are often not independent which can lead to nonsensical VI scores. One way to mitigate this issue is to use the conditional approach described in \citet{strobl-2008-conditional}; \citet{hooker-2019-stop} provides additional alternatives, such as \emph{permute-and-relearn importance}. Unfortunately, to the best of our knowledge, this approach is not yet available for general purpose. A similar modification can be applied to PDPs \citep{parr-2019-technical}.

We already mentioned that PDPs can be misleading in the presence of strong interaction effects. As discussed earlier, this can be mitigated by using ICE plots instead. Another alternative would be to use \emph{accumulated local effect} (ALE) plots \citep{apley-2016-visualizing}. Compared to PDPs, ALE plots have the advantage of being faster to compute and less affected by strong dependencies among the features. The downside, however, is that ALE plots are more complicated to implement. ALE plots are available in the \pkg{ALEPlot} \citep{R-ALEPlot} and \pkg{iml} packages in \R{}.

\citet{hooker-2007-generalized} also argues that feature importance (which concern only \emph{main effects}) can be misleading in high dimensional settings, especially when there are strong dependencies and interaction effects among the features, and suggests an approach based on a \emph{generalized functional ANOVA decomposition}---though, to my knowledge, this approach is not widely implemented in \opensource{} software.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Born-again tree ensembles}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \url{http://proceedings.mlr.press/v119/vidal20a.html}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Final thoughts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\FIXME{Reword or move to beginning.}

IML is on the rise, and so is IML-related \opensource{} software. There are simply too many methods and useful packages to discuss in one chapter, so I just mentioned a handful. If you're looking for more, I'd recommend starting with the IML awesome list hosted by Patrick Hall at \url{https://github.com/jphall663/awesome-machine-learning-interpretability}. A good resource for \R{} users is \citet{maksymiuk-2021-landscape}. And of course, \citet{molnar-2019-iml} is a freely available resource, filled with intuitive explanations and links to relevant software.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Exercises}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \begin{ExerciseList}
% 
%   \Exercise While the permutation importance method outlined in Algorithm~\ref{alg:interpret-permute} has its fair share of drawbacks, it is rather straightforward to implement for any supervised learning model. Discuss various strategies to improve the computational efficiency of the simple \code{for} loop used in Section~\ref{sec:interpret-permute}) for the Ames housing example. Can Algorithm~\ref{alg:interpret-permute} be easily parallelized? If so, how would you approach it?
%   
%   \Exercise Suppose you one-hot-encoded several categorical features prior to fitting a model. How might you modify/apply Algorithm~\ref{alg:interpret-permute} to provide a permutation-based VI score for the original categorical features? Can you think of any ideas for modifying Algorithm~\ref{alg:interpret-permute} to quantify the importance of two-way interaction effects?
%   
%   \Exercise Discuss how you could extend the brute force approach outlined in Algorithm~\ref{algo:interpret-pdp} to construct a PDP for two or more features simultaneously (e.g., like the one shown in Figure~\ref{fig:interpret-pdp-two-vars}).
%   
%   \Exercise Discuss how you could effectively compute and display a three- or four-dimensional PDP. Specifically, how would you approach it differently if all the plotting features were continuous, categorical, and a mixture thereof. \textbf{Hint:} You can find some helpful ideas in \citet{pdp2017} and by studying \pkg{pdp}'s source code at \url{https://github.com/bgreenwell/pdp}.
%   
%   \Exercise Reproduce Figure~\ref{fig:interpret-ice} without using any packages. This should only require a slight modification or two to the code that produced Figure~\ref{fig:interpret-pdp-one-var}.
%   
%   \Exercise \FIXME{Exercise on Shapley values.}
% 
% \end{ExerciseList}

