% TODO:
%
% * Talk about margins: http://nymetro.chapter.informs.org/prac_cor_pubs/RandomForest_SteinbergD.pdf
%
% * http://contrib.scikit-learn.org/forest-confidence-interval/
%
% * Read Breiman's RF paper: 
%   https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf.
%
% * Compare RF's local importance (available in randomForest and ranger) to 
%   SHAP?
%
% * Need to use $x$ or $X$, but not both!
%
% * The key to accuracy (with weak learners), is low correlation and bias (rewrite as modification 
%   of "...low bias and variance") To keep bias low, trees are grown to maximum 
%   (or near maximum) depth. TTo keep variance low... To keep correlation, low...
%
% * Unlike boosting, the \emph{law of large numbers} insures convergence as 
%   $B \rightarrow \infty$.
%
% * Check out the grf package!
%
% * Mention the computational benefit to gini-corrected over permutation 
%   importance.
%
% * Reference \citet{ishwaran-2015-splitting} at some point in regards to splitting criteria in RFs.
%
% * Mention GUIDE implementation of RFs.
%
% * The Brier score was used rather than misclassification error because it 
%   directly measures accuracy in estimating the true conditional probability 
%   $\prob\left(Y = j|X\right)$.
%
% * From the KeRF paper: "Moreover, they are easy to run since they only depend 
%   on few parameters which are easily tunable (Liaw and Wiener, 2002; 
%   Genuer et al., 2008)".
% * Add a survival example and mention randomForestSRC package. Competing risks 
%   would make for a cool example. Plenty of examples here: 
%   https://cran.r-project.org/web/packages/randomForestSRC/randomForestSRC.pdf.

<<parent-rf, include=FALSE>>=
set_parent("book.Rnw")
@

<<rf-setup, include=FALSE>>=
# Load required packages
library(dplyr)
library(ggplot2)

# Set the plotting theme
theme_set(theme_bw())

# Custom color scale
scale_oi <- function(n = 2, fill = FALSE, alpha = 1, ...) {
  okabe.ito <- palette.colors(n, palette = "Okabe-Ito", alpha = alpha)
  if (isFALSE(fill)) {
    scale_colour_manual(values = unname(okabe.ito), ...)
  } else {
    scale_fill_manual(values = unname(okabe.ito), ...)
  }
}

# Colorblind-friendly palette
oi.cols <- unname(palette.colors(8, palette = "Okabe-Ito"))

# Ames housing data
ames <- as.data.frame(AmesHousing::make_ames())
ames$Sale_Price <- ames$Sale_Price / 1000  # rescale response
set.seed(4919)  # for reproducibility
id <- sample.int(nrow(ames), size = floor(0.7 * nrow(ames)))
ames.trn <- ames[id, ]
ames.tst <- ames[-id, ]

# Root mean square error helper function
rmse <- function(pred, obs, na.rm = FALSE) {
  sqrt(mean((pred - obs) ^ 2, na.rm = na.rm))
}

# Modified version of randomForest::MDSplot(); removed par(pty = "s")
mds.plot <- function(rf, fac, k = 2, palette = NULL, pch = 20, ...) {
  if (!inherits(rf, "randomForest")) 
    stop(deparse(substitute(rf)), " must be a randomForest object")
  if (is.null(rf$proximity)) 
    stop(deparse(substitute(rf)), " does not contain a proximity matrix")
  rf.mds <- stats::cmdscale(1 - rf$proximity, eig = TRUE, k = k)
  colnames(rf.mds$points) <- paste("Dim", 1:k)
  nlevs <- nlevels(fac)
  if (is.null(palette)) {
    palette <- if (requireNamespace("RColorBrewer", 
                                    quietly = TRUE) && nlevs < 12) 
      RColorBrewer::brewer.pal(nlevs, "Set1")
    else rainbow(nlevs)
  }
  if (k <= 2) {
    plot(rf.mds$points, col = palette[as.numeric(fac)], pch = pch, 
         ...)
  }
  else {
    pairs(rf.mds$points, col = palette[as.numeric(fac)], 
          pch = pch, ...)
  }
  invisible(rf.mds)
}
@


\chapter{Random forests\label{chap:rf}}

\begin{VF}
In a forest of a hundred thousand trees, no two leaves are alike. And no two journeys along the same path are alike.

\VA{Paulo Coelho}{}
\end{VF}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 %Hopefully you're familiar with the \emph{no free lunch theorem} and how it relates to statistical/machine learning. Basically, it means no single algorithm/methodology is uniformly best across all applications. While this is absolutely true, some algorithms tend to be more useful in a wide range of applications. In particular, I find random forests to be remarkably useful across many well-defined supervised learning problems. (Note that useful here does not necessarily mean most accurate.)

In grad school I attended a data mining talk by Richard De Veaux (an excellent speaker on the topic in general), which introduced me to many topics in SML, including random forests (RFs), and I've been fascinated ever since. RFs tend to provide similar accuracy to many state-of-the-art supervised learning algorithms on tabular data, while being relatively less difficult to tune. In other words, RFs tend to be competitive right out of the box. But be warned, RFs---like any SML algorithm---enjoy their fair share of disadvantages. As we'll see in this chapter, RFs also include many bells and whistles that data scientists can leverage for non-predictions tasks, like detecting anomalies/outliers, imputing missing values, and so forth.


%===============================================================================
\section{The random forest algorithm \label{sec:rf-algorithm}}
%===============================================================================

Recall that a bagged tree ensemble (Section~\ref{sec:enesmbles-bagging}) consists of hundreds (sometimes thousands) of independently grown decision trees, where each tree is trained on a different bootstrap sample from the original training data. Each tree is intentionally grown deep (low bias), and variance is reduced by averaging the predictions across all the trees in the ensemble. For classification, a plurality vote among the individual trees is used.
 
Unfortunately, correlation limits the variance-reducing effect of averaging. Take the following example for illustration. Suppose $\left\{X_i\right\}_{i = 1} ^ N \stackrel{iid}{\sim} \left(\mu, \sigma^2\right)$ are a random sample from some distribution with mean $\mu$ and variance $\sigma^2$. Let $\bar{X} = \sum_{i = 1} ^ N X_i/ N$ be the sample mean. If the observations are independent (as is the usual connotation of a random sample), then $\mathbb{E}\left(\bar{X}\right) = \mu$ and $\mathbb{V}\left(\bar{X}\right) = \sigma^2 / N$. In other words, the variance of the average is less than the variance of any of the sample elements. This of course assumes that the $X_i$ are uncorrelated. If the pairwise correlation between any two observations is $\rho = \rho\left(X_i, X_j\right)$ $\left(i \ne j\right)$, then 
\begin{equation}
\nonumber
  \mathbb{V}\left(\bar{X}\right) = \rho \sigma^2 + \frac{1 - \rho}{N} \sigma^2,
\end{equation}

which converges to $\rho \sigma^2$ as $N \rightarrow \infty$. In other words, regardless of sample size, correlation limits the variance-reducing effect of averaging. This is illustrated in Figure~\ref{fig:rf-correlation}, where each boxplot is constructed from 30,000 sample means, each of which is based on a sample of size $N = 30$ from a centered Gaussian distribution with specified pairwise correlation ($x$-axis); note the increasing variability in the sample means as we go from $\rho = 0$ to $\rho = 1$ (left to right).

<<rf-correlation, echo=FALSE, par=TRUE, fig.cap="100 simulated averages from samples of size $N = 30$ with pairwise correlation increasing from zero to one.">>=
corfun <- function(N = 30, rho = 0) {
  Sigma <- matrix(rho, nrow = N, ncol = N)
  diag(Sigma) <- 1
  X <- MASS::mvrnorm(N * 100, mu = rep(0, N), Sigma = Sigma)
  apply(X, MARGIN = 1, mean)
}
cors <- 0:10 / 10
res <- NULL
for (i in seq_along(cors)) {
  res <- rbind(res, cbind(cors[i], corfun(rho = cors[i])))
}
res <- as.data.frame(res)
boxplot(V2 ~ V1, data = res, xlab = "Pairwise correlation", 
        ylab = "Sample mean", col = 2)
@

Similarly, bagging a set of correlated predictors (i.e., models producing similar predictions) will only reduce the variance to a certain point. Since each tree is built using an independent bootstrap sample from the original training data, the trees in the bagged ensemble will be somewhat correlated. If we can reduce correlation between the trees, the trees will be more diverse and averaging can further improve the prediction and generalization performance of the ensemble.

%The same problem also applies to bagging, where we average together several predictions in an attempt to reduce prediction variance. If those predictions are correlated with each other (e.g., because they're produced from similar tree structures), then averaging isn't going to give as much benefit to reducing variance (i.e., overfitting). 

Luckily, Leo Breiman and Adele Cutler thought of a clever way to reduce correlation in a bagged tree ensemble; that is, make the trees more diverse. The idea is to limit the potential splitters at each node in a tree to a random subset of the available predictors, which will often result in a much more diverse ensemble of trees. In essence, bagging constructs a diverse tree ensemble by introducing randomness into the rows via sampling with replacement, while an RF further increases tree diversity by also introducing randomness into the columns via subsampling the features. 

In other words, an RF is just a bagged tree ensemble with an additional layer of randomness produced by selecting a random subset of candidate splitters prior to partitioning the data at every node in every tree---\emph{eXtreemly randomized trees} (Section~\ref{sec:rf-extratrees}), take this randomization a step further in an attempt to reduce variance even more. Let $m_{try} \le p$ be the number of candidate splitters selected at random from the entire set of $p$ features prior to each split in each tree. Setting $m_{try} << p$ can often dramatically improve performance compared to bagged decision trees; note that setting $m_{try} = p$ will results in an ordinary bagged tree ensemble. That's it, that's essentially the difference between bagged decision trees and RFs. The general steps for constructing a traditional RF are given in Algorithm~\ref{alg:rf}; compare this to Algorithm~\ref{alg:ensembles-bagging}.

\begin{algo}[!htb]
\begin{enumerate}[label=\arabic*)]

  \item Start with a training sample, $\boldsymbol{d}_{trn}$, and specify integers, $n_{min}$ (the minimum node size), $B$ (the number of trees in the forest), and $m_{try} \le p$ (the number of predictors to select at random as candidate splitters prior to splitting the data at each node in each tree).
  
  \item For $b$ in $1, 2, \dots, B$:
  
  \begin{enumerate}
  
    \item Select a bootstrap sample $\boldsymbol{d}_{trn} ^ \star$ of size $N$ from the training data $\boldsymbol{d}_{trn}$.
    
    \item \strong{Optional:} Keep track of which observations from the original training data were not selected to be in the bootstrap sample; these are called the \emph{out-of-bag} (OOB) observations.
    
    \item Fit a decision tree $\mathcal{T}_b$ to the bootstrap sample $\boldsymbol{d}_{trn} ^ \star$ according to the following rules:
    
    \begin{enumerate}
    
      \item Before each attempted split, select a random sample of $m_{try}$ features to use as candidate splitters.
      
      \item Continue recursively splitting each terminal node until the minimum node size $n_{min}$ is reached.
      
    \end{enumerate}
    
  \end{enumerate}
  
  \item Return the "forest" of trees $\left\{\mathcal{T}_b\right\}_{b = 1} ^ B$.
  
  %\item Let $\mathcal{T}_b\left(x\right)$ denote the prediction for a new case $x$ from the $b$-th tree in the forest. To obtain the random forest prediction for $x$, pass the observation down each tree and aggregate as follows:
  
  \item To obtain the RF prediction for a new case $\boldsymbol{x}$, pass the observation down each tree and aggregate as follows:
  
  \begin{itemize}
  
    \item Classification: $\widehat{C}_{B} ^ {rf}\left(\boldsymbol{x}\right) = vote\left\{\widehat{C}_b\left(\boldsymbol{x}\right)\right\}_{b = 1} ^ B$, where $\widehat{C}_b\left(\boldsymbol{x}\right)$ is the predicted class label for $x$ from the $b$-th tree in the forest (in other words, let each tree vote on the classification for $x$ and take the majority/plurality vote). 
    
    \item Regression: $\widehat{f}_{B} ^ {rf}\left(\boldsymbol{x}\right) = \frac{1}{B} \sum_{b = 1} ^ B\widehat{f}_b\left(\boldsymbol{x}\right)$ (in other words, we just average the predictions for case $x$ across all the trees in the forest).
    
  \end{itemize}
  
\end{enumerate}
\caption{Traditional RF algorithm for classification and regression. \label{alg:rf}}
\end{algo}

The recommended default values for $m_{try}$ (the number of features randomly sampled before each split) and $n_{min}$ (the minimum size of any terminal node) depend on the type of outcome:
\begin{itemize}

  \item For classification, the typical defaults are $m_{try} = \floor*{\sqrt{p}}$, where $p$ is the number of features, and $n_{min} = 1$.
  
  \item For regression, the typical defaults are $m_{try} = \floor*{p / 3}$ and $n_{min} = 5$.

\end{itemize}

Note that $\floor*{x}$ is just $x$ rounded down to the nearest integer. As we'll see in Section~\ref{sec:rf-tuning}, generalization performance is typically most sensitive to the value of $m_{try}$, but the default is usually in the ballpark, and the tuning space for $m_{try}$ is simple since $m_{try} \in \left\{1, 2, \dots, p\right\}$.

% For regression, a maximum depth tree is one where the minimum number of observations in any terminal node is five; for classification it's one. 

Traditionally, CART was used for the base learners in an RF, but any decision tree algorithm will work (e.g., GUIDE or CTree); we'll use CTree in Section~\ref{sec:rf-conditional-rf} to build an RF from scratch. Also, the traditional RF algorithm used the Gini splitting criterion \eqref{eqn:brpart-impurity} for classification and the SSE splitting criterion \eqref{eqn:brpart-reduction-sse} for regression. However, many other splitting criterion can be used to great affect. For example, \citet{ishwaran-2008-rsf} proposed several splitting rules more appropriate for right-censored outcomes, including a log-rank splitting rule that splits nodes by maximization of the log-rank test statistic; see also \citet{segal-1988-regression, leblanc-1992-relative}.

<<rf-classification-diagram, echo=FALSE, eval=FALSE, out.width="100%", fig.cap="Plurality voting in an RF. Here each tree makes a classification for an observation $x$ and the ensemble uses a majority/plurality vote.">>=
knitr::include_graphics("diagrams/rf-classification.png", error = FALSE)
@

<<rf-regression-diagram, echo=FALSE, eval=FALSE, out.width="100%", fig.cap="Averaging predictions in an RF for a quantitative outcome. For probability estimation, for example, we might take $\\widehat{f}_i\\left(x\\right) = \\hatprob\\left(y = 1 | x\\right)$; that is, the probability that $x$ belongs to class 1.">>=
knitr::include_graphics("diagrams/rf-regression.png", error = FALSE)
@


%-------------------------------------------------------------------------------
\subsection{Voting and probability estimation \label{sec:rf-prob}}
%-------------------------------------------------------------------------------

The voting scheme for classification outlined in step 4) of Algorithms~\ref{alg:ensembles-bagging} and \ref{alg:rf} is called \emph{hard voting} \index{Hard voting}. In hard voting, each base learner casts a direct vote on the class label, and a majority vote (binary outcome) or plurality vote (multiclass outcome) is used to determine the overall classification of an observation. 

With categorical outcomes, however, we often care more about the predicted probability of class membership, as opposed to directly predicting a class label. In an RF (or a bagged tree ensemble) there are two ways to obtain predicted probabilities: 

\begin{enumerate}[label=\arabic*)]

  \item Take the proportion of votes for each class over the entire forest. %For example, in Figure~\ref{fig:rf-classification-diagram}, the predicted probability that $\boldsymbol{x}$ belongs to class 1 is $\hatprob\left(Y = 1 | \boldsymbol{X} = \boldsymbol{x}\right) = 3 / 5 = 0.6$ since 3/5 of the trees voted for class 1.
  
  \item Average the class probabilities from each tree in the forest. (In this case, $n_{min}$ should be considered a tuning parameter; see, for example, \citet{malley-2012-consistent}.)
  
\end{enumerate}

The first approach can be problematic. For example, suppose the probability that $\boldsymbol{x}$ belongs to class $j$ is $\prob\left(Y = j | \boldsymbol{x}\right) = 0.91$. If each tree correctly predicts class $j$ for $\boldsymbol{x}$, then $\hatprob\left(\boldsymbol{x}\right) = 1$, which is incorrect. If $n_{min} = 1$, the two approaches are equivalent and neither will produce consistent estimates of the true class probabilities (see, for example, \citet{malley-2012-consistent}). So which approach is better for probability estimation? \citet[p.~283]{hastie-2009-elements} argue that the second method tends to provide improved estimates of the class probabilities with lower variance, especially for small $B$. 

\citet{malley-2012-consistent} make a similar argument for the binary case, but from a different perspective. In particular, they suggest treating the 0/1 outcome as numeric and fitting a regression forest using the standard MSE splitting criterion (an example of a so-called \emph{probability machine} \index{Probability machine}). Seems strange to use MSE on a 0/1 outcome, right? Not really. Recall from Section~\ref{sec:brpart-splitting} that the Gini index for binary outcomes is equivalent to using the MSE. \citeauthor{malley-2012-consistent} recommend using a minimum node size equal to 10\% of the number of training cases: $n_{min} = \floor*{0.1 \cdot N}$. However, for probability estimation, it seems natural to treat $n_{min}$ as a tuning parameter. \citet[Chap.~21--22]{devroye-1997-probabilistic} Provides some guidance on the choice of $n_{min}$ for consistent probability estimation in decision trees.

%Recall that in a typical classification tree, the predicted probability of $x$ belonging to class $j$ is just the proportion of observations that belong to class $j$ in whichever terminal node case $x$ falls in. This can be done for each tree in the forest, and the predicted probabilities from each tree averaged together (much like for regression problems). Alternatively, the proportion of votes for each class can be aggregated and used as an estimate of the predicted probability of a particular class. For example, if 455 trees in a forest of size $B = 500$ vote that a new observation $x$ belongs to class $j$, then the proportion of votes $455 / 500 = 0.91$ is the predicted probability of $x$ belonging to class $j$. However, as explained in \citet[p.~283]{hastie-2009-elements}, the latter approach based on proportion of votes is not ideal for estimating class probabilities. For example, suppose the probability that $x$ belongs to class $j$ is $\prob\left(x\right) = 0.91$. If each tree correctly predicts class $j$ for $x$, then $\hatprob\left(x\right) = 1$, which is incorrect.

The predicted probabilities can be converted to class predictions (i.e., by comparing each probability to some threshold), which gives us an alternative to hard voting called \emph{soft voting}\index{Soft voting}. In soft voting, we classify $\boldsymbol{x}$ to the class with the largest averaged class probability. This approach to classification in RFs tends to be more accurate since predicted probabilities closer to zero or one are given more weight during the averaging step; hence, soft voting attaches more weight to votes with higher confidence (or smaller standard errors; Section~\ref{sec:rf-se}).

% There are (at least) three different ways to aggregate the predictions of bagging classification trees. Most famous is class majority voting (aggregation="majority") where the most frequent class is returned. The second way is choosing the class with maximal averaged class probability (aggregation="average"). The third method is based on the "aggregated learning sample", introduced by Hothorn et al. (2003) for survival trees. The prediction of a new observation is the majority class, mean or Kaplan-Meier curve of all observations from the learning sample identified by the nbagg leaves containing the new observation. For regression trees, only averaged or weighted predictions are possible.


%-------------------------------------------------------------------------------
\subsubsection{Example: Mease model simulation}
%-------------------------------------------------------------------------------

To illustrate, we'll expand upon one of the simulation studies in \citet{malley-2012-consistent}; the Mease example, in particular \citep{mease-2007-boosted}. This is a two-dimensional circle problem with a binary outcome (0/1) and two independent features. The features are independent $\mathcal{U}\left(0, 50\right)$ random variables (i.e., the points are generated at random in the square $\left[0, 50\right] ^ 2$). The probability function is defined as
\begin{equation}
\label{eqn:mease}
  p\left(\boldsymbol{x}\right) = \prob\left(Y = 1 | \boldsymbol{x}\right) = \begin{cases}
    1, & r\left(\boldsymbol{x}\right) < 8 \\
    \frac{28 - r\left(\boldsymbol{x}\right)}{20}, & 8 \le r\left(\boldsymbol{x}\right) \le 20 \\
    0, & r\left(\boldsymbol{x}\right) \ge 28
  \end{cases},
\end{equation}
where $r\left(\boldsymbol{x}\right)$ is the Euclidean distance from $x$ to the point $\left(25, 25\right)$. A single sample of $N = 1000$ observations from the Mease model is displayed in Figure~\ref{fig:rf-mease-sample}. (As always, the code to reproduce the simulation is available on the accompanying book website.)

<<rf-mease-sample, echo=FALSE, par=TRUE, fig.cap="A sample of $N = 1000$ observation from the Mease model. ">>=
set.seed(1251)  # for reproducibility
mease <- treemisc::gen_mease(1000, nsim = 250)
#cols <- palette.colors(2, palette = "Okabe-Ito", alpha = 0.5)
#plot(x2 ~ x1, data = mease, xlab = expression(x[1]), ylab = expression(x[2]),
#     pch = c(15, 19)[mease$yclass1 + 1], col = cols[mease$yclass1 + 1],
#     cex = 0.8)
#legend("topleft", legend = c(expression(Y==0), expression(Y==1)), 
#       pch = c(15, 19), col = cols, bg = "white", cex = 0.8)
ggplot(mease, aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(yclass1), shape = as.factor(yclass1)),
             alpha = 0.5) +
  scale_oi(name = "", labels = c("Y = 0", "Y = 1")) +
  scale_shape(name = "", labels = c("Y = 0", "Y = 1")) +
  #theme(legend.position = "top")
  theme(legend.justification = c(-0.1, -0.1), legend.position = c(0, 0))
@

Figures~\ref{fig:rf-mease-simulation-median}--\ref{fig:rf-mease-simulation-mse} display the results of the simulation. In Figure~\ref{fig:rf-mease-simulation-median}, the median predicted probability across all 250 simulations was computed and plotted versus the true probability of class 1 membership. Here it is clear that the regression forest (i.e., treating the 0/1 outcome as continuous and building trees using the MSE splitting criterion) outperforms the classification forest (except when $n_{min} = 1$, in which case they are equivalent.) This is also evident from Figure~\ref{fig:rf-mease-simulation-median}, which shows the distribution of the MSE between the predicted class probabilities and the true probabilities for each case. In essence, for binary outcomes, regression forests produce consistent estimates of the true class probabilities.\footnote{By consistent, I mean that $\hatprob\left(Y = 1 | \boldsymbol{x}\right) \rightarrow \prob\left(Y = 1 | \boldsymbol{x}\right)$ as $N \rightarrow \infty$.} This goes to show that $m_{try}$ isn't the only important tuning parameter when it comes to probability estimation, and you should make an effort to tune $n_{min}$ as well. 

<<rf-mease-simulation-median, echo=FALSE, fig.cap="Class probability estimation using regression forests (yellow) and classification forests (black) in the Mease simulation. Starting from the top-left and moving clockwise, we have: $n_{min} = 1$ (the typical default for classification forests), $n_{min} = 10$ (the current default for probability forests in \\R{}'s \\pkg{ranger} package \\citep{R-ranger}), $n_{min} = 40$ (1\\% of the learning sample), and $n_{min} = 400$ (10\\% of the learning sample), respectively.">>=
# Setup
prob.med <- readRDS("../data/chap-rf-simulation-mease-prob-med.rds")
prob.med$type <- ifelse(prob.med$type == "rf", "RF", "CF")  # or `toupper()`
ggplot(prob.med, aes(x = true, y = median)) +
  geom_point(aes(color = type), alpha = 0.3) +
  facet_wrap(~ nodesize) +
  scale_oi() +
  xlab("True probability") +
  ylab("Median predicted probability") +
  theme(legend.position = "top")
@

<<rf-mease-simulation-mse, echo=FALSE, fig.cap="Mean squared errors (MSEs) from the Mease simulation. Here the MSE between the predicted probabilities and the true probabilities for each simulation are are displayed using boxplots. Clearly, in this example, the regression forest (RF) with $n_{min} > 1$ produces more accurate class probability estimates.">>=
prob.mse <- readRDS("../data/chap-rf-simulation-mease-prob-mse.rds")
prob.mse$type <- ifelse(prob.mse$type == "rf", "RF", "CF")  # or `toupper()`
ggplot(prob.mse, aes(x = type, y = mse)) +
  geom_boxplot(aes(fill = type)) +
  facet_wrap(~ nodesize, nrow = 1) +
  #scale_oi() +
  scale_oi(fill = TRUE, alpha = 0.7) +
  xlab("") +
  ylab("Mean squred error") +
  theme(legend.position = "none")
@


% %-------------------------------------------------------------------------------
% \subsection{Email spam example}
% %-------------------------------------------------------------------------------
% 
% Classification accuracy probably isn't the best metric to judge and compare models if you're interested in class probabilities. In fact, it isn't a proper scoring rule at all for assessing the predictive performance of probability models; see, for example, \citet[pp.~256--258]{harrell-2015-regression} and \citet{linnet-1989-brier}.\footnote{A \emph{proper scoring rule} for probability models is a metric applied to probability forecasts.} Two of the most common proper scoring rules for probability models are the \emph{Brier score} \index{Brier score} and \emph{log loss} \index{Log loss}. We'll discuss log loss a bit more in Chapter~\ref{chap:gbm}. The Brier score---named after meteorologist Glenn Brier, who introduced the statistic in 1950---is a quadratic scoring rule similar to the mean-squared error (MSE) often used for comparing regression models. 
% 
% For a binary outcome coded as 0/1, the Brier score is defined as
% \begin{equation}
% \nonumber
%   BS = \frac{1}{N} \sum_{i = 1} ^ N \left(Y_i - \hatprob\left(Y_i = 1 | \boldsymbol{x}\right)\right) ^ 2.
% \end{equation}
% 
% The Brier score is best used relatively (e.g., when comparing two models), as opposed to interpreting its absolute value\footnote{For reference, see the following tweet by Frank Harrell (@f2harrell
% ): \url{https://twitter.com/f2harrell/status/1280884278042206212}.}. To illustrate, let's return to the email spam example. Figure~\ref{fig:rf-brier-spam} shows the Brier score from the test data as a function of the number of trees in the forest for different values of $n_{min}$. The Brier score on its own is hard to interpret, but it's useful for comparing models or judging predictive performance compared to a baseline. Here I also included a single decision tree that's been pruned using the 1-SE rule. In this case, using $n_{min} = 1$ happens to perform reasonably well.

<<rf-brier-spam, echo=FALSE, eval=FALSE, par=TRUE, fig.cap="Brier score from test set as a function of the number of trees for various values of max node size.">>=
spam.bs <- readRDS("../data/rf-spam-brier-scores.rds")

# Plot results
palette("Okabe-Ito")
plot(spam.bs$rf.nsim.1, ylim = range(unlist(spam.bs)), type = "l", col = 1,
     xlab = "Number of trees", ylab = "Brier score (test data)")
lines(spam.bs$rf.nsim.10, col = 2)
lines(spam.bs$rf.nsim.20, col = 3)
lines(spam.bs$rf.nsim.30, col = 4)
abline(h = spam.bs$tree, lty = 2)
legend("topright", legend = c(expression(n[min]==1), expression(n[min]==10),
                              expression(n[min]==20), expression(n[min]==30),
                              "Single tree"), 
       inset = 0.01, bty = "n", lty = c(1, 1, 1, 1, 2), col = c(1:4, 1),
       cex = 0.8)
palette("default") 
@


%-------------------------------------------------------------------------------
\subsection{Subsampling (without replacement)}
%-------------------------------------------------------------------------------

While a traditional RF (Algorithm~\ref{alg:rf}) uses bootstrap sampling (i.e., sample with replacement), it can be useful to subsample the training data without replacement, prior to constructing each tree. This was noted in Section~\ref{sec:ensembles-bagging-subsampling} for bagged tree ensembles, and that discussion equally applies to RFs as well. Furthermore, as we'll discuss in Section~{sec:rf-importance}, subsampling with replacement can help eliminate certain bias in computing predictor importance, resulting in VI scores that can be used reliably for variable selection even in situations where the potential predictors vary in their scale of measurement or their number of categories. Most RF software include the option to use subsampling with or without replacement. %In Exercise~\ref{ex:rf-subsampling}, you'll be asked to investigate subsampling without replacement on the Ames housing example.


%-------------------------------------------------------------------------------
\subsection{Building an RF from scratch: predicting home prices \label{sec:rf-conditional-rf}}
%-------------------------------------------------------------------------------

To help solidify the basic concepts of an RF, let's construct one from scratch.\footnote{The code we're about to write is for illustration purposes only. It will not be nearly as efficient as actual RF software, which is typically written in a compiled language, like C, C++, or Fortran.} To do that, we need a decision tree implementation that will allow us to randomly select a subset of features for consideration at each node in the tree. Such arguments are available in scikit-learn's \pypkg{tree} module in \Python{}, as well as \R{}'s \pkg{party} and \pkg{partykit} packages---unfortunately, this option is not currently available in \pkg{rpart}. In this example, we'll go with \pkg{party}, since it's \code{ctree()} function is faster, albeit less flexible, than \pkg{partykit}'s implementation.

Below is the definition for a function called \code{crforest()}, which constructs a \emph{conditional random forest} (CRF) \index{Conditional random forest} \citep{party2006b, party2008b, strobl-2007-bias}; that is, an RF using conditional inference trees (Chapter~\ref{chap:urpart}) for the base learners.\footnote{Note that \pkg{party} and \pkg{partykit} both contain a \code{cforest()} function for fitting CRFs.} The \code{oob} argument will come into play in Section~\ref{sec:rf-oob}, so just ignore that part of the code for now. Note that the function returns a list of fitted CTrees that we can aggregate later for the purposes of prediction.

<<rf-crforest>>=
crforest <- function(X, y, mtry = NULL, B = 5, oob = TRUE) {
  min.node.size <- if (is.factor(y)) 1 else 5
  N <- nrow(X)  # number of observations
  p <- ncol(X)  # number of features          
  train <- cbind(X, "y" = y)  # training data frame
  fo <- as.formula(paste("y ~ ", paste(names(X), collapse = "+")))
  if (is.null(mtry)) {  # use default definition
    mtry <- if (is.factor(y)) sqrt(p) else p / 3
    mtry <- floor(mtry)  # round down to nearest integer
  }
  # CTree parameters; basically force the tree to have maximum depth
  ctrl <- party::ctree_control(mtry = mtry, minbucket = min.node.size,
                               minsplit = 10, mincriterion = 0)
  forest <- vector("list", length = B)  # to store each tree
  for (b in 1:B) {  # fit trees to bootstrap samples
    boot.samp <- sample(1:N, size = N, replace = TRUE)
    forest[[b]] <- party::ctree(fo, data = train[boot.samp, ],
                                control = ctrl)
    if (isTRUE(oob)) {  # store row indices for OOB data
      attr(forest[[b]], which = "oob") <- 
        setdiff(1:N, unique(boot.samp))
    }
  }
  forest  # return the "forest" (i.e., list) of trees
}
@

Let's test out the function on the Ames housing data, using the same 70/30 split from previous examples (Section~\ref{sec:data-ames}). Here, we'll fit a default CRF (i.e., $m_{try} = \floor*{p / 3}$ and $n_{min} = 5$) using our new \code{crforest()} function. (Be warned, this code may take a few minutes to run; the code on the book website includes an optional progress bar and the ability to run in parallel using the \pkg{foreach} package \citep{R-foreach}.)

<<rf-crf-ames, eval=FALSE>>=
X <- subset(ames.trn, select = -Sale_Price)  # feature columns
set.seed(1408)  # for reproducibility
ames.crf <- crforest(X, y = ames.trn$Sale_Price, B = 300)
@

<<rf-crf-ames-preds, echo=FALSE>>=
ames.crf.preds <- readRDS("../data/chap-rf-crforest-ames-preds.rds")
preds.tst <- ames.crf.preds$preds.tst
preds.oob <- ames.crf.preds$preds.oob
B <- ncol(preds.tst)  # number of trees in the forest
@

To obtain predictions from the fitted model, we can just loop through each tree, extract the predictions, and then average them together at the end. This can be done with a simple \code{for} loop, which is demonstrated in the code chunk below. Here, I obtain the averaged predictions from \code{ames.crf} on the test data and compute the test RMSE. %For convenience, let's write a predict method for the regression case. The last couple of lines specify the class of the output that gets returned; essentially, the output is a list of trees that we can operate on, but the additional class information will determine how predictions are computed from the fitted random forest. (For details on this type of \emph{object-oriented programming} in \R{}, see \citet[Chap.~13]{wickham-2019-advanced}.) This isn't necessary unless we want to extend some of \R{}'s useful generics (e.g., \code{print()} or \code{predict()}), but that will make things a little cleaner, so let's do it. Below I define a \code{predict()} method for the regression case (i.e., \code{"crf.reg"}). This function will compute the (aggregated) predictions from a \code{crforest()} object using the first \code{ntree} trees, which defaults to all the trees in the forest.

<<rf-crf-ames-predictions-tst, eval=FALSE>>=
B <- length(ames.crf)  # number of trees in forest
preds.tst <- matrix(nrow = nrow(ames.tst), ncol = B)
for (b in 1:B) {  # store predictions from each tree in a matrix
  preds.tst[, b] <- predict(ames.crf[[b]], newdata = ames.tst)
}
pred.tst <- rowMeans(preds.tst)  # average predictions across trees

# Root-mean-square error function
rmse <- function(pred, obs, na.rm = FALSE) {
  sqrt(mean((pred - obs) ^ 2, na.rm = na.rm))
}

# Root mean square error on test data
rmse(pred.tst, obs = ames.tst$Sale_Price)
@

<<rf-crf-ames-predictions-tst-load, echo=FALSE>>=
pred.tst <- rowMeans(preds.tst)  # average predictions across trees
rmse(pred.tst, obs = ames.tst$Sale_Price)
@

Rather than reporting the test RMSE for the entire forest, we can compute it for each sub-forest of size $b \le B$ to see how it changes as the forest grows. We can do this using a simple \code{for} loop, as demonstrated in the code chunk below. (Note that I use \code{drop = FALSE} here so that the subsetted matrix of predictions doesn't lose its dimension when \code{b = 1}.)

<<rf-crf-ames-rmses-tst>>=
rmse.tst <- numeric(B)  # to store RMSEs
for (b in 1:B) {
  pred <- rowMeans(preds.tst[, 1:b, drop = FALSE], na.rm = TRUE)
  rmse.tst[b] <- rmse(pred, obs = ames.tst$Sale_Price, na.rm = TRUE)
}
@

The above test RMSEs are displayed in Figure~\ref{fig:rf-crforest-ames-rmses} (black curve). For comparison, I also included the test error for a single CTree fit (horizontal dashed line). Here, the CRF clearly outperforms the single tree, and the test error stabilizes after about 50 trees. Next, we'll discuss an internal cross-validation strategy based on the OOB data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Out-of-bag (OOB) data \label{sec:rf-oob}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% A particularly good motivation for bagging is that it can be used to give ongoing estimates of the generalization error

% From Breiman (2001):
%
% Tibshirani [1996] and Wolpert and Macready [1996], proposed using out-of-bag estimates as an ingredient in estimates of generalization error. Wolpert and Macready worked on regression type problems and proposed a number of methods for estimating the generalization error of bagged predictors. Tibshirani used out-of-bag estimates of variance to estimate generalization error for arbitrary classifiers. The study of error estimates for bagged classifiers in
% Breiman [1996b], gives empirical evidence to show that the out-of-bag estimate is as accurate using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set. 

% Incorporate this if true:
%
% Since the error rate decreases as the number of combinations increases, the out-of-bag estimates will tend to overestimate the current error rate. To get unbiased out-of-bag error estimates, it is necessary to run past the point where the test set error converges. But unlike cross-validation, where bias is present but its extent unknown, the out-of-bag estimates are unbiased.

% From ESL:
%
% An oob error estimate is almost identical to that obtained by N-fold crossvalidation; see Exercise 15.2. Hence unlike many other nonlinear estimators,
% random forests can be fit in one sequence, with cross-validation being performed along the way. Once the oob error stabilizes, the training can be
% terminated.

One of the most useful by-products of an RF (or bagging in general, for that matter) is the so-called OOB data (see Step 2) (b) in Algorithm~\ref{alg:rf})\footnote{While the concept of OOB is usually discussed in the context of an RF, it equally applies to bagging and boosting when sampling is involved, regardless if the sampling is done with or without replacement.}. Recall that an RF, or any bagged tree ensemble, is constructed by combining predictions from decision trees trained on different bootstrap samples.\footnote{This discussion also applies to subsampling without replacement.} Since bootstrapping involves sampling with replacement, each tree in the forest only uses a subset of the original learning sample; hence, for each tree in the forest, a portion of the original learning sample isn't used---these observations are referred to as out-of-bag (or OOB for short). The OOB data associated with a particular tree can be used to obtain an unbiased estimate of prediction error. The OOB errors can then be aggregated across all the trees in the forest to obtain an overall out-of-sample, albeit unstructured, estimate of the overall prediction performance of the forest.

Since bagging/bootstrapping involves sampling with replacement, the probability that a particular case is not selected in a particular bootstrap sample is
\begin{equation}
\nonumber
  \prob\left(\text{case } i \notin \text{bootstrap sample } b\right) = \left(1 - \frac{1}{N}\right) ^ N.
\end{equation}
As $N \rightarrow \infty$ it can be shown that $\left(1 - \frac{1}{N}\right) ^ N \rightarrow e ^ {-1} \approx 0.368$. In other words, on average, each bootstrap sample contains approximately $1 - e ^ {-1} \approx 0.632$ of the original training records; the remaining $e ^ {-1} \approx 0.368$ observations are OOB and can be used as an independent validation set for the corresponding tree. This is rather straightforward to observe without a mathematical derivation. The code below computes the proportion of non-OOB observations in $B = 10000$ bootstrap samples of size $N = 100$, and averages the results together:

<<rf-inbag-proportion>>=
set.seed(1226)  # for reproducibility
N <- 100  # sample size
obs <- 1:N  # original observations
res <- replicate(10000, sample(obs, size = N, replace = TRUE))
inbag <- apply(res, MARGIN = 2, FUN = function(boot.sample) {
  mean(obs %in% boot.sample)  # proportion in bootstrap sample
})
mean(inbag)
@

Let $w_{b, i} = 1$ if observation $i$ is OOB in the $b$-th tree and zero otherwise. Further, if we let $B_i = \sum_{i = 1} ^ B w_{b, i}$ be the number of trees in the forest for which observation $i$ is OOB, then the OOB prediction for the $i$-th training observation is given by
\begin{equation}
\label{eqn:oob-prediction}
  \widehat{Y}_i^{OOB} = \frac{1}{B_i} \sum_{b: w_{b, i} = 1} \widehat{Y}_i^b, i = 1, 2, \dots, N.
\end{equation}
The OOB error estimate is just the error computed from these OOB predictions. (See \cite[Sec.~7.11]{hastie-2009-elements} for a more general discussion on using the bootstrap to estimate prediction error, and its apparent bias.)

To illustrate, we can compute the OOB RMSE for the CRF previously fit to the Ames housing data. There are numerous ways in which this can be done programmatically given our setup; I chose the easy route. Recall that each tree in our \code{rfo} object contains an attribute called \code{"oob"} which stores the row numbers for the training records that were OOB for that particular tree. From these we can easily construct an $N \times B$ matrix, where the $\left(i, j\right)$-th element is given by
\begin{equation}
\nonumber
  \begin{cases}
  \widehat{Y}_i^b & \quad \text{if }  w_{b, i} = 1 \\
  \code{NA} & \quad \text{if }  w_{b, i} = 0
  \end{cases}.
\end{equation}
The reason for using \code{NA}s in place of the predictions for the non-OOB observations will hopefully become apparent soon.

<<rf-crf-ames-predictions-oob, eval=FALSE>>=
preds.oob <- matrix(nrow = nrow(ames.trn), ncol = B)  # to hold OOB predictions
for (b in 1:B) {  # WARNING: Might take a minute or two!
  oob.rows <- attr(ames.crf[[b]], which = "oob")  # OOB row IDs for current tree
  preds.oob[oob.rows, b] <- 
    predict(ames.crf[[b]], newdata = ames.trn[oob.rows, ])
}
pred.oob <- rowMeans(preds.oob)  # average OOB predictions across trees

# Peek at results
preds.oob[1:5, 1:3]
@

<<rf-crf-ames-predictions-oob-load, echo=FALSE>>=
preds.oob[1:5, 1:3]
@

Peeking at the first few rows and columns we can see that the first training observation was not OOB in the first three trees, whereas the second observation was OOB for all three, so we obtained the corresponding OOB prediction. Next, we compute $\widehat{Y}_i^{OOB}$ as in Equation~\eqref{eqn:oob-prediction} by computing the row means of our matrix \code{pred.oob}---setting \code{na.rm = TRUE} in the call to \code{rowMeans()} ensures that the \code{NAs} in the matrix aren't counted, so that the average is taken only over the OOB predictions (i.e., the correct denominator $B_i$ will be used). Note that the OOB error is slightly larger than the test error we computed earlier.

<<rf-ames-rmse-oob>>=
pred.oob <- rowMeans(preds.oob, na.rm = TRUE)
rmse(pred.oob, obs = ames.trn$Sale_Price, na.rm = TRUE)
@

Similar to what we did in the previous section, we can compute the OOB RMSE as a function of the number of trees in the forest. The results are displayed in Figure~\ref{fig:rf-crforest-ames-rmses}, along with the test RMSEs from the same forest (black curve) and test error from a single CTree fit (horizontal blue line). Here, we can see that the OOB error is consistently higher than the test error, but both begin to stabilize at around 50 trees. 

<<rf-crforest-ames-rmses-oob>>=
rmse.oob <- numeric(B)  # to store RMSEs
for (b in 1:B) {
  pred <- rowMeans(preds.oob[, 1:b, drop = FALSE], na.rm = TRUE)
  rmse.oob[b] <- rmse(pred, obs = ames.trn$Sale_Price, na.rm = TRUE)
}
@

<<rf-crforest-ames-rmses, echo=FALSE, par=TRUE, fig.cap="RMSEs for the Ames housing example: the CRF test RMSEs (black curve), the CRF OOB RMSEs (yellow curve), and the test RMSE from a single CTree fit (horizontal blue line).">>=
# Include test error for a single tree for comparison
ames.ctree <- party::ctree(Sale_Price ~ ., data = ames.trn)
rmse.tst.ctree <- rmse(predict(ames.ctree, newdata = ames.tst), 
                       obs = ames.tst$Sale_Price)

# Plot OOB and test RMSE
palette("Okabe-Ito")
plot(rmse.tst, type = "l", col = 1, xlab = "Number of trees", 
     ylab = "Root-mean-square error", las = 1)
lines(rmse.oob, col = 2)
abline(h = rmse.tst.ctree, lty = 2, col = 3)
legend("topright", legend = c("CRF (test)", "CRF (OOB)", "Ctree (test)"), 
       lty = c(1, 1, 2), col = 1:3, inset = 0.01, bty = "n")
palette("default")
@

As noted in \citet{hastie-2009-elements}, the OOB error estimate is almost identical to that obtained by $N$-fold cross-validation, where $N$ is the number of rows in the learning sample; this is also referred to as \emph{leave-one-out cross-validation} (LOOCV). Hence, algorithms that produce OOB data can be fit in one sequence, with cross-validation being performed along the way. The OOB error can be monitored during fitting and the training can stop once the OOB error has "stabilized". In the Ames housing example (Figure~\ref{fig:rf-crforest-ames-rmses}) it can be seen that the test and OOB errors both stabilize after around 50 trees.

While the OOB error is computationally cheap, \citet{janitza-2018-oob} observed that it tends to overestimate the true error in many practical situations, including:
\begin{itemize}
  \item when the class frequencies are reasonably balanced in classification settings;
  \item when the sample size $N$ is large;
  \item when there is a large number of predictors;
  \item when there is correlation between the predictors;
  \item when the main effects are weak.
\end{itemize}
The positive bias of OOB error estimates was also noted in \citet{bylander-2002-estimating}. In light of this, it seems reasonable to consider the OOB error estimate as an upper bound on the true error. Fortunately, \citet{janitza-2018-oob} argue that the OOB error can be used effectively for hyperparameter tuning in RFs---which we'll discuss in the next section---without substantially affecting performance. This is good news since $k$-fold cross-validation can be computationally expensive, especially when tuning more complex models, like tree-based ensembles.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hyperparameters and tuning \label{sec:rf-tuning}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

RF is one of the most useful \emph{off-the-shelf} statistical learning algorithms you can know. By off-the-shelf, I mean a procedure that can be used effectively without much tweaking or tuning. Don't get me wrong, you can (and should try to) improve performance with a bit of tuning, but relative to other algorithms, the RFs often do reasonably well at their default settings. In contrast, \emph{gradient tree boosting} (Chapter~\ref{chap:gbm}) can often outperform RFs, but typically require a lot more tuning.

The most important tuning parameter in an RF is $m_{try}$. But I'd argue that the typical defaults (i.e., $m_{try} = \floor*{\sqrt{p}}$ for classification and $m_{try} = \floor*{p / 3}$ for regression) are quite good. For selecting $m_{try}$, a simple heuristic is to try the default, half of the default, and twice the default, and pick the best \citep{randomForest2002}. According to \citet{randomForest2002}, the results generally do not change dramatically, and even setting $m_{try} = 1$ can give very good performance for some data. Setting $m_{try} < p$ also lessens the computational burden of split variable selection (e.g., CART's exhaustive search for the best split), making RFs more computationally efficient than bagged tree ensembles, especially for larger data sets. On the other hand, if you only suspect a small fraction of the predictors to be "important", then larger values of $m_{try}$ may give better generalization performance.

The number of trees in the forest ($B$) is not a tuning parameter. You just need to make sure enough trees are aggregated for the error to stabilize (see, for example, Figure~\ref{fig:rf-crforest-ames-rmses}). However, it can be wasteful to fit more trees than necessary, especially when dealing with large data sets. For this reason, some RF implementations have the option to "stop early" if the validation error stops improving; the \R{} package \pkg{h2o} \citep{R-h2o} includes an RF implementation that supports early stopping with a wide variety of performance metrics. Such early stopping can be based on an independent test set, cross-validation, or the OOB error.

What about using the OOB error estimate or tuning? Although its been argued that the OOB error tends to overestimate the true error in certain cases (see, for example, \citet{mitchell-2011-oob}), \citet{janitza-2018-oob} noted that the overestimation seems to have little to no impact on tuning parameter selection, at least in their simulations. If ordinary cross-validation is too expensive, and you don't have access to separate validation and test sets, then using the OOB error is a certainly reasonable thing to do, and in many cases, more efficient. 

To illustrate, I carried out a small simulation using the Friedman 1 benchmark problem (Section~\ref{sec:data-friedman1}). For these data, there are 10 possible values for the $m_{try}$ parameter. For each value, I generated 100 separate train and test sets of 1000 observations each. For each repetition, I computed the OOB and test MSE and plotted the results; the results are displayed in Figure~\ref{fig:rf-mtry-errors}. In this example, you can see that the OOB error is quite in line with the test error, and both suggest an optimal value of $m_{try}$ around 5 or 6 (the traditional default here, indicated by a dashed vertical line, is $m_{try} = \floor*{10 / 3} = 3$).

<<rf-mtry-errors, par=TRUE, echo=FALSE, fig.cap="OOB and test error versus $m_{try}$ for the Friedman benchmark data using $N = 2000$ with a 50/50 split. The dashed line indicates the standard default for regression; in this case, $m_{try} = 3$.">>=
res <- readRDS("../data/chap-rf-tuning-mtry.rds")
pal <- palette.colors(3, palette = "Okabe-Ito", alpha = 0.7)[2:3]
boxplot(error ~ method * i, data = res, col = pal, xlab = expression(m[try]),
        ylab = "Mean squared error", names = NA, xaxt = "n")
axis(1, at = seq(from = 1.5, to = 19.5, by = 2), labels = 1:10)
legend("topright", inset = .05, legend = c("OOB", "TEST"), fill = pal)
abline(v = floor(10 / 3) + 2.5, lty = 2)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variable importance \label{sec:rf-importance}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\FIXME{Need to tidy up this section and clearly present which methods are more effective in which situations. Maybe add a tl;dr?}

\citet{breiman-2001-manual} proposed two measures of variable importance for RFs: 

\begin{itemize}

  \item A measure based on the mean decrease in node impurity, which I'll refer to as the impurity-based measure. Originally, only the Gini splitting criterion (Section~\ref{sec:splitting}) was used. This VI measure was discussed for general tree ensembles in Section~\ref{sec:ensemble-importance}.
  
  \item A novel permutation-based measure, which I'll refer to as the OOB-based permutation measure. This VI measure was discussed for general use in any supervised learning model in Section~\ref{sec:interpret-permutation}.
  
\end{itemize}

While these VI measures were originally introduced for the classification case, they naturally extend to the regression case as well. Note that many other measures have also been defined. An up-to-date and thorough overview of quantifying predictor importance in different tree-based methods is given in \citet{oh-2021-variable}.


%-------------------------------------------------------------------------------
\subsection{Impurity-based importance \label{sec:rf-gini-importance}}
%-------------------------------------------------------------------------------

\FIXME{Make notation here consistent with previous chapters!}

As discussed in Section~\ref{sec:ensemble-importance}, the importance of any predictor can be measured by aggregating the individual VI scores across all the trees in the forest. For an arbitrary feature $X$, we can use the total decrease in node impurities from splitting on $X$ (e.g., as measured by MSE for regression or the Gini index for classification), averaged over all $B$ trees in the forest:

\begin{equation}
\label{eqn:rf-importance-impurity}
  VI\left(X\right) = B^{-1}\sum_{b = 1} ^ B VI_{T_b}\left(X\right),
\end{equation}

where $VI_{T_b}\left(X\right)$ is the relative importance of $X$ in tree $T_b$ (Section~\ref{sec:brpart-importance}). Since averaging helps to stabilize variance, $VI\left(X\right)$ tends to be more reliable than $VI_{T_b}\left(X\right)$ \citep[p.~368]{hastie-2009-elements}. 

%Like any tree-based ensemble, a random forest can seem overwhelmingly difficult to interpret compared to an individual tree. Fortunately, we can summarize useful information, like variable importance, using tables and charts. The original random forest algorithm offers two ways to quantify the importance of each predictor: 1) the total decrease in node impurities from splitting on the variable, averaged over all trees in the forest, and 2) a permutation-based measure (Sections~\ref{sec:interpret-permutation} and~\ref{sec:rf-oob-importance}).

The split variable selection bias inherent in CART-like decision trees also affects the impurity-based importance measure in their ensembles (\eqref{eqn:rf-importance-impurity}). The bias tends to result in higher VI scores for predictors with more potential split points (e.g., categorical variables with many categories). Several authors have proposed methods for eliminating the bias when the Gini index is used as the splitting criterion; see, for example, \citet{sandri-2008-bias} and the references therein. An interesting (and rather simple) approach is provided in \citet{sandri-2008-bias}, and later modified in \citet{nembrini-2018-revival} for RFs.

The idea in \citet{sandri-2008-bias} is to realize that the impurity-based importance measure from a single CART-like decision tree can be expressed as the sum of two components:

\begin{equation}
\nonumber
  VI_{T_b}\left(X_i\right) = VI_{T_b}^{true}\left(X_i\right) +  VI_{T_b}^{bias}\left(X_i\right),
\end{equation}

where $VI_{T_b}^{true}\left(X_i\right)$ is the part attributable to informative splits and is related to the "true" importance of $X_i$, and $VI_{T_b}^{bias}\left(X_i\right)$ is the part attributable to uninformative splits and is a source of bias. The algorithm they propose attempts to eliminate the bias in $VI_{T_b}\left(X_i\right)$ by subtracting off an estimate of $VI_{T_b}^{bias}\left(X_i\right)$. This is done many times and the results averaged together. The basic steps are outlined in Algorithm~\ref{alg:rf-importance-impurity-corrected} below.

\begin{algo}
\begin{enumerate}

  \item For $r = 1, 2, \dots, R$:
  
  \begin{enumerate}
  
    \item Given the original $N \times p$ matrix of predictor values $\boldsymbol{X}$, generate an $N \times p$ matrix of \emph{pseudo predictors} $\boldsymbol{Z}_r$ using one of the following techniques:
    
    \begin{itemize}
    
      \item Randomly permuting each column of the original predictor values $\boldsymbol{X}$ (hence, the $j$-th column of $\boldsymbol{Z}_r$ can be obtained by randomly shuffling the values in the $j$-th column of $\boldsymbol{X}$).
      
      \item Randomly permuting the rows of $\boldsymbol{X}$; this has the advantage of maintaining any existing relationships between the original predictors.
      
    \end{itemize}

    \item Apply the ensemble procedure (e.g., bagging, boosting, or RF) using $\tilde{\boldsymbol{X}}_r = \left(\boldsymbol{X}, \boldsymbol{Z}_r\right)$ as the set of available predictors (i.e., use both the original predictors, as well as the randomly generated pseudo predictors).
    
    \item Use Equation~\ref{eqn:rf-importance-impurity} to compute both $imp\left(X_i\right)$ and $imp\left(Z_i\right)$; that is, compute the usual impurity-based variable importance measure for each predictor $X_i$ and pseudo predictor $Z_i$, for $i = 1, 2, \dots, p$.
    
  \end{enumerate}
  
  \item Compute the bias-adjusted impurity-based importance measure for each predictor $X_i$ ($i = 1, 2, \dots, p$) as $VI^{\star}\left(X_i\right) = R^{-1} \sum_{r = 1}^R \left(VI\left(X_i\right) - VI\left(Z_i\right)\right)$.

\end{enumerate}
\caption{Bias-corrected Gini importance. \label{alg:rf-importance-impurity-corrected}}
\end{algo}

Algorithm~\ref{alg:rf-importance-impurity-corrected} can be used to correct biased VI scores from a single CART-like tree or an ensemble thereof. Also, while the original algorithm was developed for the Gini-based importance measure, \citet{sandri-2008-bias} suggest it is also effective at eliminating bias for other impurity measures, like cross-entropy and SSE. One of the drawbacks of Algorithm~\ref{alg:rf-importance-impurity-corrected}. however, is that it effectively doubles the number of predictors to $2p$ and requires multiple ($R$) iterations. This can be computationally prohibitive for large data sets, especially for tree-based ensembles. Fortunately,  \citet{nembrini-2018-revival} proposed a similar technique specific to RFs that only requires a single replication. I'll omit the details, but the procedure is available in the \pkg{ranger} package for \R{} (which has also been ported to \Python{} and is available in the \pypkg{skranger} package \citep{Python-skranger}); an example is given in Figure~\ref{fig:rf-vi-bias}.

\FIXME{Move to section on conditional permutation importance?}

Even though our quick-and-dirty \code{crforest()} function in Section~\ref{sec:rf-conditional-rf} used bootstrap sampling, the actual CRF procedure described in \citet{strobl-2007-bias}, and implemented in \R{} packages \pkg{party} and \pkg{partykit}, defaults to growing trees on random subsamples of the training data without replacement (by default, the size of each sample is given by $\floor*{0.632 N}$), as opposed to bootstrapping. \citet{strobl-2007-bias} showed that this effectively removes the bias in CRFs due to the presence of predictor variables that vary in their scale of measurement or their number of categories.


%-------------------------------------------------------------------------------
\subsection{OOB-based permutation importance \label{sec:rf-oob-importance}}
%-------------------------------------------------------------------------------

%RFs offer an additional method for computing VI scores. The idea is to use the leftover \dfn{out-of-bag} (OOB) data to construct validation-set errors for each tree. Then, each predictor is randomly shuffled in the OOB data and the error is computed again. The idea is that if variable \(X\) is important, then the validation error will go up when \(X\) is perturbed in the OOB data. The difference in the two errors is recorded for the OOB data then averaged across all trees in the forest. Note that both methods for constructing VI scores can be unreliable in certain situations; for example, when the predictor variables vary in their scale of measurement or their number of categories \citep{party2007a}, or when the predictors are highly correlated \citep{strobl-2019-conditional}. The \pkg{varImp} package discussed earlier provides methods to address these concerns for RFs in package \pkg{party}, with similar functionality also built into the \CRANpkg{partykit} package \citep{R-partykit}. The \pkg{vip} package also supports the conditional importance described inwa \citep{strobl-2019-conditional} for both \pkg{party}- and \pkg{partykit}-based RFs; see \texttt{?vip::vi\_model} for details. Later on, we'll discuss a more general permutation method that can be applied to any supervised learning model.

RFs offer an additional (and unbiased) VI method; the approach is quite similar to the more general permutation approach discussed in Section~\ref{sec:interpret-permutation}, but based on permuting observations in the OOB data instead. The idea is that if predictor $X$ is important, then the OOB error will go up when $X$ is perturbed in the OOB data. In particular, we start by computing the OOB error for each tree. Then, each predictor is randomly shuffled in the OOB data and the OOB errors are computed again. The difference in the two errors is recorded for the OOB data, then averaged across all trees in the forest. As with the more general permutation-based importance measure, these scores can be unreliable in certain situations; for example, when the predictor variables vary in their scale of measurement or their number of categories \citep{party2007a}, or when the predictors are highly correlated \citep{strobl-2008-conditional}. Additionally, the corrected Gini-based importance discussed in \citet{nembrini-2018-revival} has the advantage of being faster to compute and more memory efficient.

Figure~\ref{fig:rf-vi-bias} shows the results from three difference RF VI measures on the simulation example from Section~\ref{sec:ctree-intro}. Here, we can see that the traditional Gini-based VI measure is biased towards the categorical variables, while the corrected Gini and permutation-based VI scores are relatively unbiased.

<<rf-vi-bias, echo=FALSE, fig.cap="TBD.">>=
# Read in results, wrangle, and plot
levs <- c("RF (Gini)", "RF (permutation)", "RF (Gini-corrected)")
temp <- readRDS("../scripts/bias_rf_vi.rds")
temp$method <- factor(temp$method, levels = levs)  # to reorder panels
temp %>%
  group_by(Variable, method) %>%
  summarize(avg = mean(Importance)) %>%
  ggplot(aes(Variable, avg)) +
    geom_col() +
    geom_hline(yintercept = 4, linetype = "dashed") +
    xlab("") +
    ylab("Average rank") +
    facet_wrap(~ method) +
    theme_bw()
@

The next two sections discuss more contemporary permutation schemes for RFs that deserve some consideration.


%-------------------------------------------------------------------------------
\subsubsection{Holdout permutation importance}
%-------------------------------------------------------------------------------

One drawback to computing variable importance in general is the lack of a natural cutoff that can be used to discriminate between "important" and "non-important" predictors. A number approaches based on null hypothesis testing and \emph{thresholding} have been developed for addressing this problem; see, for example, \citet{altmann-2010-permutation} and \citet[Sec.~6]{loh-2021-variable}. \citet{janitza-2018-holdout} argued that the null distribution of the OOB-based permutation measure is not necessarily symmetric; in particular, for irrelevant features. This makes the OOB-based permutation VI scores less suitable for selecting relevant features using a hypothesis-driven approach. Instead, \citet{janitza-2018-holdout} propose a method referred to as \emph{holdout variable importance}, which has a symmetric null distribution for both relevant and irrelevant predictors. The idea is to split the data into two halves, grow an RF on one half, and use the leftover half to compute a permutation-based variable importance score. 


%-------------------------------------------------------------------------------
\subsubsection{Conditional permutation importance}
%-------------------------------------------------------------------------------

A major draw back of permutation-based importance measures is the inherent assumption of independent features (e.g., the features are uncorrelated). For example, if $X_1$ and $X_2$ have a strong dependency, then it doesn't make sense to randomly permute $X_1$ while holding $X_2$ constant (or vice versa). To this end, \citep{strobl-2008-conditional} describe a \emph{conditional permutation importance} measure that adjusts for correlations between predictor variables. In particular, the conditional permutation importance of each variable is computed by permuting within a grid defined by the covariates that are associated with the variable of interest. According to \citet{strobl-2008-conditional}, the resulting VI scores are conditional in the sense of coefficients in a regression model, but represent the effects of a variables in both main effects and interactions. When missing values are present in the predictors, the procedure described in \citet{hapfelmeier-2014-new} can be used to measure variable importance. While this idea applies in general to any type of RF, it's implementations currently seem limited to the conditional inference trees and forest provided by the \pkg{party} and \pkg{partykit} packages in \R{}. 

% The \pkg{varImp} package discussed earlier provides methods to address these concerns for RFs in package \pkg{party}, with similar functionality also built into the \pkg{partykit} package \citep{R-partykit}. The \pkg{vip} package also supports the conditional importance described in \citep{strobl-2008-conditional} for both \pkg{party}- and \pkg{partykit}-based RFs; see \texttt{?vip::vi\_model} for details. %Later on, we'll discuss a more general permutation method that can be applied to any supervised learning model.


% %-------------------------------------------------------------------------------
% \subsubsection{Local variable importance}
% %-------------------------------------------------------------------------------
% 
% In some cases, it can be useful to know the importance of each feature at the individual row level (i.e., on an individual prediction). In Breiman's RF, this is referred to as \emph{local importance}. According to the original RF website\footnote{The original website, \url{https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm}, is still maintained and provides one of the best references to Breiman's original RF algorithm.}, the local importance of variable $X$ for a case $\boldsymbol{x}^\star$ is computed as follows: 1) Consider all the trees for which $\boldsymbol{x}^\star$ is OOB. 2) Subtract the percentage of votes for the correct class in the OOB data where $X$ has been randomly shuffled from the percentage of votes for the correct class in the untouched OOB data. 
% 
% I have not seen any convincing use-case for Breiman's RF local importance measure, and it seems less stable than the global permutation measure \citep{touw-2013-data}. Further, modern prediction explanation techniques, like Shapley feature contributions (Section~\ref{sec:iml-shapley}), provide a more general measure of the local importance of each feature and can be applied to any type of model, not just a RF.

% From \code{?randomForest::randomForest}: The “local” (or casewise) variable importance is computed as follows: For classification, it is the increase in percent of times a case is OOB and misclassified when the variable is permuted. For regression, it is the average increase in squared OOB residuals when the variable is permuted.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proximities \label{sec:rf-proximities}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Not every SML problem is supervised. For example, it is often of interest to understand how the data clusters---that is, whether the rows of the data form any "interesting" groups. (This is an application of \emph{unsupervised learning} \index{Unsupervised learning}.) Many clustering methods rely on computing the pairwise distances between any two rows in the data, but the challenge becomes choosing the right distance metric. Euclidean distance (i.e., the  "ordinary" straight-line, or "as the crow flies" distance between two points), for example, is quite sensitive to the scale of the inputs. It's also rather awkward to compute the Euclidean distance between two rows of data when the features are a mix of both numeric and categorical types. Fortunately, other distance (or distance-like) measures are available which more naturally apply to mixed data types.

Another useful output that can be obtained from an RF, provided it's implemented, are pairwise case \emph{proximities}. RF proximities are distance-like measures of how similar any two observations are, and can be used for:

\begin{itemize}[nosep]

  \item clustering in supervised and unsupervised (Section~\ref{sec:rf-unsupervised}) settings;

  \item detecting outliers/novel cases (Section~\ref{sec:rf-proximity-outliers});
  
  \item imputing missing values (Section~\ref{sec:rf-proximity-imputation}). 
  
\end{itemize}

To compute the proximities between all pairs of training observations, do the following:

\begin{enumerate}[nosep, label={\arabic*)}]

  \item pass all of the data, both training and OOB, down teach tree;
  
  \item every time records $i$ and $j$ cohabitate in the same terminal node of a tree, increase their proximity by one;
  
  \item At the end, normalize the proximities by dividing by the number of trees in the forest.  
  
\end{enumerate}

So how does this measure similarity between cases? Recall that RFs (and bagged decision trees in general) intentionally build deep, overgrown decision trees. In order for two observations to land in the the same terminal node, they have to satisfy all of the same conditions leading to it. If two observations occupy the same terminal node across a majority of the trees in the forest, then they are likely very similar to each other in terms of feature values. Note that using all the training data can lead to unrealistic proximities. To circumvent this, proximities can be computed on only the OOB cases. It is also possible to compute proximities for new cases (an example application is given in Section~\ref{sec:rf-csrf}).

The end result is an $N \times N$ proximity matrix, where $N$ is the sample size of the data set proximities are being computed for. As it turns out, this matrix is symmetric (since $\prox\left(i, j\right) = \prox\left(j, i\right)$), positive definite (i.e., has all positive eigenvalues), and bounded above by one, with the diagonal elements equal to one (since $\prox\left(i, i\right) = 1$). Consequently, for any two cases $i$ and $j$, we can treat $1 - \prox\left(i, j\right)$ as a squared distance-like metric, which can be used as input into any distance-based clustering algorithm. For example, \citet{shi-2005-tumor} used RF proximities to help identify fundamental subtypes of cancer.

The proximities from an RF provide natural measure of similarity between records when the predictor variables are of mixed types (e.g., numeric and categorical) and measured on different scales; they are invariant to monotone transformations and naturally support categorical variables. The biggest drawback, as with any pairwise distance-like metric, is that it requires storing an $N \times N$ matrix; technically, since the matrix is symmetric, you only need to store the upper or lower triangular part (see, for example, \code{?treemisc::proximity}). Proximities are also not implemented in most \opensource{} RF software. However, if you can obtain the $N \times B$ matrix of terminal node assignments (which is available in most \opensource{} RF software), then it is rather straightforward to compute the proximities yourself; an example, specific to the \R{} package \pkg{ranger} \citep{R-ranger}, can be found at \url{https://mnwright.github.io/ranger/r/oob-proximity-matrix/}, while a C++ implementation is available in \pkg{treemisc}'s \code{proximity()} function. The next two sections discuss more specific uses of proximities that are useful in a supervised learning context.

% For example, a simple C++ implementation, via the \pkg{Rcpp} package \citet{R-Rcpp}, is given below (note that this function can be extended to compute proximities using only the OOB cases). The required input (\code{x}) is an $N \times B$ matrix of terminal node assignments. Since the proximity matrix is symmetric, the implementation below only computes the upper-triangular portion. If you're not familiar with \pkg{Rcpp}, the \code{cppFunction()} function dynamically defines an \R{} function with C++ source code, compiles and links a shared library with bindings to the C++ function, then defines an \R{} function that uses \code{.Call()} to invoke the library; see \code{?Rcpp::cppFunction} for details and links to additional resources. This is essentially the code behind \pkg{treemisc}'s \code{proximity()} function; see \code{?treemisc::proximity} for details.

<<rf-proximity-R, echo=FALSE, eval=FALSE>>=
library(Rcpp)

cppFunction(
  "
  NumericMatrix proximity(IntegerMatrix x) {
    int nrow = x.nrow();
    NumericMatrix out(nrow, nrow);
    for (int i = 0; i < nrow; i++) {
      for (int j = i + 1; j < nrow; j++) {
        out(i, j) = sum(x(i, _) == x(j, _));
      }
    }
    return out / x.ncol();
  }
  "
)
@

% A similar example, specific to the \R{} package \pkg{ranger} \citep{R-ranger}, can be found at \url{https://mnwright.github.io/ranger/r/oob-proximity-matrix/}.


%-------------------------------------------------------------------------------
\subsection{Detecting anomalies and outliers \label{sec:rf-proximity-outliers}}
%-------------------------------------------------------------------------------

Outliers (or anomalies) are generally defined as cases that are removed from the main body of the data. In the context of an RF, Leo Breiman defined outliers as cases whose proximities to all other cases in the data are generally small. For classification, he proposed a simple measure of "outlyingness" based on the RF proximity values. Define the average proximity from case $m$ in class $j$ to the rest of the training data in class $j$ as

\begin{equation}
\nonumber
  \prox^{\star}\left(m\right) = \sum_{\text{k } \in \text{ class } j} \prox^2\left(m, k\right),
\end{equation}

where the sum is over all training instances belonging to class $j$. The outlyingness of case $m$ in class $j$ to all other cases in class $j$ is defined as 

\begin{equation}
\nonumber
  \out\left(m, j\right) = \frac{N}{\prox^{\star}\left(m\right)},
\end{equation}

where $N$ is the number of training instances. Generally, a value above 10 is reason to suspect the case of being an outlier \FIXME{Need reference}. Obviously, this measure is limited to smaller data sets and RF implementations where proximities can be efficiently computed. In Section~\ref{sec:rf-isolation-forest}, we'll look at a specialized RF extension that's more suitable for detecting outliers and anomalies, especially in higher dimensions. An interesting use case for the proximity-based outlyingness measure is presented in the next section.

%Multidimensional scaling (MDS) can be useful for visualizing multivariate data in low-dimension space, with minimal distortion. In particular, for a set of observed similarities (or proximities) between every pair of $N$ observations, MDS finds a new representation in fewer dimensions such that the projected proximities "nearly match" the original proximities; see, for example, \citep[pp.~706--715]{johnson-2007-applied}.


%-------------------------------------------------------------------------------
\subsubsection{Example: Swiss banknotes}
%-------------------------------------------------------------------------------

An interesting use case for the proximity-based outlyingness measure is in detecting potentially mislabeled response classes in classification problems. Consider, for example, the Swiss banknote data from Section~\ref{sec:intro-data-banknote}. Before fitting a default RF, I switched the label for observation 101; this observation is supposedly a counterfeit banknote (\code{y = 1}), but I switched the class label to genuine (\code{y = 0}). The proximity-based outlier scores are displayed in Figure~\ref{fig:rf-proximity-outlier-banknote}. There are two obvious potential outliers, labeled with their corresponding row number. Here, you can see that the counterfeit banknote I mislabeled as genuine (observation 101) received the largest outlier score. Observation 70 is also interesting and worth investigating; perhaps it was also mislabeled?

<<rf-proximity-outlier-banknote, echo=FALSE, par=TRUE, fig.cap="Proximity-based outlier scores for the Swiss banknote data. The largest outlier score corresponds to observation 101, which was a counterfeit banknote that was mislabeled as genuine.">>=
library(randomForest)

# Read in Swiss banknote data and mislabel one of the cases
bn2 <- treemisc::banknote
bn2$y[101] <- 0  # mislabeled case

# Fit a default RF
set.seed(2117)  # for reproducibility
bn2.rfo <- randomForest(as.factor(y) ~ ., data = bn2, proximity = TRUE)

# Compute proximity-based outlyingness measure
out <- outlier(bn2.rfo)

# Plot results
palette("Okabe-Ito")
plot(outlier(bn2.rfo), col = bn2$y + 1, las = 1, 
     xlab = "Case number", ylab = "Proximity-based outlier score")
top2 <- order(out, decreasing = TRUE)[1:2]
text(top2, out[top2], labels = top2, pos = 1)
legend("topright", legend = c("Labeled as genuine", "Labeled as counterfeit"), 
       col = 1:2, pch = 1, inset = 0.01, bty = "n")
palette("default")
@


%-------------------------------------------------------------------------------
\subsection{Missing value imputation \label{sec:rf-proximity-imputation}}
%-------------------------------------------------------------------------------

Many decision tree algorithms can can naturally handle missing values; CART and CTree, for example, employ surrogate splits to handle missing values (Section~\ref{sec:surrogate}). Unfortunately, the idea does not carry over to RFs. I suppose that makes sense: searching for surrogates would greatly increase the computation time of the RF algorithm. Although some RF software can handle missing values without casewise deletion (e.g., the \pkg{h2o} package in both \R{} and \Python{}), most often they have to be imputed or otherwise dealt with.

Breiman also developed a clever way to use RF proximities for imputing missing values. The idea is to first impute missing values with a simple method (such as using the mean or median for numeric predictors and the most common value for categorical ones). Next, fit an initial RF to the $N$ complete observations and generate the $N \times N$ proximity matrix. For a numeric feature, the initial imputed values can be updated using a weighted mean over the non-missing values where the weights are given by the proximities. For categorical variables, the imputed values are updated using the most frequent non-missing values where frequency is weighted by the proximities. Then just iterate until some convergence criterion is met (typically 4--6 runs). In other words, this method just imputes missing values using a weighted mean/mode with more weight on non-missing cases. 

\citet{breiman-2001-manual} noted that the OOB estimate of error in RFs tends to be overly optimistic when fit to training data that has been imputed. As with proximity-based outlier detection, this approach to imputation does not scale well (especially since it requires fitting multiple RFs and computing proximities). Further, this imputation method is often not as accurate as more contemporary techniques, like those implemented in the \R{} package \pkg{mice} \citep{R-mice}. 

Perhaps the biggest drawback to proximity-based imputation, like many other imputation methods, is that it only generates a single completed data set. As discussed in \citet[Chap.~1]{van-2018-flexible}, our level of confidence in a particular imputed value can be expressed as the variation across a number of completed data sets. In Section~\ref{sec:rf-ex-titanic}, we'll use the CART-based multiple imputation procedure discussed in Section~\ref{sec:brpart-imputation}, and show how we can have confidence in the interpretation of the RF output by incorporating the variability associated with multiple imputation runs.


%-------------------------------------------------------------------------------
\subsection{Unsupervised random forests \label{sec:rf-unsupervised}}
%-------------------------------------------------------------------------------

As it turns out, RFs can be used in unsupervised setting as well (i.e., when there is no defined response variable). In this case, the goal is to cluster the data; that is, see if the rows from the learning sample form any "interesting" groups.

In an unsupervised RF, the idea is formulate a two-class problem. The first class corresponds to the original data, while the second class corresponds to a synthetic data set generated from the original sample. There are two ways to generate the synthetic data corresponding to the second class \citep{randomForest2002}:

\begin{enumerate}[nosep, label={\arabic*)}]

  \item a bootstrap sample is generated from each predictor column of the original data;
  
  \item a random sample in generated uniformly from the range of each predictor column of the original data.
  
\end{enumerate}

These two data sets are then stacked on top of each other and an ordinary RF is used to build a binary classifier to try and distinguish between the real and synthetic data. (A necessary drawback here is that the resulting data set is twice as large as the original learning sample.) If the OOB misclassification error rate in the new two-class problem is, say, $\ge40$\%, then the columns look too much like independent variables in the eyes of the RF; in other words, the dependencies among the columns do not play a large role in discriminating between the two classes. On the other hand, if the OOB misclassification rate is lower, then the dependencies are playing an important role. If there is some discrimination between the two classes, then the resulting proximity matrix can be used as an input into any distance-based clustering algorithm (like $k$-means or hierarchical clustering).


%-------------------------------------------------------------------------------
\subsubsection{Example: Swiss banknotes}
%-------------------------------------------------------------------------------

Continuing with the Swiss banknote example, I generated a synthetic version of the data set using the bootstrap approach outlined in the previous section, and then stacked the data together into a two-class problem:

<<rf-unsupervised-banknote-synthetic>>=
bn <- treemisc::banknote
X.original <- subset(bn, select = -y)  # features only
X.synthetic <- X.original
set.seed(1034)
for (i in seq_len(ncol(X.original))) {
  X.synthetic[[i]] <- sample(X.synthetic[[i]], replace = TRUE)
}
X <- rbind(X.original, X.synthetic)

# Add binary indicator (doesn't)
X$y <- rep(c("original", "synthetic"), each = nrow(bn))
@

I then fit an RF of 1000 trees using the newly created binary indicator \code{y} and generated proximities for the original (i.e., first 200) observations. So how well did the unsupervised RF cluster the data? Well, we could convert the proximity matrix into a dissimilarity matrix and feed it into any distance-based clustering algorithm. Another approach, which we'll take here, is to visualize the dissimilarities using \emph{multidimensional scaling} (MDS). MDS is one of many methods for displaying (transformed) multidimensional data in a lower-dimensional space; for details, see \citet[Sec.~12.6]{johnson-2007-applied}. Essentially, MDS takes a set of dissimilarities---one minus the proximities, in this case---and returns a set of points such that the distances between the points are approximately equal to the dissimilarities. Figure~\ref{fig:rf-proximity-unsupervised-banknote-mds} shows the best-fitting two-dimensions representation. Here you can see a clear separation between the genuine bills (black) and counterfeit bills (yellow). %Observation 70, which is labeled as "genuine" and was highlighted as a potential outlier within the genuine class using the RF proximities (in supervised mode), is given by the solid blue point. 

<<rf-proximity-unsupervised-banknote-mds, echo=FALSE, par=TRUE, fig.cap="MDS coordinates of the proximities from an unsupervised RF fit to the Swiss banknote data. As can be seen, there are two noticeable clusters, although not perfectly separated. The genuine banknotes (black circles) generally fall into one cluster, while the counterfeit banknotes (yellow circles) tend to fall in the other.">>=
# Construct original and synthetic data, then combine
set.seed(1327)  # for reproducibility
bn.urfo <- randomForest(as.factor(y) ~ ., data = X, ntree = 1000,
                        proximity = TRUE)

# Compute scaling coordinates from proximities of the original observations
bn.mds <- cmdscale(1 - bn.urfo$proximity[1:200, 1:200])

# Plot scaling coordinates (include outlying case corresponding to row 70)
palette("Okabe-Ito")
plot(bn.mds, col = bn$y + 1, las = 1,
     xlab = "Scaling coordinate 1", ylab = "Scaling coordinate 2")
# points(bn.mds[70, 1], bn.mds[70, 2], pch = 19, col = 3)
legend("topleft", legend = c("Labeled as genuine", "Labeled as counterfeit"), 
       col = 1:2, pch = 1, inset = 0.01, bty = "n")
palette("default")
@

% Pull from Wiki: https://en.wikipedia.org/wiki/Random_forest#Unsupervised_learning_with_random_forests

% In unsupervised learning the data consist of a set of x -vectors of the same dimension with no class labels or response variables. There is no figure of merit to optimize, leaving the field open to ambiguous conclusions. The usual goal is to cluster the data - to see if it falls into different piles, each of which can be assigned some meaning.
% 
% The approach in random forests is to consider the original data as class 1 and to create a synthetic second class of the same size that will be labeled as class 2. The synthetic second class is created by sampling at random from the univariate distributions of the original data. Here is how a single member of class two is created - the first coordinate is sampled from the N values {x(1,n)}. The second coordinate is sampled independently from the N values {x(2,n)}, and so forth.
% 
% Thus, class two has the distribution of independent random variables, each one having the same univariate distribution as the corresponding variable in the original data. Class 2 thus destroys the dependency structure in the original data. But now, there are two classes and this artificial two-class problem can be run through random forests. This allows all of the random forests options to be applied to the original unlabeled data set.
% 
% If the oob misclassification rate in the two-class problem is, say, 40% or more, it implies that the x -variables look too much like independent variables to random forests. The dependencies do not have a large role and not much discrimination is taking place. If the misclassification rate is lower, then the dependencies are playing an important role.
% 
% Formulating it as a two class problem has a number of payoffs. Missing values can be replaced effectively. Outliers can be found. Variable importance can be measured. Scaling can be performed (in this case, if the original data had labels, the unsupervised scaling often retains the structure of the original scaling). But the most important payoff is the possibility of clustering.


%###############################################################################
\section{Prediction standard errors \label{sec:rf-se}}
%###############################################################################

\FIXME{Need to finish...ref new prediction interval paper: \citet{zhang-2020-random}.}

\FIXME{Make sure LOOCV is defined previously. Perhaps in a CV section in the intro chapter?}

Using a similar technique to OOB error estimation, \citet{wagner-2014-confidence} proposed a method for estimating the variance of an RF prediction using a technique called the \emph{jackknife} \index{Jackknife}. The jackknife procedure is very similar to LOOCV, but specifically used for estimating the variance of a statistic of interest; for details, see. If we have a statistic, $\widehat{\theta}$, estimated from $N$ training records, then the jackknife estimate of the variance of $\widehat{\theta}$ is given by:

\begin{equation}
\label{eqn:rf-jackknife-variance}
  \widehat{\V}_{jack}\left(\widehat{\theta}\right) = \frac{N - 1}{N} \sum_{i = 1} ^ N \left(\widehat{\theta}_{\left(i\right)} - \widehat{\theta}_{\left(\cdot\right)}\right) ^ 2,
\end{equation}

where $\widehat{\theta}_{\left(i\right)}$ is the statistic of interested using all the $N$ training observations except observation $i$, and $\widehat{\theta}_{\left(\cdot\right)} = \sum_{i = 1} ^ N \widehat{\theta}_{\left(i\right)} / N$.

For brevity, let $\hat{f}\left(\boldsymbol{x}\right) = \hat{f}_B^{rf}\left(\boldsymbol{x}\right)$, for some arbitrary observation $\boldsymbol{x}$ (see Algorithm~\ref{alg:rf}). A natural jackknife variance estimate for the RF prediction $\hat{f}\left(\boldsymbol{x}\right)$ is given by

\begin{equation}
\label{eqn:rf-prediction-jackknife-variance}
  \widehat{\V}_{jack}\left(\hat{f}\left(\boldsymbol{x}\right)\right) = \frac{N - 1}{N} \sum_{i = 1} ^ N \left(\hat{f}_{\left(i\right)}\left(\boldsymbol{x}\right) - \hat{f}\left(\boldsymbol{x}\right)\right) ^ 2.
\end{equation}

This is derived under the assumption that $B = \infty$ trees were averaged together in the forest, which of course, is never the case. Consequently, \eqref{eqn:rf-prediction-jackknife-variance} has a positive bias. Fortunately, the same $B$ bootstrap samples used to derive the forest can also be used to provide the bias corrected variance estimate

\begin{equation}
\label{eqn:rf-prediction-jackknife-variance-corrected}
  \widehat{\V}_{jack}^{BC}\left(\hat{f}\left(\boldsymbol{x}\right)\right) = \widehat{\V}_{jack}\left(\hat{f}\left(\boldsymbol{x}\right)\right) - \left(e - 1\right)\frac{N}{B}\widehat{v}\left(\boldsymbol{x}\right),
\end{equation}

where $e = 2.718...$ is Euler's constant and 

\begin{equation}
  \widehat{v}\left(\boldsymbol{x}\right) = \frac{1}{B} \sum_{b = 1} ^ B \left(\hat{f}_b\left(X\right) - \hat{f}\left(\boldsymbol{x}\right)\right) ^ 2
\end{equation}

is the bootstrap estimate of the variance of a prediction from a single RF tree. Fortunately, all of the required quantities for computing \eqref{eqn:rf-prediction-jackknife-variance-corrected} are readily available in the output from most \opensource{} RF software. This procedure is implemented in the \R{} packages \pkg{ranger} and \pkg{grf} \citep{R-grf}; the latter is a pluggable package for nonparametric statistical estimation and inference based on RFs, also known as \empg{generalized random forests} \citep{athey-2019-grf}. The \pypkg{forestci} package \citep{polimis-2017-forestci} provides a \Python{} implementation compatible with scikit-learn RF objects.


%-------------------------------------------------------------------------------
\subsection{Example: predicting email spam}
%-------------------------------------------------------------------------------

Switching back to the email spam data, let's compute jackknife-based standard errors for the test set predicted class probabilities. Following \citet{wagner-2014-confidence}, I fit an RF using $B = 20000$ trees and three different values for $m_{try}$: 5, 19 (based on Breiman's default for classification), and 57 (an ordinary bagged tree ensemble). 

The predicted class probabilities for \code{type = "spam"}, based on the test data, from each RF are displayed in Figure~\ref{fig:rf-se-spam-plot} ($x$-axis), along with their bias-corrected jackknife estimated standard errors ($y$-axis). Notice how the misclassified cases (solid black points) tend to correspond to observations where the predicted class probability is closer to 0.5. It also appears that the more constrained RF with $m_{try} = 5$ produced smaller standard errors, while the default RF ($m_{try} = 19$) and bagged tree ensemble ($m_{try} = 57$) produced noticeably larger standard errors, with the bagged tree ensemble performing the worst. \FIXME{Any intuition as to why?}

<<rf-se-spam-plot, echo=FALSE, fig.asp=1/2, fig.cap="Bias-corrected jackknife variance estimates for the predicted probabilities using the spam training data. The solid black points correspond to the misclassified cases which appear to be more closely concentrated near the center (i.e., 0.5).">>=
spam.rfos.preds <- readRDS("../data/chap-rf-se-spam-preds.rds")
mtry <- c(5, 19, 57)

# Load the email spam data and split into train/test sets using a 70/30 split
data(spam, package = "kernlab")
set.seed(1258)  # for reproducibility
trn.id <- sample(nrow(spam), size = 0.7 * nrow(spam), replace = FALSE)
spam.trn <- spam[trn.id, ]
spam.tst <- spam[-trn.id, ]

par(mar = c(4, 4, 2, 0.1), cex.lab = 0.95, cex.axis = 0.8, 
    mgp = c(2, 0.7, 0), tcl = -0.3, las = 1, mfrow = c(1, 3))

# Plot RF test set prediction standard errors, colored by whether or not the 
# observations were misclassified
ylim <- range(sapply(spam.rfos.preds, FUN = function(x) {
  range(x$se[, "spam"])
}))
palette("Okabe-Ito")
for (i in seq_along(spam.rfos.preds)) {
  pred <- spam.rfos.preds[[i]]
  classes <- ifelse(pred$predictions[, "spam"] > 0.5, "spam", "nonspam")
  id <- (classes == spam.tst$type) + 1
  plot(pred$predictions[, "spam"], pred$se[, "spam"], col = id, 
       pch = c(19, 1)[id], main = paste("mtry =", mtry[i]), 
       ylim = c(0, max(ylim)), xlab = "Predicted probablitiy", 
       ylab = "Standard error")
  if (i == 1) {
    legend("topleft", legend = c("Misclassified"), pch = 19, col = 1,
           inset = 0.01, bty = "n")
  }
}
palette("default")
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Random forest extensions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following subsections highlight some notable extensions to the RF algorithm. Note that this is by no means an exhaustive list, so I tried to choose extensions that have been shown to be practically useful, and that also have available (and currently maintained) \opensource{} implementations.


%-------------------------------------------------------------------------------
\subsection{Oblique random forests}
%-------------------------------------------------------------------------------

As briefly mentioned in Section~\ref{sec:guide-linear-splits}, linear (or oblique) splits---which are splits based on linear combinations of the predictors---can sometimes improve predictive accuracy, even if they do make the tree harder to interpret. Many decision tree algorithms support linear splits (e.g., CART and GUIDE) and \citet{breiman-2001-rf} even proposed a variant of RFs that employed linear splits based on random linear combinations of the predictors. This approach did not gain the same traction as the traditional RF algorithm based on univariate splits. In fact, I'm not aware of any \opensource{} RF implementations that support his original approach based on random coefficients.

\citet{bjoern-2011-oblique} proposed a variant, called \emph{oblique random forests} (oRFs), that explicitly learned optimal split directions at internal nodes using linear discriminative models\footnote{Similar to the LDA-based approach used in GUIDE (Section~\ref{sec:guide-linear-splits}). However, GUIDE restricts itself to linear splits in only two features at a time to help with interpretation and reduce the impact of missing values.}, as opposed to random linear combinations. Similar to \emph{random rotation forests} (Section~\ref{sec:rotation-forests}), oRFs tend to have a smoother topology; see, for example, Figure~\ref{fig:rf-mixture-rfos}. \citet{bjoern-2011-oblique} even go as far as to recommend the use of oRFs over the traditional RF when applied to mostly numeric features. Nonetheless, the idea of non-axis oriented splits in RFs has still not caught on. The only \opensource{} implementation of oRFs that I'm aware of is in the \R{} package \pkg{obliqueRF} \citep{R-obliqueRF}, which has not been updated since 2012.

A more recent approach, called \emph{projection pursuit random forest} (PPforest) \citep{silva-2021-ppf} uses splits based on a linear combinations of randomly chosen inputs. Each linear combination is found by optimizing a projection pursuit index \citep{friedman-1974-pp} to get a projection of the features that best separates the classes; hence, this method is also only suitable for classification. PPforests are implemented in the \R{} package \pkg{PPforest} \citep{R-PPforest}. Individual \emph{projection pursuit trees} (PPtrees) \citep{lee-2013-pptree}, which are used as the base learners in a PPforest, can be fit using the \R{} package \pkg{PPtreeViz} \citep{R-PPtreeViz} (which seems to have superseded the older \pkg{PPtree} package.


%-------------------------------------------------------------------------------
\subsection{Case-specific random forests \label{sec:rf-csrf}}
%-------------------------------------------------------------------------------

The \emph{case-specific RF} \citep{xu-2016-csrf} is another interesting application of RF proximities (Section~\ref{sec:rf-proximities}. The idea is to build a new RF to more accurately predict each individual observation in the test set. The individual RFs give more weight to the training observations that have higher proximity to the observations in the test set. 

Let $\boldsymbol{d}_{trn}$ and $\boldsymbol{d}_{tst}$ be the train and test data sets with $N_{trn}$ and $N_{tst}$ observations, respectively. The general algorithm for growing a case-specific RF is as follows:

\begin{enumerate}[label={\arabic*)}]

  \item Grow an ordinary RF of size $B$ to the training data $\boldsymbol{d}_{trn}$.
  
  \item For each observation in the test set, say, $\boldsymbol{x}_0$, do the following:
  
  \begin{enumerate}[label={\alph*)}]
  
    \item Compute the proximities between $\boldsymbol{x}_0$ and each observation in $\boldsymbol{d}_{trn}$ using the initial RF from Step 1); let $\left\{\prox_i\left(\boldsymbol{x}_0\right)\right\}_{i = 1}^{N_{trn}}$ be the proximities between $\boldsymbol{x}_0$ and each case from $\boldsymbol{d}_{trn}$; that is, the fraction of times $\boldsymbol{x}_0$ cohabitates with each training instance across the $B$ trees in the initial RF.
    
    \item Define the case weight for training case $i$ relative to $\boldsymbol{x}_0$ as $w_i^\star = \prox_i\left(\boldsymbol{x}_0\right) / \sum_{i = 1}^{N_{trn}} \prox_i\left(\boldsymbol{x}_0\right)$.
    
    \item Predict $\boldsymbol{x}_0$ with a new RF grown to $\boldsymbol{d}_{trn}$ using case weights $\left\{w_i^\star\right\}_{i = 1}^N$.
    
  \end{enumerate}

\end{enumerate}

In essence, a case-specific RF predicts a new case $\boldsymbol{x}_0$ using a new RF that gives more weight to the original training observations that have higher proximity (i.e., are more similar) to $\boldsymbol{x}_0$. Note that most \opensource{} RF software provide the option to specify case weights for the training observations, which are used to weight each row when taking bootstrap samples (but many implementations do not provide proximities). While the idea of case-specific RFs makes sense, it has a couple of limitations. First off, it requires fitting $N_{tst} + 1$ RFs, which can be expensive whenever $N_{trn}$ or $N_{tst}$ are large. Second, it requires computing $N_{trn} \times N_{tst}$ proximities from an RF, which aren't always available from software.

Case-specific RFs are relatively straight forward to implement with traditional RF software, provided you can compute proximity scores\footnote{Even if you don't have access to an implementation of RFs that can compute proximities, they're still obtainable as long as you can compute terminal node assignments for new observations (i.e., compute which terminal node a particular observation falls in for each of the $B$ trees), which is readily available in most RF software. See Section~\ref{sec:rf-proximities} for details.}. The \R{} package \pkg{ranger} provides an implementation of case-specific RFs. I applied this methodology to the Ames housing example (Section~\ref{sec:intro-data-ames}), which actually resulted in a slight increase to the test RMSE when compared to a traditional RF; the code to reproduce the example is available on the companion website for this book.

<<rf-csrf-ames, echo=FALSE, eval=FALSE>>=
library(ranger)

# Read in Ames housing data and split into train/test sets
ames <- as.data.frame(AmesHousing::make_ames())
set.seed(2101)  # for reproducibility
trn.id <- sample.int(nrow(ames), size = floor(0.7 * nrow(ames)))
ames.trn <- ames[trn.id, ]
ames.tst <- ames[-trn.id, ]

# Compute test set predictions from an RF and CSRF
set.seed(1645)  # for reproducibility
rfo <- ranger(Sale_Price ~ ., data = ames.trn)
rfo.pred <- predict(rfo, data = ames.tst)$predictions
# csrfo.pred <- csrf(Sale_Price ~ ., training_data = ames.trn,
#                    test_data = ames.tst, verbose = TRUE)

# Read in previously generate results
csrfo.pred <- readRDS("../data/rf-csrf-ames.rds")

# Mean squared error
mean((rfo.pred - ames.tst$Sale_Price) ^ 2)
mean((csrfo.pred - ames.tst$Sale_Price) ^ 2)

# Mean absolute error
mean(abs(rfo.pred - ames.tst$Sale_Price))
mean(abs(csrfo.pred - ames.tst$Sale_Price))
@


%-------------------------------------------------------------------------------
\subsection{Quantile regression forests}
%-------------------------------------------------------------------------------

The goal of many supervised learning algorithms is to infer something about the relationship between the response and a set of predictors. In regression, for example, the goal is often to estimate the conditional mean $\E\left(y | \boldsymbol{x}\right)$, for some observation $\boldsymbol{x}$. In a typical regression tree, an estimate of the conditional mean is given by the mean response of the terminal node observation $\boldsymbol{x}$ falls into. In an RF, the terminal node means are simply averaged across all the trees in the forest.

The conditional mean response, however, provides only a limited summary of the conditional distribution function $F\left(y | \boldsymbol{x}\right) = Pr\left(Y \le y | \boldsymbol{x}\right)$. Denote the $\alpha$-quantile of $Y | \boldsymbol{x}$ as $Q_\alpha\left(\boldsymbol{x}\right)$. In other words, $\Pr\left(Y \le y |  Q_\alpha\left(\boldsymbol{x}\right)\right) = \alpha$. Compared to the conditional mean, the quantiles give a more useful summary of the distribution. For example, $\alpha = 0.5$ corresponds to the median. If the conditional distribution of $Y | \boldsymbol{x}$ were symmetric, then the conditional mean and median would be the same. However, if $Y | \boldsymbol{x}$ is skewed, then, compared to the conditional median, the conditional mean can be a misleading summary of what a typical value of $Y | \boldsymbol{x}$ is. Furthermore, estimating $\Pr\left(Y \le y | Q_\alpha\left(\boldsymbol{x}\right)\right)$ for various values of $\alpha$ can give insight into the variability around a single point estimate, like the conditional median. This is the idea behind \emph{quantile regression}.

The same idea can applied to an RF, and was formerly introduced in \citet{meinshausen-2006-quantile} as \emph{quantile regression forests} (QRF). In a QRF, the conditional distribution of $Y | \boldsymbol{x}$ is approximated by the weighted mean

\begin{equation}
\nonumber
\widehat{F}\left(y | \boldsymbol{x}\right) = \sum_{i = 1}^N w_i\left(\boldsymbol{x}\right) I\left(Y_i \le y\right),
\end{equation}

where $I\left(expression\right)$ is the indicator function that evaluates to one whenever $expression$ is true, and zero otherwise. The weights $w_i\left(\boldsymbol{x}\right)$ are estimated from the terminal node observations across all the tress in the forest and are defined in \citet{meinshausen-2006-quantile}. In contrast to a traditional regression forest, this requires storing all the observations in each node, as opposed to just the mean.

As it turns out, there's not much difference between QRFs and RFs, aside from what information gets stored from each tree, and how fitted values and predictions are obtained. RFs only need to keep track of the terminal node means. To estimate the full conditional distribution of $Y | \boldsymbol{x}$, QRFs need to retain all observations across all terminal nodes. 


%-------------------------------------------------------------------------------
\subsubsection{Example: Predicting sale prices with prediction intervals}
%-------------------------------------------------------------------------------

<<rf-qrf-ames-fit, cache=TRUE, echo=FALSE>>=
library(ranger)

# Read in Ames housing data and split into train/test sets using a 70/30 split
ames <- as.data.frame(AmesHousing::make_ames())
ames$Sale_Price <- ames$Sale_Price / 1000
set.seed(2101)  # for reproducibility
trn.id <- sample.int(nrow(ames), size = floor(0.7 * nrow(ames)))
ames.trn <- ames[trn.id, ]
ames.tst <- ames[-trn.id, ]

# Fit a quantile regression forest
set.seed(1235)  # for reproducibility
rfo <- ranger(Sale_Price ~ ., data = ames.trn)
qrfo <- ranger(Sale_Price ~ ., data = ames.trn, quantreg = TRUE)

# Find most expensive house in test set
id <- which.max(ames.tst$Sale_Price)
pred.rfo <- predict(rfo, data = ames.tst[id, ])$predictions
pred.qrfo <- predict(qrfo, data = ames.tst[id, ], type = "quantiles",
                     quantiles = c(0.025, 0.5, 0.975))$predictions
@

To illustrate, I fit a traditional RF and a QRF to the Ames housing data using the same train/test split discussed in Section~\ref{sec:intro-data-ames}; similar to before, I used \code{Sale\_Price / 1000} as the response. For each house in the test set, the predicted 0.025, 0.5, and 0.975 quantiles were obtained (which corresponds to a predicted median sale price and 95\% prediction interval). Following the Boston housing example in \citet{meinshausen-2006-quantile}, the test set observations were ordered according to the length of the corresponding prediction intervals, and each observation was centered by subtracting the midpoint from the corresponding prediction interval. The observed sale prices are shown in black and the estimated conditional medians are given shown in yellow. The black lines show the corresponding 95\% prediction bounds. (Note that the prediction intervals here are pointwise prediction intervals.)

The most expensive house in the test set sold for a (rescaled) sale price of \Sexpr{scales::dollar(ames.tst[id, "Sale_Price"], prefix = "\\$")}. A traditional RF estimated a conditional mean sale price of \Sexpr{scales::dollar(pred.rfo, prefix = "\\$")}, whereas the QRF produced a conditional median sale price of \Sexpr{scales::dollar(pred.qrfo[1, 2], prefix = "\\$")} with a 0.025 quantile of \Sexpr{scales::dollar(pred.qrfo[1, 1], prefix = "\\$")} and a 0.975 quantile of \Sexpr{scales::dollar(pred.qrfo[1, 3], prefix = "\\$")}. Here, the QRF gives a much better sense of the variability in the predicted outcome, as well as a sense of the skewness of its distribution.

<<rf-qrf-ames-prediction, cache=TRUE, echo=FALSE, par=TRUE, fig.cap="Rescaled sale prices for each home in the test set, along with the predicted 0.025, 0.5, and 0.975 quantiles from a QRF. To enhance visualization, the observations were ordered according to the length of the corresponding prediction intervals, and the mean of the upper and lower end of the prediction interval is subtracted from all observations and prediction intervals.">>=
# Compute predictions for different quantiles
pred <- predict(qrfo, data = ames.tst, type = "quantiles", 
                quantiles = c(0.025, 0.5, 0.975))

# Sort, center, and add other relevant columns
pred <- pred$predictions
pred <- cbind(pred, "length" = pred[, 3] - pred[, 1])
pred <- cbind(pred, "y" = ames.tst$Sale_Price)
ord <- order(pred[, "length"], decreasing = FALSE)
pred <- pred[ord, ]
means <- (pred[, 1] + pred[, 3]) / 2
pred[, 1] <- pred[, 1] - means
pred[, 2] <- pred[, 2] - means
pred[, 3] <- pred[, 3] - means
pred[, "y"] <- pred[, "y"] - means

# Plot results (similar to Figure 3 of Meinshausen (2006))
res <- as.data.frame(pred)
res$index <- seq_len(nrow(res))
ylim <- range(res[, 1:3])
palette("Okabe-Ito")
plot(y ~ index, data = res, xlab = "Ordered observation number", 
     ylab = "Sale price / 1000 (centered)", col = 2, ylim = ylim)
points(res$index, res$`quantile= 0.5`, col = 3)
lines(res$index, res$`quantile= 0.025`, col = 1)
lines(res$index, res$`quantile= 0.975`, col = 1)
legend("topleft", inset = 0.01, bty = "n", pch = c(1, 1, NA), 
       lty = c(NA, NA, 1), col = c(2, 3, 1),
       legend = c("Observed", "Predicted median", "Prediction internal"))
palette("default")
# cols <- palette.colors(3, palette = "Okabe-Ito")
# ggplot(res, aes(x = index, y = y)) + 
#   geom_point(color = cols[3], alpha = 0.5) +
#   geom_point(aes(x = index, y = `quantile= 0.5`), 
#              color = cols[2], alpha = 0.3) +
#   geom_line(aes(x = index, y = `quantile= 0.025`), color = cols[1]) +
#   geom_line(aes(x = index, y = `quantile= 0.975`), color = cols[1]) +
#   xlab("Ordered observation number") +
#   ylab("Sale price (centered)") +
#   theme_bw()
@


%-------------------------------------------------------------------------------
\subsection{Rotation forests and random rotation forests \label{sec:rotation-forests}}
%-------------------------------------------------------------------------------

%In this sense, PCA can be thought of as a feature extraction method. By performing PCA on random subsets of features prior to fitting each tree, rotation forests can result in increased performance to bagging.

Before talking about rotation forests and random rotation ensembles, it might help to briefly discuss \emph{rotation matrices}. A rotation matrix $\boldsymbol{R}$ of dimension $p$ is a $p \times p$ square transformation matrix that's used to perform a rotation in $N$-dimensional Euclidean space. 

A common application of rotation matrices in statistics is \emph{principal component analysis} (PCA). The details of PCA are beyond the scope of this book, but the interested reader is pointed to \citet[Chap. 8]{johnson-2007-applied}, among others. While PCA has many use cases, it is really just an unsupervised dimension reduction technique that seeks to explain the variance-covariance structure of a set of variables through a few linear combinations of these variables. 

To illustrate, consider the $N = 100$ data points shown in Figure~\ref{fig:rf-rotations} (left); the axes for $X_1$ and $X_2$ are shown using dashed yellow and blue lines, respectively. The data were generated from a simple linear regression defined by
\begin{equation}
\label{eqn:rf-random-rotations}
  X_{2i} = X_{1i} + \epsilon_i, \quad i = 1, 2, \dots, 100,
\end{equation}
where $X_{1i} \stackrel{iid}{\sim} \mathcal{U}\left(0, 1\right)$ and $\epsilon_i \stackrel{iid}{\sim} N\left(0, 1\right)$. Further, let $\boldsymbol{X}$ be the $100 \times 2$ matrix whose first and second columns are given by $X_{1i}$ and $X_{2i}$, respectively. As a rotation in two dimensions, PCA finds the rotation of the axes that yields maximum variance. The rotated axes for this example are shown in Figure~\ref{fig:rf-rotations} (middle). Notice that the first (i.e., yellow) axis is aligned with with the direction of maximum variance in the sample. An alternative would be to rotate the data points themselves (right side of Figure~\ref{fig:rf-rotations}). In this case, the variable loadings from PCA form a $2 \times 2$ rotation matrix, $\boldsymbol{R}$, that can be used to rotate $\boldsymbol{X}$ so that the direction of maximal variance aligns with the first (i.e., yellow) axis; this is shown in the right side of Figure~\ref{fig:rf-rotations}. The rotated matrix is given by $\boldsymbol{X}' = \boldsymbol{X}\boldsymbol{R}$. Notice how the relative position of the points between $X_1$ and $X_2$ is preserved, albeit rotated about the axes.

<<rf-rotations, echo=FALSE, fig.asp=1/3, fig.width=7, fig.cap="Data generated from a simple linear regression model. Left: Original data points and axes. Middle: Original data with rotated axes (notice how the first/yellow axis aligns with the direction of maximum variance in the sample). Right: Rotated data points on the original axes (here the data are rotated so that the direction of maximal variance aligns with the first/yellow axis).">>=
# Generate data
set.seed(1038)  # for reproducibility
x1 <- runif(100, min = -5, max = 5)
x2 <- x1 + rnorm(length(x1))
X <- cbind(x1, x2)
#R <- loadings(princomp(X, cor = FALSE, fix_sign = FALSE))  # rotation matrix from PCA
R <- eigen(cov(X))$vectors  # same as above
XR <- X %*% R
colnames(XR) <- colnames(X)

# Wrangle data for plotting
d1 <- rbind(
  cbind(as.data.frame(X), "label" = "Original axes"),
  cbind(as.data.frame(X), "label" = "Rotated axes"),
  cbind(as.data.frame(XR), "label" = "Rotated points")
)
d2 <- data.frame(
  "label" = c("Original axes", "Rotated axes", "Rotated points"),
  intercept = c(0, 0, 0),
  slope = c(0, R[2, 1] / R[1, 1], 0)
)
d3 <- data.frame(
  "label" = c("Original axes", "Rotated axes", "Rotated points"),
  intercept = c(0, 0, 0),
  slope = c(1e+06, R[2, 2] / R[1, 2], 1e+06)
)

# Color palette
cols <- unname(palette.colors(3, palette = "Okabe-Ito"))

# Plot data
ggplot(d1, aes(x1, x2)) +
  geom_point(color = cols[1], alpha = 0.3) +
  geom_abline(data = d2, aes(intercept = intercept, slope = slope), 
              linetype = 2, color = cols[2]) +
  geom_abline(data = d3, aes(intercept = intercept, slope = slope), 
              linetype = 2, color = cols[3]) +
  xlab(expression(X[1])) +
  ylab(expression(X[2])) +
  coord_fixed() +
  facet_wrap(~ label, nrow = 1) +
  theme_bw()
# set.seed(1038)  # for reproducibility
# x <- runif(100, min = -5, max = 5)
# y <- x + rnorm(length(x))
# X <- cbind(x, y)
# R <- loadings(princomp(X, cor = FALSE))  # rotation matrix from PCA
# par(mfrow = c(1, 3), las = 1)
# pt.col <- adjustcolor("purple2", alpha.f = 0.5)
# plot(X, xlim = c(-8, 8), ylim = c(-8, 8), 
#      col = pt.col, asp = 1,
#      xlab = expression(X[1]), ylab = expression(X[2]),
#      main = "Original axes")
# abline(h = 0, v = 0, lty = 2)
# plot(X, xlim = c(-8, 8), ylim = c(-8, 8), 
#      col = pt.col, asp = 1,
#      xlab = expression(X[1]), ylab = expression(X[2]),
#      main = "Rotation of axes")
# abline(a = 0, b = R[2, 1] / R[1, 1], lty = 2)
# abline(a = 0, b = R[2, 2] / R[1, 2], lty = 2)
# plot(X %*% R, xlim = c(-8, 8), ylim = c(-8, 8), 
#      col = pt.col, asp = 1,
#      xlab = expression(X[1]), ylab = expression(X[2]),
#      main = "Rotation of points")
# abline(h = 0, v = 0, lty = 2)
@

So what does any of this have to do with RFs? Recall that the key to accuracy with model averaging is diversity. In an RF, diversity is achieved by choosing a random subset of predictors prior to each split in every tree. A \emph{rotation forests} \citep{rodriguez-2006-rotation}, on the other hand, introduces diversity to a bagged tree ensemble by using PCA to construct a rotated feature space prior to the construction of each tree. Rotating the feature space allows adaptive nonparametric learning algorithms, like decision trees, to learn potentially interesting patterns in the data that might have gone unnoticed in the original feature space. Applying PCA to all the predictors prior to the construction each tree, even when using sampling with replacement, won't be enough to diversify the ensemble. Instead, prior to the construction of each tree, the predictor set is randomly split into $K$ subsets, PCA is run separately on each, and a new set of linearly extracted features is constructed by pooling all the principal components (i.e., the rotated data points). $K$ is treated as a tuning parameter, but the value of $K$ that results in roughly three features per subset seems to be the suggested default \citep{ludmila-2007-experimental}. Rotation forests can be thought of as a bagged tree ensemble with a random feature transformation applied to the predictors prior to constructing each tree. In this sense, PCA can be thought of as a feature extraction method. By performing PCA on random subsets of features prior to fitting each tree, rotation forests can improve the performance of a begged tree ensemble. In this case, the derived features come from PCA applied to random subsets of the data, and while other feature extraction methods have also been considered, PCA was found to be the most suitable \citep{ludmila-2007-experimental}. 

Rotation forests have been shown to be competitive with RFs, and can achieve better performance on data sets with mostly quantitative variables; although, this seems to be true mostly for smaller ensemble sizes \citep{rodriguez-2006-rotation, ludmila-2007-experimental}. However, most comparative studies I've seen seem to focus on classification accuracy for comparison, which we know is not the most appropriate metric for comparing models in classification settings. 


%-------------------------------------------------------------------------------
\subsubsection{Random rotation forests}
%-------------------------------------------------------------------------------

A similar approach, called \emph{random rotation ensembles} \citet{blaser-2016-random}, apply a random rotation to all the features prior to constructing each tree. \citet{blaser-2016-random} discuss two algorithms for generating random rotation matrices, and provide general \R{} and C++ code for doing so. The \pkg{treemisc} function \code{rrm()} uses the \emph{indirect method} discussed in \citet{blaser-2016-random} and is shown below. Note that rotations are only applied to numeric features and can be sensitive to both scale and outliers, hence, rescaling the numeric features is often required; see \citet{blaser-2016-random} for several recommendations.

Let $\boldsymbol{X}_c$ be the subset of numeric/continuous features from the full feature set $\boldsymbol{X}$. In a random rotation forest, before fitting the $b$-th tree, the numeric features are randomly transformed using $\boldsymbol{X}_{c, b} = \boldsymbol{X}_c \boldsymbol{R}_b$ (for $b = 1, 2, \dots, B$), where $\boldsymbol{R}_b$ is a randomly generated rotation matrix of dimension equal to the number of columns of $\boldsymbol{X}_c$.

<<rf-random-rotation-code>>=
treemisc::rrm
@

To illustrate the effect of applying random rotations to a set of features, let's continue with the simulated data from the previous section. In the code chunk below, I re-generate the same $N = 100$ points from \eqref{eqn:rf-random-rotations}; the original data are displayed in Figure~\ref{fig:rf-random-rotations} (black points), along with the observations under various random rotations, including PCA (orange points). Such rotations preserve the inter-relationships between predictors, but cast them into a different space resulting in equally accurate, but more diverse trees.

<<rf-random-rotations, fig.asp=1, fig.cap="Scatterplot of $X_1$ versus $X_2$. The original data are shown in black and a dashed black line gives the direction of maximum variance. The rotated points under PCA are shown in dark yellow along with the new axis of maximal variance; notice how in two dimensions this shifts the points to have maximal variance along the $x$-axis. The rest of the colors display the data points under random rotations.">>=
set.seed(1038)
X1 <- runif(100, min = -5, max = 5)
X2 <- X1 + rnorm(length(X1))
X <- cbind(X1, X2)
palette("Okabe-Ito")  # colorblind-friendly color palette
plot(X, xlim = c(-8, 8), ylim = c(-8, 8), col = 1, las = 1,
     xlab = expression(X[1]), ylab = expression(X[2]))
pcR <- loadings(princomp(X, cor = FALSE, fix_sign = FALSE))  # PCA
points(X %*% pcR, col = 2)  # plot PCA rotation
abline(0, 1, lty = 2, col = 1)  # original axis
abline(h = 0, lty = 2, col = 2)  # axis after PCA rotation
for (i in 3:5) {  # plot random rotations
  R <- treemisc::rrm(2)  # generate a random 2x2 rotation matrix 
  points(X %*% R, col = adjustcolor(i, alpha.f = 0.5))
}
legend("topleft", legend = "Original sample", pch = 1, col = 1, 
       inset = 0.01, bty = "n")
palette("default")
@

%-------------------------------------------------------------------------------
\subsubsection{Example: Gaussian mixture data}
%-------------------------------------------------------------------------------

In this section, we'll use the Gaussian mixture data from \citet{hastie-2009-elements} to compare the results of an RF, rotation forest, and random rotation forest. The data for each class come from a mixture of ten normal distributions, whose individual means means are also normally distributed. A full description of the data generating process is can be found in \citet[Sec.~2.3.3]{hastie-2009-elements} and an application to RFs is provided in \citet[Sec.~15.4.3]{hastie-2009-elements}. The raw data are available at \url{https://web.stanford.edu/~hastie/ElemStatLearn/data.html}. For convenience, the data are also available in the \pkg{treemisc} \R{} package that accompanies this book, and can be read in using:

<<rf-mixture>>=
library(treemisc)

eslmix <- load_eslmix()
class(eslmix)  # should be a list
names(eslmix)  # names of components
@

Note that this is not a data frame, but rather a list with several components; for a description of each, see \code{?treemisc::load\_eslmix}.

The code chunk below constructs a scatterplot of the training data (i.e., component \code{x}) along with the Bayes decision boundary\footnote{As noted in \citet[Chap.~2]{hastie-2009-elements}, since the data generating mechanism is known for each of thew two classes, the theoretically optimal decision boundary can be computed exactly. This makes it useful to compare classifiers visually in terms of their estimated decision boundaries.}; see Figure~\ref{fig:rf-mixture-bayes}. The Bayes error rate for these data---that is, the theoretically optimal error rate---is 0.210.

<<rf-mixture-bayes, fig.cap="Simulated mixture data with optimal (i.e., Bayes) decision boundary.">>=
x <- as.data.frame(eslmix$x)  # training data
xnew <- as.data.frame(eslmix$xnew)  # evenly spaced grid of points
x$y <- as.factor(eslmix$y)  # coerce to factor for plotting 
xnew$prob <- eslmix$prob  # Pr(Y = 1 | xnew) 

# Colorblind-friendly palette
oi.cols <- unname(palette.colors(8, palette = "Okabe-Ito"))

# Construct scatterplot of training points
p <- ggplot(x, aes(x = x1, y = x2, color = y)) +
  geom_point(alpha = 1, show.legend = FALSE) + 
  scale_colour_manual(values = oi.cols) +
  theme_bw()

# Add optimal (i.e., Bayes) decision boundary
p + geom_contour(data = xnew, aes(x = x1, y = x2, z = prob), 
                 breaks = 0.5, color = oi.cols[4], 
                 inherit.aes = FALSE, linetype = 2)
@

Next I fit three tree ensembles: a traditional RF, a rotation forest, and a random rotation forest. The rotation forest was fit using the \pkg{rotationForest} package \citep{R-rotationForest}, while the RF and random rotation forest were fit using \pkg{treemisc}'s \code{rforest()} function. Note that this is a poor man's implementation of Breiman's RF algorithm I wrote that optionally rotates the features at random prior to the construction of each tree. It is based on the well-known \pkg{randomForest} package \citep{R-randomForest} and is missing many of the bells and whistles, hence not recommended for general use, but it works. Also, this implementation only uses regression trees for the base learners, hence, only regression and binary classification are supported. For the latter, the probability machine approach discussed in Section~\ref{sec:rf-prob} is implemented. 

The resulting decision boundaries from each forest are in Figure~\ref{fig:rf-mixture-rfos}. Here, you can see that the axis-oriented nature of the individual trees in a traditional RF leads to a decision boundary with an axis-oriented flavor (i.e., the decision boundary is rather "boxy"). The RF also exhibits more signs of overfitting, as suggested by the little islands of decision boundaries. On the other hand, using feature rotation (with PCA or random rotations) prior to building each tree results in a noticeably smoother and non-axis-oriented decision boundary. The test error rates for the RF, rotation forest, and random rotation forest, under this random seed, are 0.235, 0.239, and 0.226, respectively. (As always, the code to reproduce this example is available on the companion website for this book.)

<<rf-mixture-rfos, cache=TRUE, echo=FALSE, fig.cap="Traditional RF versus random rotation forest on the mixture data from \\citet{hastie-2009-elements}. The random rotation forest produces a noticeably smoother decision boundary than the axis-oriented decision boundary from a traditional RF.">>=
library(rotationForest)
library(rpart)

# Fit an RF with and without random rotations
set.seed(2200)  # for reproducibility
rfo1 <- rforest(eslmix$x, y = eslmix$y, ntree = 1000, nodesize = 10)
rfo2 <- rotationForest(eslmix$x, y = eslmix$y, L = 1000)  # rotation forest
rfo3 <- rforest(eslmix$x, y = eslmix$y, ntree = 1000, nodesize = 10, 
                rotate = TRUE)  # random rotation forest

# Compute predicted probabilities (i.e., Pr(Y = 1)) for each method and stack
# into a single data frame
res <- lapply(1:3, FUN = function(x) {  # one copy for each model
  as.data.frame(eslmix$xnew)
})
res[[1]]$prob <- predict(rfo1, newX = eslmix$xnew) 
res[[2]]$prob <- predict(rfo2, newdata = as.data.frame(eslmix$xnew)) 
res[[3]]$prob <- predict(rfo3, newX = eslmix$xnew)  
res[[1]]$method <- "Random forest"
res[[2]]$method <- "Rotation forest"
res[[3]]$method <- "Random rotation forest"
res <- do.call(rbind, args = res)

# Plot estimated decision boundary from each model
ggplot(x, aes(x = x1, y = x2, color = y)) +
  geom_point(alpha = 0.7, show.legend = FALSE) + 
  scale_colour_manual(values = oi.cols) +
  theme_bw() +
  geom_contour(data = res, aes(x = x1, y = x2, z = prob), breaks = 0.5, 
               color = oi.cols[3], inherit.aes = FALSE) +
  facet_wrap(~ method) + 
  geom_contour(data = xnew, aes(x = x1, y = x2, z = prob), breaks = 0.5,
               color = oi.cols[4], inherit.aes = FALSE, linetype = 2)
@


%-------------------------------------------------------------------------------
\subsection{Extremely randomized trees \label{sec:rf-extratrees}}
%-------------------------------------------------------------------------------

Just as RFs offer an additional layer of randomization to bagged decision trees, \emph{extremely randomized trees} (or extra-trees) citep{guerts-2006-extratrees} are essentially an RF with an additional randomization step. In particular, the split point for any feature at each node in a tree is essentially selected at random from a uniform distribution. %Another subtle difference from bagging and random forests is that an extraTrees ensemble is that each tree is grown on the entire learning sample, as opposed to a bootstrap sample (i.e., sample with replacement).
The extra randomization can further decrease variance, but sometimes at the cost of additional bias \citep{geurts-2006-xtrees}; this is especially true if the data contain irrelevant features. To combat the extra bias, extra-trees utilize the full learning sample to grow each tree, rather than bootstrap replicas (another subtle difference from the bagging and RF algorithms). Note that bootstrap sampling can be used in extra-Trees ensembles, but \citeauthor{guerts-2006-extratrees} argue that it can often lead significant drops in accuracy\footnote{Most \opensource{} implementations of extra-trees optionally allow for bootstrap sampling.}.

The primary tuning parameters for an extra-Trees ensemble are $K$ and $n_{min}$, where $K$ is the number of random splits to consider for each candidate splitter, and $n_{min}$ is the minimum node size, which is a common parameter in many tree-based models, and can act as a smoothing parameter. A common default for $K$ is
\begin{equation}
\nonumber
K = \begin{cases}
    \sqrt{p} & \quad \text{for classification} \\
    p        & \quad \text{for regression}
  \end{cases},
\end{equation}
where $p$ is the number of available predictors. The optimal split is chosen from the sample of $K$ splits in the usual way (i.e., the split that results in the largest reduction in node impurity). $n_{min}$ has the same defaults as it does in an RF (Section~\ref{sec:rf}).

The extra-trees ensemble still makes use of the RF $m_{try}$ parameter, but note that, in the extreme case where $K = 1$, an extra-trees tree is unsupervised in that the response variable is not needed in determining the any of the splits. Such a \emph{totally randomized tree} \citep{geurts-2006-xtrees} can be useful in detecting potential outliers and anomalies, as will be discussed in Section~\ref{sec:rf-isolation-forest}. Extra-trees can be fit in \R{} via the \pkg{ranger} package. In \Python{}, an implementation of the extra-trees algorithm is provided by scikit-learn's \pypkg{ensemble} module.


%-------------------------------------------------------------------------------
\subsection{Anomaly detection with isolation forests \label{sec:rf-isolation-forest}}
%-------------------------------------------------------------------------------

While the proximities of an RF can be used to detect novelties and potential outliers, they're rather computationally expensive to compute and store, especially for large data sets; also, as previously mentioned, many RF implementations do not support proximities. A more general approach to anomaly detection, called an \emph{isolation forest} \index{Isolation forest}, was proposed in \citet{liu-2008-isolation}. An isolation forests is essentially an ensemble of \emph{isolation trees} (iTrees) \index{Isolation tree}. iTrees are similar to extra-trees with $K = 1$ (Section~\ref{sec:rf-extratrees}), except that the splitting variables are also chosen at random; hence, iTrees are unsupervised in the sense that the tree building process does not make use of any response variable information.

So how does it work? Isolation forests are quite simple actually. The core idea is to "isolate" anomalous observations, rather than creating a profile for "normal" ones---the latter seems to be the more common approach taken by other methods in practice. Isolation forests assume that

\begin{enumerate}[label={\arabic*)}]

  \item anomalies are rare in comparison to "normal" observations;
  
  \item anomalies differ from "normal" instances in terms of the values of their features. 
  
\end{enumerate}

In other words, isolation forests assume anomalies are "few and different" \citep{liu-2008-isolation}. If anomalies are "few and different", then they are susceptible to \emph{isolation} (i.e., easy to separate from the rest of the observations).

In an iTree, observations are recursively separated until all instances are isolated to their own terminal node. Anomalous observations tend to be easier to isolate with fewer random partitions compared to normal instances. That is to say, the relatively few instances of anomalies tend to have shorter path lengths in an iTree when compared to normal observations. The path length to each observation can be computed for a forest of independently grown trees and aggregated into a single \emph{anomaly score}. The anomaly score for an arbitrary observation $\boldsymbol{x}$ is given by
\begin{equation}
\label{eqn:anomaly-score}
  s\left(\boldsymbol{x}, N\right) = 2 ^ {-\frac{1}{B}\sum_{b = 1} ^ B h_b\left(\boldsymbol{x}\right) / c\left(N\right)}.
\end{equation}
where $N$ is the sample size, $h_b\left(\boldsymbol{x}\right)$ is the path length to $\boldsymbol{x}$ in the $b$-th tree, and $c\left(N\right)$ is the \emph{average path length of unsuccesful searches}; in a binary tree constructed from $N$ observations, $c\left(N\right)$ is given by 
\begin{equation}
\nonumber
  c\left(N\right) = 2H\left(N - 1\right) - 2\left(N - 1\right) / N,
\end{equation}
where $H\left(i\right)$ is the $i$-th \emph{harmonic number} (\url{https://mathworld.wolfram.com/HarmonicNumber.html}). 

Let $\bar{h}\left(\boldsymbol{x}\right) = \frac{1}{B}\sum_{b = 1} ^ B h_b\left(\boldsymbol{x}\right)$ be the average path length for instance $\boldsymbol{x}$ across all trees in the forest, and note that $0 < s\left(\boldsymbol{x}, N\right) \le 1$ and $0 < \bar{h}\left(\boldsymbol{x}\right) \le N - 1$. A few useful relationships regarding \eqref{eqn:anomaly-score} are worth noting:

\begin{itemize}[nosep]

  \item $s\left(\boldsymbol{x}, N\right) \rightarrow 0.5$ as $\bar{h}\left(\boldsymbol{x}\right) \rightarrow c\left(N\right)$;
  
  \item $s\left(\boldsymbol{x}, N\right) \rightarrow 1$ as $\bar{h}\left(\boldsymbol{x}\right) \rightarrow 0$;
  
  \item $s\left(\boldsymbol{x}, N\right) \rightarrow 0$ as $\bar{h}\left(\boldsymbol{x}\right) \rightarrow N - 1$.
  
\end{itemize}

Since the assumption is that anomalies are easier to isolate, they are likely to have shorter path lengths on average. Hence, any instance $\boldsymbol{x}$ with values of $s\left(\boldsymbol{x}, N\right)$ close to one tend to be highly anomalous. If $s\left(\boldsymbol{x}, N\right)$ is much smaller than 0.5, then it is safe to regard $\boldsymbol{x}$ as a "normal" instance. If all the instances return a value close to 0.5, then it is safe to say the sample does not really contain any anomalies. Note that these are just guidelines.

Isolation forests are a top-performing unsupervised method for detecting potential outliers and anomalies \citep{domingues-2018-comparative}. Compared to other outlier detection algorithms, isolation forests are scalable (e.g., they have relatively little computational and memory requirements), fully nonparametric, and do not require a distance-like matrix. The Anti-Abuse AI Team at LinkedIn uses isolation forests to help detect abuse on LinkedIn (e.g., fake accounts, account takeovers, and profile scraping) \citep{verbus-2019-detecting}.

Let's illustrate the overall idea with a simple simulated example. Consider the data in Figure~\ref{fig:rf-itree-scatter}. Here, both $X_1$ and $X_2$ contain $N = 2000$ observations independently sampled from a standard normal distribution. However, I changed the first observation to $\boldsymbol{x} = \left(5.5, 5.5\right)$ (purple point), which is quite anomalous compared to the rest of the sample.

<<rf-itree-scatter, echo=FALSE, par=TRUE, fig.cap="Scatter plot of two independent samples from a standard normal distribution. The observation with coordinates $\\left(5.5, 5.5\\right)$ is a clear anomaly.">>=
np <- reticulate::import("numpy")
X <- np$load("../data/rf-itree-example.npy")
plot(X, pch = 19, col = adjustcolor(1, alpha.f = 0.1), xlab = expression(X[1]),
     ylab = expression(X[2]))
points(X[1, , drop = FALSE], pch = 19, col = "purple")
@

A single iTree from an isolation forest is displayed in Figure~\ref{fig:rf-itree-path}, along with the path taken by $\boldsymbol{x}$ (in purple). Here $\boldsymbol{x}$ has a path length of two and you can see that the path $\boldsymbol{x}$ takes in the tree is relatively shorter than most of the other available paths.

<<rf-itree-path, echo=FALSE, par=TRUE, fig.cap="A single iTree from an isolation forest showing the path of an anomaly.", out.width="70%">>=
knitr::include_graphics("diagrams/itree-path.png", error = FALSE)
@


%-------------------------------------------------------------------------------
\subsubsection{Extended isolation forests}
%-------------------------------------------------------------------------------

An \emph{extended isolation forest} \citep{hariri-2021-eif} improves the consistency and reliability of the anomaly score produced by a standard isolation forest by using random oblique splits (in this case, hyperplanes with random slopes)---as opposed to axis-oriented splits---which often results in improved anomaly scores. The tree in Figure~\ref{fig:rf-itree-path} is from an extended isolation forest fit using the \pkg{eif} package \citep{hariri-2021-eif}, which is available in both \R{} and \Python{}.


%-------------------------------------------------------------------------------
\subsubsection{Example: detecting fraud in credit card transactions}
%-------------------------------------------------------------------------------

\FIXME{Check if Kaggle has been introduced elsewhere; you mention it in a footnote here.}

To illustrate the basic use of an isolation forest, we'll use a data set from Kaggle\footnote{Kaggle is an online community of data scientists and machine learning practitioners who can find and publish data sets and enter competitions to solve data science challenges.} containing anonymized credit card transactions, labeled as fraudulent or genuine, obtained over a 48 hour period in September of 2013; the data can be downloaded from Kaggle at \url{https://www.kaggle.com/mlg-ulb/creditcardfraud}. Recognizing fraudulent credit card transactions is an important task for credit card companies to ensure that customers are not charged for items that they did not purchase. Since fraudulent transactions are relatively rare (as we'd hope), the data are highly imbalanced, with 492 frauds (0.17\%) out of the $N = 284{,}807$ transactions. 

For reasons of confidentiality, the original features have been transformed using PCA, resulting in 28 numeric features labelled \code{V1}, \code{V2}, ..., \code{V28}. Two additional variables, \code{Time} (the number of seconds that have elapsed between each transaction and the first transaction in the data set) and \code{Amount} (the transaction amount), are also available. These are labeled data, with the binary outcome \code{Class} taking on values of 0 or 1, where a 1 represents a fraudulent transaction. While this can certainly be framed as a supervised learning problem, we'll only use the class label to measure the performance of our isolation forest-based anomaly detection, which will be unsupervised. I would argue that it is probably more often that you will be dealing with unlabeled data of this nature, as it is rather challenging to accurately label each transaction in a large database.

To start, we'll split the data into train/test samples using only $N = 10{,}000$ observations (3.51\%) for training; the remaining 274,807 observations (96.49\%) will be used as a test set. However, before doing so, I'm going to shuffle the rows just to make sure they are in random order first. (Assume I've already read the data into a data frame called \code{ccfraud}.)

<<rf-fraud-detection-display, eval=FALSE>>=
# ccfraud <- data.table::fread("some/path/to/ccfraud.csv")

# Randomly permute rows
set.seed(2117)  # for reproducibility
ccfraud <- ccfraud[sample(nrow(ccfraud)), ]

# Split data into train/test sets
set.seed(2013)  # for reproducibility
trn.id <- sample(nrow(ccfraud), size = 10000, replace = FALSE)
ccfraud.trn <- ccfraud[trn.id, ]
ccfraud.tst <- ccfraud[-trn.id, ]

# Check class distribution in each
proportions(table(ccfraud.trn$Class))
proportions(table(ccfraud.tst$Class))
@

<<rf-fraud-detection, echo=FALSE>>=
ccfraud <- data.table::fread("../data/ccfraud.csv")

# Randomly permute rows
set.seed(2117)  # for reproducibility
ccfraud <- ccfraud[sample(nrow(ccfraud)), ]

# Split data into train/test sets
set.seed(2013)  # for reproducibility
trn.id <- sample(nrow(ccfraud), size = 10000, replace = FALSE)
ccfraud.trn <- ccfraud[trn.id, ]
ccfraud.tst <- ccfraud[-trn.id, ]

# Check class distribution in each
proportions(table(ccfraud.trn$Class))
proportions(table(ccfraud.tst$Class))
@

Next, I'll use the \pkg{isotree} package \citep{R-isotree} to fit a default isolation forest to the training set and provide anomaly scores for the test set. (Notice how I exclude the true class labels (column 31) when constructing the isolation forest!)

<<rf-fraud-detection-iforest-code, eval=FALSE>>=
library(isotree)

# Fit a default isolation forest
ccfraud.ifo <- isolation.forest(ccfraud.trn[, -31], nthreads = 1, 
                                random_seed = 2223)

# Compute anomaly scores for the test observations
head(scores <- predict(ccfraud.ifo, newdata = ccfraud.tst))
@

<<rf-fraud-detection-iforest-results, echo=FALSE>>=
library(isotree)

# Read in previously stored results
ccfraud.ifo <- readRDS("../data/rf-ccfraud-ifo.rds")
scores <- readRDS("../data/rf-ccfraud-scores-test.rds")

# Compute anomaly scores for the test observations
head(scores)
@

Although this isn't necessarily a classification problem, we can treat the anomaly scores as probabilities and compute informative graphics. While a precision-recall curve\footnote{Remember that precision/PPV is directly proportional to the prevalance of the positive outcome. They are not appropriate for case-control studies (e.g., which also includes case-control sampling---like down sampling---with imbalanced data sets) and should only be used when the true class priors are reflected in the data.} could be useful here, I think a simple cumulative lift chart would be more informative. Below I compute both; see Figure~\ref{fig:rf-fraud-detection-iforest-liftchart-prcurve}. Looking at the lift chart, for example, we can see that if we were to audit 5\% of the highest scoring transactions in the test set, then we will have found roughly 87\% of the fraudulent cases. The PR curve doesn't look good, as you might expect after looking at the lift chart. For example, even though we can identify roughly 87\% of the fraudulent transactions by looking at only 5\% of the test sample, that still leaves more than 1300 non-fraudulent transaction that also have to be audited. In this example, it seems that we're not able to detect the majority of frauds without accepting a large number of false positives.

<<rf-fraud-detection-iforest-prcurve>>=
#cutoff <- sort(unique(scores))
# Compute precision and recall across various cutoffs
cutoff <- seq(from = min(scores), to = max(scores), length = 999)
cutoff <- c(0, cutoff)
precision <- recall <- numeric(length(cutoff))
for (i in seq_along(cutoff)) {
  yhat <- ifelse(scores >= cutoff[i], 1, 0)
  tp <- sum(yhat == 1 & ccfraud.tst$Class == 1)  # true positives
  tn <- sum(yhat == 0 & ccfraud.tst$Class == 0)  # true negatives
  fp <- sum(yhat == 1 & ccfraud.tst$Class == 0)  # false positives
  fn <- sum(yhat == 0 & ccfraud.tst$Class == 1)  # false negatives
  precision[i] <- tp / (tp + fp)  # precision (or PPV)
  recall[i] <- tp / (tp + fn)  # recall (or sensitivity)
}
precision <- c(precision, 0)
recall <- c(recall, 0)
head(cbind(recall, precision))

# Compute data for lift chart
ord <- order(scores, decreasing = TRUE)
y <- ccfraud.tst$Class[ord]  # order according to sorted scores
prop <- seq_along(y) / length(y)
lift <- cumsum(y) / sum(ccfraud.tst$Class)  # convert to proportion
head(cbind(prop, lift))
@

<<rf-fraud-detection-iforest-liftchart-prcurve, echo=FALSE, fig.cap="Precision-recall curve (left) and lift/cumulative gain chart (right) for the isolation forest applied to the credit card fraud detection test set.">>=
# Precision-recall curve
ccfraud.pr <- data.frame(recall, precision)
p1 <- ggplot(ccfraud.pr, aes(recall, precision)) +
  geom_line(color = oi.cols[2]) +
  geom_hline(yintercept = mean(ccfraud.tst$Class), linetype = 2, color = oi.cols[1]) +
  #xlim(0, 1) +
  #ylim(0, 1) +
  scale_x_continuous(breaks = 0:10 / 10) +
  scale_y_continuous(breaks = 0:10 / 10) +
  xlab("Recall") +
  ylab("Precision")

# Lift curve
ccfraud.lift <- data.frame(prop, lift)
p2 <- ggplot(ccfraud.lift, aes(prop, lift)) +
  geom_line(color = oi.cols[2]) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, color = oi.cols[1]) +
  #xlim(0, 1) +
  #ylim(0, 1) +
  scale_x_continuous(breaks = 0:10 / 10) +
  scale_y_continuous(breaks = 0:10 / 10) +
  xlab("Proportion of sample inspected") +
  ylab("Proportion of anomalies identified")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, nrow = 1)
@

We can take the analysis a step further by using Shapley values (Section~\ref{sec:iml-shapley}) to help explain the observations with the highest/lowest anomaly scores, whichever is of more interest. To illustrate, let's estimate the feature contributions for the test observation with the highest anomaly score. Keep in mind that the features in this data set have been anonymized using PCA, so we won't be able to understand much of the output from a contextual perspective, but the idea applies to any application of anomaly detection based on a model that produces anomaly scores, like isolation forests. We're just treating the scores as ordinary predictions and applying Shapley values in the usual way. 

<<rf-fraud-detection-iforest-baseline, cache=TRUE, echo=FALSE>>=
scores.trn <- predict(ccfraud.ifo, newdata = ccfraud.trn)
to.explain <- max(scores) - mean(scores.trn)
@

In the code chunk below, I find the observation in the test data that corresponds to the highest anomaly score. Here, we see that \code{max.x} corresponds to an actual instance of fraud (\code{Class = 1}) and was assigned an anomaly score of \Sexpr{round(max(scores), digits = 3)}. The average anomaly score on the training data is \Sexpr{round(mean(scores.trn), digits = 3)}, for a difference of \Sexpr{round(to.explain, digits = 3)}. The question we want to try and answer is: how did each feature contribute to the difference \Sexpr{round(to.explain, digits = 3)}? This is precisely the type of question that Shapley values can help with.

<<rf-fraud-detection-iforest-max>>=
max.id <- which.max(scores)  # row ID for max anomaly score
(max.x <- ccfraud.tst[max.id, ])
max(scores)
@

Next, I'll use \pkg{fastshap} to generate Shapley-based feature contributions using the Monte-Carlo approach discussed in Section~\ref{sec:interpret-SampleSHAP} with 1000 repetitions. Note that we have to tell \pkg{fastshap} how to generate scores from an \code{isotree::isolation.forest()} model by providing a helper prediction function. The estimated contributions are displayed in Figure~\ref{fig:rf-fraud-detection-iforest-shapley-plot}. Here you can see that \code{Amount=25691.16} had the largest (positive) contribution (well above the 99-th percentile for the entire data set) to this observation having a higher than average anomaly score.

<<rf-fraud-detection-iforest-shapley, cache=TRUE>>=
X <- ccfraud.trn[, 1:30]  # feature columns only
max.x <- max.x[, 1:30]  # feature columns only!
pfun <- function(object, newdata) {  # prediction wrapper
  predict(object, newdata = newdata)
}

# Generate feature contributions
set.seed(1351)  # for reproducibility
(ex <- fastshap::explain(ccfraud.ifo, X = X, newdata = max.x, 
                         pred_wrapper = pfun, adjust = TRUE, 
                         nsim = 1000))
sum(ex)  # should sum to f(x) - baseline whenever `adjust = TRUE` 
@

<<rf-fraud-detection-iforest-shapley-plot, echo=FALSE, fig.cap="Estimated feature contributions for the test observation with highest anomaly score. There's a dashed vertical line at zero to differentiate between features with a positive/negative contribution. In this case, all feature values contributed positively to the difference $f\\left(x^\\star\\right) - \\bar{y}_{trn} = 0.501$.">>=
# Transpose feature contributions
res <- data.frame(
  "feature" = paste0(names(ex), "=", round(max.x, digits = 2)),
  "shapley.value" = as.numeric(as.vector(ex[1,]))
)

# Plot feature contributions
ggplot(res, aes(x = shapley.value, y = reorder(feature, shapley.value))) +
  geom_point() +
  geom_vline(xintercept = 0, linetype = "dashed") +
  xlab("Shapley-based feature contribution") +
  ylab("") +
  theme(axis.text.y = element_text(size = rel(0.8)))
@


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Class imbalance}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \FIXME{Reword and discuss \citet{chen-2004-imbalance}.}
% Random forests can inherently down-sample by controlling the bootstrap sampling process within a stratification variable. If class is used as the stratification variable, then bootstrap samples will be created that are roughly the same size per class. These internally down-sampled versions of the training set are then used to construct trees in the ensemble.
% 
% We still wouldn't necessarily treat at this as a classification problem in practice. For example, suppose it's expensive and/or time-consuming to audit a particular account if it's suspected of fraud. A good strategy would be to sort the anomaly scores in descending order and focus on the largest scores (akin to creating a lift chart).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software and examples\label{sec:rf-software}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\FIXME{Make sure citations are added for packages that have yet to be cited in the book. Probably wait til the end.}

RFs are available in numerous software, both \opensource{} and proprietary. The \R{} packages \pkg{randomForest}, \pkg{ranger}, and \pkg{randomForestSRC} \citep{R-randomForestSRC} implement the traditional RF algorithm for classification and regression; the latter two also support survival analysis, as well as several other extensions. The \pkg{party} and \pkg{partykit} packages offer an implementation using conditional inference trees as the base learners (Section~\ref{sec:ctree-ctree}) for the base learners; our \code{cforest()} from Section~\ref{sec:rf-conditional-rf} follows the same approach. The CRAN task view on "Machine Learning \& Statistical Learning" includes a section dedicated RFs in \R{}, so be sure to check that out as well: \url{https://cran.r-project.org/view=MachineLearning}.

While \pkg{randomForest} is a close port of Breiman's original Fortran code, the \pkg{ranger} package is far more scalable and implements a number of modern extensions and improvements discussed in this chapter (e.g., RF as a probability machine, Gini-corrected importance, quantile regression, case-specific RFs, extra-trees, etc.). Another scalable implementation is available from \pkg{h2o} \citep{R-h2o}. RFs are also part of Spark's MLlib library, which includes several \R{} and \Python{} interfaces (in particular, \pkg{SparkR}, \pkg{sparklyr}, and \pkg{pyspark}); an example using \pkg{SparkR} is provided in Section~\ref{sec:rf-ex-spark}. 

In \Python{}, RFs, extra-trees, and isolation forests are available in scikit-learn's \pypkg{ensemble} module. \Julia{} users can fit RFs via the \pkg{DecisionTree.jl} package. 

\FIXME{Strongly consider adding Ames housing ISLE example using \code{deforest()} function, once PR is accepted.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classifying the edibility of mushrooms \label{sec:rf-ex-mushroom}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

No! These data are easy and an ensemble would be overkill here. Remember, the original goal of the problem was to come up with an accurate, but simple rule for determining the edibility of a mushroom. This was easily accomplished using a single decision tree (e.g., CART with some manual pruning) or a rule-based model like CORELS; see, for example, Figure~\ref{fig:brpart-ex-mushroom-stump}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Understanding survival on the Titanic \label{sec:rf-ex-titanic}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this example, we'll walk through a simple RF analysis of the well-known titanic data set, where the goal is to understand survival probability aboard the ill-fated Titanic. A more thoughtful analysis using logistic regression and spline-based techniques is provided in \citet[Chap.~12]{harrell-2015-regression}.

Several versions of this data set are publically available; for example, in the \R{} package \pkg{titanic} \citep{R-titanic}. Here we'll use a more complete version of the data\footnote{A description of the original source of these data is provided in \citet[p.~291]{harrell-2015-regression}.} which can be loaded using the \code{getHdata()} from package \pkg{Hmisc} \citep{R-Hmisc}; the raw data can also be downloaded from \url{https://hbiostat.org/data/}. In this example, we'll only consider a handful of the original variables:

<<rf-ex-titanic-load>>=
t3 <- read.csv("https://hbiostat.org/data/repo/titanic3.csv",
               stringsAsFactors = TRUE)
keep <- c("survived", "pclass", "age", "sex", "sibsp", "parch")
t3 <- t3[, keep]  # only retain key variables
@

Note that roughly \Sexpr{round(mean(is.na(t3$age)) * 100, digits = 2)}\% of the values for \code{age}, the age in years of the passenger, are missing: 

<<rf-ex-titanic-nas>>=
sapply(t3, FUN = function(x) mean(is.na(x)))
@

Following \citet[Sec.~12.4]{harrell-2015-regression}, we use a decision tree to investigate which kinds of passengers tend to have a missing value for \code{age}. In the example below, I use the \pkg{partykit} package to apply the CTree algorithm (Chapter~\ref{tbd}) using a missing value indicator for \code{age} as the response. From the tree output we can see that third-class passengers had the highest rate of missing \code{age} values (\Sexpr{round(mean(is.na(t3[t3$pclass > 2, "age"])) * 100, digits = 1)}\%), followed by first-class male passengers with no siblings or spouses aboard (\Sexpr{round(mean(is.na(t3[t3$pclass <= 1 & t3$sex == "male" & t3$sibsp <= 0, "age"])) * 100, digits = 1)}\%). This makes sense, since males and third-class passengers supposedly had the least likelihood of survival ("women and children first").

<<rf-ex-titanic-nas-tree>>=
library(partykit)

# Fit a conditional inference tree using missingness as response
temp <- t3  # temporary copy
temp$age <- as.factor(ifelse(is.na(temp$age), "y", "n"))
(t3.ctree  <- ctree(age ~ ., data = temp))
# plot(t3.ctree)  # plot omitted
@

Next, we'll use the CART-based multiple imputation procedure described in Section~\ref{sec:brpart-imputation} to perform $m = 21$ separate imputations for each missing \code{age} value. Why did I choose $m = 21$? \citet{white-2011-multiple} propose setting $m \ge 100 f$, where $f$ is the fraction of incomplete cases\footnote{When $f \ge 0.03$, \citet[p.~57]{harrell-2015-regression} suggests setting $m = \max\left(5, 100 f\right)$}. Since \code{age} is the only missing variable, with $f =$ \Sexpr{round(mean(is.na(t3$age)), digits = 3)}, I chose $m = 21$. Using multiple different imputations will give us an idea of the sensitivity of the results of our (yet to be fit) RF. 

<<rf-ex-titanic-mice, fig.cap="Nonparametric density estimate of \\code{age} for the complete cases (blue line) and 15 imputed data sets.">>=
library(mice)

set.seed(1125)  # for reproducibility
imp <- mice(t3, method = "cart", m = 21, minbucket = 5,
            printFlag = FALSE)

# Display nonparametric densities
densityplot(imp)
@

Nonparametric densities for passenger age are given in Figure~\ref{fig:rf-ex-titanic-mice}. There is one density for each of the imputed data sets (red curves) and one density for the original complete case (blue curve). The overall distributions are comparable, but there is certainly some variance across the $m = 21$ imputation runs. Our goal is to run an RF analysis for each of the $m = 21$ completed data sets and inspect the variability of the results. For instance, we might graphically show the $m = 21$ variable importance scores for each feature, along with the mean or median. 

Next, we call \code{mice::complete()} to produce a list of the $m = 21$ completed data sets which we can use to carry on with our analysis. The only difference is, we will perform the same analysis on each for the completed data sets.\footnote{This approach is probably not ideal in situations where the analysis is expensive (e.g., because the data are "big" and the model is expensive to tune). In such cases, you may have to settle for a smaller, less optimal value for $m$.}

<<rf-ex-titanic-impute>>=
t3.mice <- complete(
  data = imp,      # "mids" object (multiply imputed data set)
  action = "all",  # return list of all imputed data sets
  include = FALSE  # don't include original data (i.e., data with NAs)
)
length(t3.mice)  # returns a list of completed data sets
@

For comparison, let's look at the results from using the proximity-based RF imputation procedure discussed in Section~\ref{sec:rf-proximity-imputation}. The code snippet below uses \code{rfImpute()} from package \pkg{randomForest} to handle the proximity-based imputation. The results are plotted along with those from MICE in Figure~\ref{fig:rf-ex-titanic-imputation-comparison-plot}. 

<<rf-ex-titanic-imputation-comparison-plot, par=TRUE, fig.cap="Imputed values for the 263 missing \\code{age} values. The yellow line corresponds to the proximity-based imputation. The light gray lines correspond to the 15 different imputation runs using MICE, and the blue line corresponds to their average.">>=
# Generate completed data set using RF's proximity-based imputation
set.seed(2121)  # for reproducibility
t3.rfimpute <- 
  randomForest::rfImpute(as.factor(survived) ~ ., data = t3, 
                         iter = 5, ntree = 500)

# Construct matrix of imputed values 
m <- imp$m  # number of MICE-based imputation runs
na.id <- which(is.na(t3$age))
x <- matrix(NA, nrow = length(na.id), ncol = m + 1)
for (i in 1:m) x[, i] <- t3.mice[[i]]$age[na.id]
x[, m + 1] <- t3.rfimpute$age[na.id]

# Plot results
palette("Okabe-Ito")
plot(x[, 1], type = "n", xlim = c(1, length(na.id)), ylim = c(0, 100),
     las = 1, ylab = "Imputed value")
for (i in 1:m) {
  lines(x[, i], col = adjustcolor(1, alpha.f = 0.1))
}
lines(rowMeans(x[, 1:m]), col = 1, lwd = 2)
lines(x[, m + 1], lwd = 2, col = 2)
legend("topright", legend = c("MICE: CART", "RF: proximity"), lty = 1,
       col = 1:2, bty = "n")
palette("default")
@

Here, you can see that the imputed values from both procedures are similar, but that multiple imputations provide a range of plausible values. Also, there are a few instances where there's a bit of a gap between the imputed values for the two procedures. For example, consider observations \Sexpr{na.id[119L]} and \Sexpr{na.id[122L]}, whose records are printed below. The first passenger is recorded to be a third-class female with three siblings (or spouses) and one parent (or child) aboard. This individual is likely a child. The proximity-based imputation imputed the age for this passenger as \Sexpr{x[119L, m + 1]} years, whereas MICE gives an average of \Sexpr{mean(x[119L, 1:m])} years and a plausible range of 0.75--8.00 years. Similarly, the proximity method imputed the age for case \Sexpr{na.id[122L]}---a third-class female with three children---as \Sexpr{x[122L, m + 1]} whereas MICE gave an average of \Sexpr{mean(x[122L, 1:m])}. Which imputations do you think are more plausible?

<<rf-ex-titanic-imputation-comparison>>=
t3[c(956, 959), ]
@

With the $m = $ \Sexpr{imp$m} completed data sets in hand, we can proceed with the RF analysis. The idea here is to fit an RF separately to each completed data set. We'll then look at the variance of the results (e.g., OOB error, variable importance scores, etc.) to judge its sensitivity to the different plausible imputations. Below we use the \pkg{ranger} package to fit a (default) RF/probability machine to each of the $m = 21$ completed data sets. In anticipation of looking at the sensitivity of the variable importance scores, we set \code{importance = "permutation"} to employ the OOB-based permutation variable importance procedure discussed in Section~\ref{sec:rf-oob-importance}.

<<rf-ex-titanic-rfos>>=
library(ranger)

# Obtain a list of probability forests, one for each imputed data set
set.seed(2147)  # for reproducibility
rfos <- lapply(t3.mice, FUN = function(x) {
  ranger(as.factor(survived) ~ ., data = x, probability = TRUE, 
         importance = "permutation")
})

# Check OOB errors (Brier-score, in this case)
sapply(rfos, FUN = function(forest) forest$prediction.error)
@

The OOB errors from each model are comparable, that's a good start! The average OOB Brier-score is \Sexpr{round(mean(sapply(rfos, FUN = function(forest) forest$prediction.error)), digits = 3)}, with a standard deviation of \Sexpr{round(sd(sapply(rfos, FUN = function(forest) forest$prediction.error)), digits = 3)}.

Next we'll look at variable importance. With multiple imputation I think the most sensible thing to do is to just plot the variable importance scores from each run together, so that you can see the variability in the results:

<<rf-ex-titanic-rfos-vis, par=TRUE, fig.cap="Boxplot of variable importance scores across all $m = 21$ RF fits.">>=
# Compute list of VI scores, one for each model. Note: can use 
#`FUN = ranger::importance` to be safe
vis <- lapply(rfos, FUN = importance)

# Stack into a data frame
head(vis <- as.data.frame(do.call(rbind, args = vis)))

# Display boxplots of results
boxplot(vis, las = 1)
@

Figure~\ref{fig:rf-ex-titanic-rfos-vis} shows a boxplot of the $m = 21$ variable importance scores for each feature; these are the OOB-based permutation scores discussed in Section~\ref{sec:rf-oob-importance}. The results do not vary too much, and so we can be somewhat confident in the overall ranking of the features. Clearly \code{sex} is the most important predictor of survivability in these models, followed by passenger class (\code{pclass}) and passenger age (\code{age}). The remaining features are comparatively less important, with no discernible difference between \code{sibsp} and \code{parch}. 

It's natural to look at feature effect plots after examining the importance of each variable (i.e., main effects). A common feature effect plot is the PDP (Section~\ref{tbd}). Following a similar strategy, we can compute the partial dependence for each feature across all $m = 21$ fitted RFs, and display the results on the same graph. 

The code snippets below relies on the \pkg{pdp} package for constructing each partial dependence curve; for details, see \citet{R-pdp}. To start, we define a simple prediction wrapper for extracting predictions from a fitted \pkg{ranger} model. Note that PDPs are essentially constructed from averaged predictions, so the function below returns the average predicted probability of survival:

<<rf-ex-titanic-rfos-pdp-pfun>>=
pfun <- function(object, newdata) {  # mean(prob(survived=1|x))
  mean(predict(object, data = newdata)$predictions[, "1"])
}
@

Since we have $m =$ \Sexpr{imp$m} imputed data sets, we're essentially computing $m =$ \Sexpr{imp$m} 3-way PDPs (i.e., visualizing a 3-way interaction effect), which can be quite cumbersome computationally. Since we only have one numeric predictor (\code{age}), it won't take but a few minutes in this example. For larger problems, or problems with many numeric features, you may want to consider computing PDPs for each feature individually (i.e., main effects), or using parallel computing (or other computational tricks). Below we instruct \code{pdp::partial()} to plot over an evenly spaced grid of 19 percentiles for \code{age} (from 5-th to 95-th) within each unique combination of \code{pclass}\footnote{I'm using \code{cats = "pclass"} here to treat \code{pclass} as categorical since it's restricted to \code{pclass} $\in \left\{1, 2, 3\right\}$.} and \code{sex}, giving a total of $19 \times 3 \times 2 = 114$ plotting points.

<<rf-ex-titanic-rfos-pdps, cache=TRUE>>=
library(pdp)

# Construct PDPs for each model
pdps <- lapply(1:m, FUN = function(i) {
  partial(rfos[[i]], pred.var = c("age", "pclass", "sex"), 
          pred.fun = pfun, train = t3.mice[[i]], cats = "pclass",
          quantiles = TRUE, probs = 1:19/20)
})

# Stack into a single data frame for plotting
for (i in seq_along(pdps)) {
  pdps[[i]]$m <- i
}
head(pdps <- do.call(rbind, args = pdps))
@

Next, we plot the results. There's some \R{}-ninja trickery happening in the code chunk below in order to get the plot we want. Using \pkg{ggplot2}, we want to group a line plot by two variables, but color by just one of them. We can paste the two grouping variables together into a new column. However, base \R{}'s \code{interaction()} function can accomplish this for us; see \code{?interaction} for details. 

The results are displayed in Figure~\ref{fig:rf-ex-titanic-rfos-pdps-plot}; compare this to Figure 12.22 in \citet[p. 308]{fig:harrell-2015-regression}. I also included a rug representation (i.e., 1-d plot) to each panel showing the deciles (i.e., the 10-th percentile, 20-th percentile, etc.) of passenger age from the original (incomplete) training set. This helps guide where the plots are potentially extrapolating. Using deciles means that 10\% of the observations lie between any two consecutive rug marks; see \citet{pdp2017} for some remarks on the importance of avoiding extrapolation when interpreting PDPs, as well as some mitigation strategies (e.g., using rug plots and \emph{convex hulls}). 

<<rf-ex-titanic-rfos-pdps-plot, par=TRUE, fig.cap="Partial dependence of the probability of surviving on passenger age, class, and sex. There's one curve for each of the $m = 21$ completed data sets.">>=
library(ggplot2)

# Plot results
deciles <- quantile(t3$age, prob = 1:9/10, na.rm = TRUE)
ggplot(pdps, aes(age, yhat, color = sex,
               group = interaction(m, sex))) +
  geom_line(alpha = 0.3) +
  geom_rug(aes(age), data = data.frame("age" = deciles), 
           sides = "b", inherit.aes = FALSE) +
  labs(x = "Age (years)", y = "Surival probability") +
  facet_wrap(~ pclass) +
  scale_colour_manual(values = c("black", "orange")) +  # Okabe-Ito
  theme_bw() +
  theme(legend.title = element_blank(),
        legend.position = "top")
@

While there's some variability in the results, the overall patterns are clear. First-class females had the best chances of surviving, regardless of age or class. Passenger age seems to have a stronger negative affect on passenger survivability for males compared to females, regardless of class. The difference in survivability between males and females is less pronounced for third-class passengers. Do you agree? What other conclusions can you draw from this plot? %(See Exercise~\ref{ex:tbd} for more.)

Finally, let's use Shapley values (Section~\ref{sec:interpret-shapley}) to help us understand individual passenger predictions. To illustrate, lets focus on a single (hypothetical/fictional) passenger. Everyone, especially those who haven't seen the movie, meet Jack\footnote{I guestimated some of Jack's inputs, based on the movie I saw in seventh grade.}:

<<rf-ex-titanic-jack>>=
jack.dawson <- data.frame(
  #survived = 0L,  # in case you haven't seen the movie
  pclass = 3L,  # using `3L` instead of `3` to treat as integer
  age = 20.0,
  sex = factor("male", levels = c("female", "male")),
  sibsp = 0L,  
  parch = 0L  
)
@

Here, we'll use the \pkg{fastshap} package for computing Shapley values, but you can use any Shapley value package you like (e.g., \R{} package \pkg{iml}). First, we need to setup a prediction wrapper---this is a function that tells \pkg{fastshap} how to extract predictions from the fitted \pkg{ranger} model, which is the purpose of function \code{pfun()} below. Next, we compute approximate feature contributions for Jack's predicted probability of survival using 1000 Monte-Carlo repetitions, which is done for each of the $m = $\Sexpr{imp$m} completed data sets:

<<rf-ex-titanic-shapley>>=
library(fastshap)

# Prediction wrapper for `fastshap::explain()`; has to return a single
# (atomic) vector of predictions
pfun <- function(object, newdata) {  # compute prob(survived=1|x)
  predict(object, data = newdata)$predictions[, 2]
}

# Estimate feature contributions for each imputed training set
set.seed(754)
ex.jack <- lapply(1:21, FUN = function(i) {
  X <- subset(t3.mice[[i]], select = -survived)
  explain(rfos[[i]], X = X, newdata = jack.dawson, nsim = 1000, 
          adjust = TRUE, pred_wrapper = pfun)
})

# Bind together into one data frame
ex.jack <- do.call(rbind, args = ex.jack)

# Add feature values to column names
names(ex.jack) <- paste0(names(ex.jack), "=", t(jack.dawson))
print(ex.jack)
@

Fortunately, again, the results are relatively stable across imputations. A summary of the overall Shapley explanations, along with Jack's predictions, is shown in Figure~\ref{fig:rf-ex-titanic-shapley-boxplots}. Here, we can see that Jack being a third-class male contributed the most to his poor predicted probability of survival, aside from him not being able to fit on the floating door that Rose was hogging...

<<rf-ex-titanic-shapley-boxplots, fig.cap="Predicted probability of survival for Jack across all imputed data sets (left) and their corresponding Shapley-based feature contributions (right).">>=
# Jack's predicted probability of survival across all imputed
# data sets
pred.jack <- data.frame("pred" = sapply(rfos, FUN = function(rfo) {
  pfun(rfo, jack.dawson)
}))

# Plot setup (e.g., side-by-side plots)
par(mfrow = c(1, 2),  mar = c(4, 4, 2, 0.1), 
    las = 1, cex.axis = 0.7) 

# Construct boxplots of results
boxplot(pred.jack, col = adjustcolor(2, alpha.f = 0.5))
mtext("Predicted probability of surviving", line = 1)
boxplot(ex.jack, col = adjustcolor(3, alpha.f = 0.5), horizontal = TRUE)
mtext("Feature contribution", line = 1)
abline(v = 0, lty = "dashed")
@

We just walked through a simple analysis of the well-known Titanic data set, with a focus on using RFs to understand which passengers were most likely to survive, and why. The analysis was complicated by the fact that one variable, \code{age}, contained many missing values. As a result, we performed multiple imputation, followed by an RF analysis on each of the plausible imputed data sets to gauge the sensitivity of the resulting imputations. In this example, the results seemed relatively stable across imputations, so we can confident in our conclusions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Unbalanced data (the good, the bad, and the ugly)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\FIXME{Make a point regarding the balanced, but unrealistic nature of the letter recognition and Swiss banknote data sets. The former can be fixed given the historical knowledge regarding the true class frequencies. And unless similar historical knowledge exists for the Swiss banknote data, not much can be done.}

As discussed in Section~\ref{sec:rf-prob}, we often don't want to predict a class label, but rather the probability of belonging to a particular class---the latter has the benefit of providing some indication of uncertainty (e.g., if we predicted Jack to have a 15\% chance of surviving his voyage on the Titanic, then we also estimate that he has an 85\% chance of not surviving). Some models, however, can give you poor estimates of the class probabilities. \emph{Calibration} \index{Calibration} addresses the issue by allowing you to better calibrate the probabilities of a fitted model, or provide estimated probabilities for models that are unable to naturally produce them in the first place (\emph{support vector machines}, for example).

Calibration refers to the degree of agreement between observed and predicted probabilities, and its utility is well-discussed in the statistical literature (even if not commonly practiced); see, for example, \citet{rufibach-2010-use}, \citet{austin-2014-graphical}, and \citet{mizil-2005-predicting}---the latter discusses calibration in a broader context and includes some discussion on calibrating RFs. A well calibrated (binary) probability model should produce accurate class probabilities such that among those probability estimates close to, say, 0.7, approximately 70\% actually belong to the positive class, and so on.

\FIXME{Triple check claim regarding bagging to purity and consistency.}

Unless the trees are grown to purity, RFs generally produce consistent and \emph{well-calibrated} probabilities \citep{malley-2012-consistent, mizil-2005-predicting}, while boosted trees (Chapter~\ref{chap:gbm}) do not\footnote{Surprisingly, in contrast to RFs, bagging decision trees grown to purity produces consistent probability estimates \citep{malley-2012-consistent, biau-2008-consistency}.}. However, we'll shortly see that that's not always the case. Furthermore, \emph{calibration curves} \index{Calibration curves|see {Calibration}} can be useful in comparing the performance of fitted probability models.

Real binary data are often unbalanced. For example, in modeling loan defaults, the target class (default on a loan) is often underrepresented. This is expected since we would hope that most people don't default on their loan over the course of paying it off. However, many SML learning practitioners perceive class imbalance as an issue that affects "accuracy". In actuality, the problem is usually that the data are balanced \citep[p.~193]{matloff-2017-statistical}.

In this example, we're going to simulate some unbalanced data. In particular, we're going to convert the Friedman 1 benchmark regression data (Section~\ref{sec:intro-data-friedman1}) into a binary classification problem using a \emph{latent variable model}. Essentially, we'll treat the observed response values as the linear predictor of a logistic regression model and covert them to probabilities. We can then use a binomial random number generator to simulate the observed class labels. The important thing to remember about this simulation study is that we have access to the true underlying class probabilities!

A simple function to convert the Friedman 1 regression problem into a binary classification problem, as described above, is given below. (Note that the line \code{d\$y <- d\$y - 23} shifts the intercept term and effectively controls the balance of the generated 0/1 outcomes---here, it was chosen to obtain a 0/1 class balance of roughly 0.95/0.05.)

<<rf-gen-binary-data>>=
gen_binary <- function(...) {
  d <- treemisc::gen_friedman1(...)  # regression data
  d$y <- d$y - 23  # shift intercept
  d$prob <- plogis(d$y)  # inverse logit to obtain class probabilities
  #d$prob <- exp(d$y) / (1 + exp(d$y))  # same as above
  d$y <- rbinom(nrow(d), size = 1, prob = d$prob)  # 0/1 outcomes
  d
}

# Generate samples
set.seed(1921)  # for reproducibility
trn <- gen_binary(100000)  # training data
tst <- gen_binary(100000)  # test data
@

Let $N = N_0 + N_1$ be the number of observations in the learning sample, where $N_0$ and $N_1$ represent the number of observations that belong to class $0$ and class $1$, respectively. If the learning sample is a random (i.e., representative) sample form the population of interest, then we can estimate the true class priors from the data using $\pi_i = N_i / N$, for $i = 1, 2$; this was discussed for CART-like decision trees in Section~\ref{sec:brpart-priors}. There are three scenarios to consider:

\begin{enumerate}[label=\alph*), nosep]

  \item the data form a representative sample and the observed class frequencies reflect the true class priors in the population (\strong{the good});

  \item the class frequencies have been artificially balanced but the true class frequencies/priors are known (\strong{the bad});
  
  \item the class frequencies have been artificially balanced and the true class frequencies/priors are unknown (\strong{the ugly}).

\end{enumerate}

In the code chunk below I use an independent sample of size $N = 10^6$ to estimate $\pi_1$ (i.e., the \emph{prevalance} of observations in class 1 in the population):

<<rf-ex-unbalanced-prevalence, cache=TRUE>>=
(pi1 <- proportions(table(gen_binary(1000000)$y))["1"])
@

Next, we'll define a simple calibration function that can be used for \emph{isotonic calibration}; for a brief overview of different calibration methods, see \citet{mizil-2005-predicting, kull-2017-beyond}. Note that there are many \R{} and \Python{} libraries for calibration; for example, \code{val.prob()} from \R{} package \pkg{rms} \citep{R-rms} and scikit-learn's \pypkg{calibration} module. %The examples in this section use the \code{calibrate()} function from \pkg{treemisc}. %The function below was taken from part of the \code{calibrate()} function in \pkg{treemisc}.

<<rf-ex-unbalanced-calibrate>>=
isocal <- function(prob, y) {  # isotonic calibration function
  ord <- order(prob)
  prob <- prob[ord]  # put probabilities in increasing order
  y <- y[ord]  
  prob.cal <- isoreg(prob, y)$yf  # fitted values
  data.frame("original" = prob, "calibrated" = prob.cal)
}
@

To start, let's fit a default RF to the original (i.e., unbalanced) learning sample. Note that we exclude the \code{prod} column when specifying the model formula.

<<rf-ex-unbalanced-calibrate-original, cache=TRUE>>=
library(ranger)

# Fit a probability forest (omitting the prob column)
set.seed(1446)  # for reproducibility
(rfo1 <- ranger(y ~ . - prob, data = trn, probability = TRUE,
                verbose = FALSE))
@

The OOB prediction error (in this case, the Brier score) is \Sexpr{round(rfo1$prediction.error, digits = 3)}. The Brier score on the test data can also be computed, but since we have access to the true probabilities we might as well compare them with the predictions too (for this, we'll compute the MSE between the predicted and true probabilities). In this case, we see that the Brier score on the test set is comparable to the OOB Brier score.

<<rf-ex-unbalanced-calibrate-original-tst, cache=TRUE>>=
prob1 <- predict(rfo1, data = tst)$predictions[, 2]

mean((prob1 - tst$y) ^ 2)  # Brier score
mean((prob1 - tst$prob) ^ 2)  # MSE between predicted and true probs
@

Looking at a single metric (or metrics) does not paint a full picture, so it can be helpful to look at specific visualizations, like calibration curves, to further assess the accuracy of the model's predicted probabilities (\emph{lift charts} \index{Lift chart} can also be useful). The leftmost plot in Figure~\ref{fig:rf-ex-calibration-curves} shows the actual versus predicted probabilities for the test set, as well as the isotonic-based calibration curve from the above RF. In this case, the RF seems to be doing a reasonable job in terms of accuracy. The model seems well-calibrated for probabilities below 0.5, but seems to have a slight negative bias for probabilities above 0.5, which makes sense since most of the probability mass is concentrated near zero (as we might have expected given the true class frequencies).

To naively combat the perceived issue of unbalanced class labels, the learning sample is often artificially rebalanced (e.g., using down sampling) so that the class outcomes have roughly the same distribution. In general, THIS IS A BAD IDEA for probability models, and can lead to serious bias in the predicted probabilities---in fact, any algorithm that requires you to remove good data to optimize performance is suspect. Nonetheless, sometimes the data have been artificially rebalanced in a preprocessing step outside of our control, or maybe you decided to down sample the data to reduce computation time (in which case, you should try to preserve the original class frequencies, or at least store them for adjustments later). In any case, let's see what happens to our predictions when we down sample the majority class. 

In scenarios b)--c), we cannot estimate $\pi_0$ and $\pi_1$ from the learning sample, however, in scenario b) we might have estimates of $\pi_0$ and $\pi_1$, perhaps from historical data. If the data have been artificially balanced, then it's possible to use good estimates of $\pi_0$ and $\pi_1$ to "correct" (or adjust) the output predicted probabilities. With CART and GUIDE, it's possible to provide the true priors and let the tree algorithm handle the adjustment (we saw how this is handled in CART in Section~\ref{sec:priors} and provided an example with \pkg{rpart} using the letter image recognition example in Section~\ref{sec:brpart-ex-letter}). What if no priors argument is available in your software? Fortunately, you can apply a simple adjustment to the output predicted probabilities, as discussed in \citet[pp.~197-198]{matloff-2017-statistical}. A simple function to adjust the predicted probabilities is given below. Here, \code{p} is a vector of predicted probabilities for the positive class (i.e., $\hatprob\left(Y = 1 | \boldsymbol{x}\right)$), \code{observed.ratio} is the ratio of the observed class frequencies (i.e., $N_0 / N_1$), and \code{true.ratio} is the ratio of the true class priors (i.e., $\pi_0 / \pi_1$.

<<rf-ex-unbalanced-adjust>>=
prob.adjust <- function(p, observed.ratio, true.ratio) {
  f.ratio <- (1 / p - 1) * (1 / observed.ratio)
  1 / (1 + true.ratio * f.ratio)
}
@

Let's try this out on an RF fit to a down sampled version of the training data. Below I artificially balance the classes by removing rows corresponding to the dominant class (i.e., \code{y = 0}):

<<rf-ex-unbalanced-calibrate-down-sample, cache=TRUE>>=
trn.1 <- trn[trn$y == 1, ]
trn.0 <- trn[trn$y == 0, ]
trn.down <- rbind(trn.0[seq_len(nrow(trn.1)), ], trn.1)
table(trn.down$y)
@

Next, we'll fit another (default) RF, but this time to the down samples training set. We then apply the adjustment formula to the predicted probabilities for the positive class in the test set:

<<rf-ex-unbalanced-calibrate-down-sample-probs, cache=TRUE>>=
set.seed(1146)  # for reproducibility
rfo2 <- ranger(y ~ . - prob, data = trn.down, probability = TRUE)

# Predicted probabilities for the positive class: P(Y=1|x)
prob2 <- predict(rfo2, data = tst)$predictions[, 2]
mean((prob2 - tst$y) ^ 2)  # Brier score
mean((prob2 - tst$prob) ^ 2)  # MSE
@

<<rf-ex-unbalanced-calibrate-down-sample-probs-adjusted, cache=TRUE>>=
prob3 <- prob.adjust(prob2, observed.ratio = 1, 
                     true.ratio = (1 - pi1) / pi1)
mean((prob3 - tst$y) ^ 2)  # Brier score
mean((prob3 - tst$prob) ^ 2)  # MSE between predicted and true probs
@

Figure~\ref{fig:rf-ex-calibration-curves} shows the predicted vs. true probabilities across three different cases: 1) predicted probabilities (\code{prob1}) from an RF applied to the original training data (left display), 2) predicted probabilities (\code{prob2}) from an RF applied to a down-sampled version of the original training data, but adjusted using the original class frequencies (middle display), and 3) predicted probabilities (\code{prob2}) from an RF applied to a down-sampled version of the original training data with no adjustment (right display).

<<rf-ex-calibration-curves, cache=TRUE, echo=FALSE, fig.cap="True versus predicted probabilities for the test set from an RF fit to the original and down sampled (i.e., artificially balanced) training sets. Left: probabilities from the original RF. Middle: Probabilities from the down sampled RF with post-hoc adjustment. Right: Probabilities from the down sampled RF. The calibration curve is shown in orange, with the blue curve representing perfect calibration.">>=
# Colors
cols <- palette.colors(4, palette = "Okabe-Ito")

# Compute calibration curves
cal1 <- isocal(prob1, tst$y)
cal2 <- isocal(prob2, tst$y)
cal3 <- isocal(prob3, tst$y)

# Add additional columns and bind results together
cal1$true <- tst$prob[order(prob1)]
cal2$true <- tst$prob[order(prob2)]
cal3$true <- tst$prob[order(prob3)]
cal1$method <- "The good"
cal2$method <- "The ugly"
cal3$method <- "The bad"
cal <- rbind(cal1, cal2, cal3)

# Reorder factor levels to that the facets are in the right order
cal$method <- factor(cal$method, levels = c("The good", "The bad", "The ugly"))

# Construct plot
ggplot(cal, aes(original, true)) +
  geom_point(alpha = 0.03) +
  geom_abline(intercept = 0, slope = 1, color = cols[3]) +
  geom_line(aes(original, calibrated), color = cols[2]) +
  facet_wrap(~ method) +
  xlab("Predicted probability") +
  ylab("Actual probability")
@

Compared to the predicted probabilities from an RF fit to the original (i.e., unbalanced) learning sample, down sampling appears to have produce biased and poorly calibrated probabilities, although, the adjustment formula seems to provide some relief, but requires access to the true class priors (or good estimates thereof). 

\FIXME{The following sentense doesn't seem to fit. Maybe revise?}

Basically, it is ill-advised to choose a model based on a metric that forces a classification based on an arbitrary threshold. Instead, choose a model using a proper scoring rule (e.g., log-loss or the Brier score) that makes use of the full range of predicted probabilities and is optimized when the true probabilities are recovered. Down sampling, adjusted or not, seems to highly under or over estimate the true class probabilities.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Partial dependence plots with Spark MLlib \label{sec:rf-ex-spark}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<rf-ex-bank-load, echo=FALSE>>=
pred <- readRDS("../data/chap-rf-bank-pred.rds")
rfo.summary <- readRDS("../data/chap-rf-bank-rfo-summary.rds")
vi <- readRDS("../data/chap-rf-bank-vi.rds")
euribor3m.grid <- readRDS("../data/chap-rf-bank-quantiles.rds")
pd <- readRDS("../data/chap-rf-bank-pd.rds")
@

In this section, we'll look at a well-known bank marketing data set available from the UC Irvine Machine Learning Repository \citep{uci-bank-marketing}. The data concern the direct marketing campaigns of a Portuguese banking institution, which were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product, a bank term deposit, would be subscribed or not; hence, this is a binary classification problem with response variable \code{y} taking on values yes/no depending on whether or not the client did/did not subscribe to the bank term deposit. For details and a description of the different columns, visit \url{https://archive.ics.uci.edu/ml/datasets/Bank+Marketing}.

% The classification goal is to predict if the client will subscribe (yes/no) a term deposit

\FIXME{Is there a reference to pyspark or MLlib?}

In this section, we'll illustrate using Spark and MLlib to fit an RF to the email spam data set and show how to construct partial dependence plots---which are not currently available in Spark MLlib---for any predictive model in Spark. For instructions on installing Apache Spark, visit \url{https://spark.apache.org/docs/latest/sparkr.html}. In this example, I'll use \pkg{SparkR}\footnote{\pkg{SparkR} is an R package providing a light-weight frontend to Apache Spark from \R{}. Starting with Spark 3.1.1, \pkg{SparkR} also provides a distributed data frame implementation that supports common data wrangling operations like selection, filtering, aggregation, etc. (similar to using \pkg{data.table} or \pkg{dplyr} with \R{} data frames) but on large data sets. \pkg{SparkR} also supports distributed machine learning using Spark MLlib.} \citep{R-SparkR}, but the same strategy can be applied with \pkg{sparklyr} \citep{R-sparklyr} as well

Furthermore, we'll do the analysis via one of the \R{} front ends to Apache Spark and MLlib: SparkR\footnote{This analysis can easy be translated to any other front end to Spark, including \pkg{sparklyr} \citep{R-sparklyr} or \pkg{pyspark}.} \citep{R-SparkR}. Starting with Spark 3.1.1, \pkg{SparkR} also provides a distributed data frame implementation that supports common data wrangling operations like selection, filtering, aggregation, etc. (similar to using \pkg{data.table} or \pkg{dplyr} with \R{} data frames, but on large data sets that don't fit into memory). For instructions on installing Spark, visit \url{https://spark.apache.org/docs/latest/sparkr.html}.

While the bank marketing data contain 21 columns on 41,188 records, this is by no means "Spark territory". However, you may find yourself in a situation where you need to use Spark MLlib for scalable analytics, so I think it's useful to show how to perform common SML tasks in Spark and MLlib, like fitting RFs, assessing performance, and computing PDPs.

To start, we'll download a zipped file containing a directory with the full bank marketing data set. The code downloads the zipped file into a temporary directory, unzips it, and reads in the CSV file of interest. (If the following code does not work for you, then you may find it easier to just manually download the \textsf{bank-additional-full.csv} and read it into \R{} on your own.)

<<rf-ex-bank-read>>=
url <- paste0("https://archive.ics.uci.edu/ml/machine-learning",
              "-databases/00222/bank-additional.zip")
temp <- tempfile(fileext = ".zip")  # to store zipped file
download.file(url, destfile = temp)
bank <- read.csv(unz(temp, "bank-additional/bank-additional-full.csv"), 
                 sep = ";", stringsAsFactors = FALSE)
unlink(temp)  # delete temporary file
@

Next, we'll clean up the data a bit. First off, we'll replace the dots in the column names with underscores; Spark does not like dots in column names! Second, we'll coerce the response (\code{y}) from a factor (\code{no}/\code{yes}) to a binary indicator (0/1) and treat it as numeric to fit a probability forest/machine. Finally, we'll remove the column called \code{duration}. Too often have I seen online analyses of the same data, only for the analyst to be fooled into thinking that \code{duration} is a useful indicator of whether or not a client will subscribe to a bank term deposit. If you take care and read the data documentation, you'd notice that the value of \code{duration} is not known before a call is made to a client. In other words, the value of \code{duration} is not known at prediction time---a textbook example of target leakage. KNOW YOUR DATA! Finally, the data are split into train/test sets using a 50/50 split; we could do this manually, but here we'll use the \pkg{caret} package's \code{createDataPartition()} function, which uses stratified sampling to ensure the distribution of classes is similar between the resulting partitions\footnote{Several other packages could also be used here, \pkg{rsample} \citep{R-rsample}, for example.}:

<<rf-ex-bank-clean>>=
names(bank) <- gsub("\\.", replacement = "_", x = names(bank))
bank$y <- ifelse(bank$y == "yes", 1, 0)
bank$duration <- NULL  # remove target leakage

# Split data into train/test sets using a 50/50 split
set.seed(1056)
trn.id <- caret::createDataPartition(bank$y, p = 0.5, list = FALSE)
bank.trn <- bank[trn.id, ]  # training data
bank.tst  <- bank[-trn.id, ]  # test data
@

Next, we'll load \pkg{ggplot2}, \pkg{SparkR}, and initialize a \emph{SparkSession}---the entry point into \pkg{SparkR} (see \code{?SparkR::sparkR.session} for details and the various Spark properties that can be set). If you're new to Spark, start with the online documentation: \url{https://spark.apache.org/docs/latest/index.html}, you can also find links to \pkg{SparkR} here as well. Note that \pkg{SparkR} is not on CRAN, but is included with a standard install of Apache Spark. To load \pkg{SparkR} in an existing \R{} session\footnote{You can also start an \R{} session with \pkg{sparkR} already available from the terminal by running \code{./bin/sparkR} from your Spark home folder; for details, see \url{https://spark.apache.org/docs/latest/sparkr.html}}, say, in RStudio, you need to tell \code{library()} the location of the package. (Note that the code snippet below may need to change for you depending on where you have Spark installed; for me, it's in \code{C:\\spark}.)

<<rf-ex-sparkR-connect, eval=FALSE>>=
loc <- "/usr/local/Cellar/apache-spark/3.1.1/libexec/R/lib"
library(SparkR, lib.loc = loc)
library(ggplot2)

# Start a local connection to Spark using all available cores
sparkR.session(master = "local[*]")
@

Next, we'll apply MLlib's Spark-enabled RF algorithm by calling \code{spark.randomForest()}; here, we'll use $B = 500$ trees with a max depth of 10. Note that \pkg{SparkR} works with Spark DataFrames, not \R{} data frames, so we have to coerce our train/test sets to Spark DataFrames using \code{createDataFrame()} before applying any Spark operations (ideally, we'd read the original data into a Spark DataFrame directly and process the data using Spark operations, but I was being lazy):

<<rf-ex-bank-rf, eval=FALSE>>=
bank.trn.sdf <- createDataFrame(bank.trn)
bank.tst.sdf <- createDataFrame(bank.tst)

# Fit a regression/probability forest
bank.rfo <- spark.randomForest(
  bank.trn.sdf, y ~ ., type = "regression",
  numTrees = 500, maxDepth = 10, seed = 1205
)
@

To assess the performance of the probability forest, we can compute the Brier score on the test set. A couple of things are worth noting about the code chunk below. First, the \code{predict()} method, when applied to a \pkg{SparkR} MLlib model, returns the predictions along with the original columns from the supplied Spark DataFrame. Second, note that we have to compute the Brier score using Spark DataFrame operations, like \code{SparkR::summarize()}, in this case).

<<rf-ex-bank-rf-brier-score, eval=FALSE>>=
p <- predict(bank.rfo, newData = bank.tst.sdf)  # Pr(Y=yes|x)
head(summarize(p, brier_score = mean((p$prediction - p$y)^2)))

#>   brier_score                                                                   
#> 1  0.07815544
@

The AUC on the test set for this model, if you care purely about discrimination, is \Sexpr{round(Metrics::auc(pred$y, predicted = pred$prediction), digits = 3)}\footnote{Even if discrimination is the goal, AUC does not take into account the prior class probabilities, it is not necessarily appropriate in situation with severe class imbalance; in this case, area under the PR curve would be more informative. \FIXME{Triple check the validity of your claim here!}}, which is in line with even some of the more advanced analyses I've seen in these data. Nice! In addition, shows an isotonic regression-based calibration curve (left) and cumulative gains chart, both computed from the test data. The model seems reasonably calibrated (as we would hope from a probability forest). The cumulative gains chart tells us, for example, that we could expect roughly 1,500 subscriptions by contacting the top \%20 of clients with the highest predicted probability of subscribing.

<<rf-ex-bank-rf-cal-lift, echo=FALSE, fig.cap="Graphical assessment of the performance on the test set. Left: isotonic regression-based calibration curve. Right: Cumulative gains chart.">>=
cal <- treemisc::calibrate(pred$prediction, y = pred$y, 
                           method = "iso", pos.class = 1)
cg <- treemisc::lift(pred$prediction, y = pred$y, pos.class = 1,
                     cumulative = TRUE)

# Plot calibration curve and cumulative gains chart
par(
  mfrow = c(1, 2),
  mar = c(4, 4, 0.1, 0.1), 
  cex.lab = 0.95, 
  cex.axis = 0.8,  # was 0.9
  mgp = c(2, 0.7, 0), 
  tcl = -0.3, 
  las = 1
)
palette("Okabe-Ito")
plot(cal, refline = FALSE)
abline(0, 1, lty = 2, col = 2)
legend("topleft", legend = "Perfectly calibrated", lty = 2, col = 2,
       bty = "n", inset = 0.01, cex = 0.6)
plot(cg$prop, cg$lift, type = "l",
     xlab = "% Contacted", ylab = "% Subscribed")
abline(0, sum(bank.tst$y == 1), lty = 2, col = 2)
grid()
legend("topleft", legend = "Baseline", lty = 2, col = 2,
       bty = "n", inset = 0.01, cex = 0.6)
palette("default")
@

Next, we'll look at the RF-based variable importance scores. Unfortunately, \pkg{SparkR} does not return the variable importance scores from tree-based models in a friendly format, it just gives one long nasty string, as can be seen below\footnote{It is not clearly documented which variable importance metric MLlib uses in its implementation of RFs, but I suspect it is the impurity-based metric (Section~\ref{sec:rf-importance}) since, as far as I'm aware, MLlib's RF implementation does not support the notion of OOB observations (Section~\ref{sec:rf-oob}).}:

<<rf-ex-bank-vi, eval=FALSE>>=
rfo.summary <- summary(rfo)  #  extract summary information
(vi <- rfo.summary$featureImportances)  # gross...
@

<<rf-ex-bank-vi-hide, echo=FALSE>>=
vi
@

Nonetheless, we can use some \emph{regular expression} (regex) magic to parse the output into a more friendly data frame. (By no means do I claim this to be the best solution, I'm certainly no regexpert---ba dum tsh.)

<<rf-ex-bank-vi-parse>>=
vi <- substr(vi, start = regexpr(",", text = vi)[1] + 1, stop = nchar(vi) - 1)
vi <- gsub("\\[", replacement = "c(", x = vi)
vi <- gsub("\\]", replacement = ")", x = vi)
vi <- paste0("cbind(", vi, ")")
vi <- as.data.frame(eval(parse(text = vi)))
names(vi) <- c("feature.id", "importance")
vi$feature.name <- rfo.summary$features[vi[, 1] + 1]
head(vi[order(vi$importance, decreasing = TRUE), ], n = 10)
@

The output suggests that the number of employees (\code{nr\_employed}), a quarterly economic indicator, the euribor 3 month rate (\code{euribor3m}), a daily economic indicator, and the number of days passed since the client was last contacted from a previous campaign (\code{pdays}) are important predictors.

To further investigate the effect of these features on the model output, we can look at feature effect plots (such as PDPs and ICE plots). Here, we'll construct a PDP for the economic indicator \code{euribor3m}\footnote{You can find a similar example using \pkg{dplyr} with the \pkg{sparklyr} front end to Spark here: \url{https://github.com/bgreenwell/pdp/issues/97}.}. The trick to computing PDPs in Spark, if you can afford the memory, is to generate all the necessary data up front so that you only need one call to a scoring function. Once you have all the predictions, you can just post process the results into partial dependence values by averaging the predictions within each unique value of the feature of interest; the same idea works for ICE plots as well (Section~\ref{sec:interpret-ice}). We saw how to do this in base \R{} in Section~\ref{sec:interpret-pd-ames} using the Ames data. Even with large training data sets that don't fit into memory, the aggregated partial dependence values will be small enough to bring into memory as an ordinary \R{} data frame and plotted using your favorite plotting library. 

Following the same recipe outlined in Section~\ref{sec:interpret-pd-ames}, we'll start by creating a grid of values we want the PDP for \code{euribor3m} to cover. For example, we can use an evenly spaced grid of points that covers the range of the predictor values of interest, or the sample quantiles; the latter has the benefit of potentially excluding outliers/extremes from the resulting plot. Since the data resides in a Spark data frame, we can't just use base \R{} functionality. Luckily, \pkg{SparkR} provides the functionality we need via \code{approxQuantile()}, which we use to construct a new Spark DataFrame containing only the plotting values for \code{euribor3m}. Then, we just need to create a Cartesian product with the original training data (excluding the variable \code{euribor3m}), or representative sample thereof. This is accomplished in the next code chunk.

A word of caution is in order. Even though Spark is designed to work with large data sets in a distributed fashion, Cartesian products can still be costly! Hence, if you're learning sample is quite large (e.g., in the millions), which is probably the case if you're using MLlib, then keep in mind that you don't necessarily need to utilize the entire training sample for computing partial dependence and the like. If you have 50 million training records, for example, then consider only using a small fraction, say, 10,000, for constructing feature effect plots. 

<<rf-ex-bank-grid, eval=FALSE>>=
euribor3m.grid <- as.DataFrame(unique(  # DataFrame of unique quantiles
  approxQuantile(bank.trn.sdf, cols = "euribor3m", 
                 probabilities = 1:29 / 30, relativeError = 0)
))
names(euribor3m.grid) <- "euribor3m"

# Training data without euribor3m
trn.wo.euribor3m <- bank.trn.sdf  # copy of training data
trn.wo.euribor3m$euribor3m <- NULL  # remove euribor3m

# Create a Cartesian product
pd <- crossJoin(euribor3m.grid, trn.wo.euribor3m)
dim(pd)  # nrow(euribor3m.grid) * nrow(trn.wo.euribor3m)

#> [1] 514850     20
@

Finally, we can construct the partial dependence values by aggregating the predictions using a simple grouping operator combined with a summary function (for PDPs, we just average the predictions). The results are displayed in Figure~\ref{fig:rf-bank-pd}. Here you can see that the relative frequency of exclamation marks is positively associated with spam (note that the $y$-axis is on the probability scale).

<<rf-ex-bank-pdp, eval=FALSE>>=
ggplot(pd, aes(x = euribor3m, y = yhat)) + 
  geom_line() +
  geom_rug(data = as.data.frame(euribor3m.grid), 
           aes(x = euribor3m), inherit.aes = FALSE) +
  xlab("Euribor 3 month rate") +
  ylab("Partial dependence") +
  theme_bw()

sparkR.stop()  # stop the Spark session
@

<<rf-ex-bank-pdp-hide, echo=FALSE, fig.cap="Partial dependence of subscription probability on euribor 3 month rate from the bank marketing probability forest.">>=
ggplot(pd, aes(x = euribor3m, y = yhat)) + 
  geom_line() +
  geom_rug(data = as.data.frame(euribor3m.grid), 
           aes(x = euribor3m), inherit.aes = FALSE) +
  xlab("Euribor 3 month rate") +
  ylab("Partial dependence") +
  theme_bw()
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Final thoughts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Yikes, that was a long chapter, but necessarily so. While RFs were originally introduced in \citet{breiman-2001-rf}, many of the ideas have been seen before. For example, the term "random forest" was actually coined by \citet{ho-1995-rf}, who used "the random subspace" method to combine trees grown in random subspaces of the original features. \citet{breiman-2001-rf} references several other attempts to further improve bagging my introducing more diversity among the trees in a "forest".

Leo Breiman was a phenomenal statistician (and theoretical probabilist) who's had a profound impact on the field of SML. If you're interested in more of his work, especially on the development of RF, and the many collaborations it involved, see \citet{cutler-2010-remembering}. Adele Cutler, a close collaborator with Breiman on RFs, still maintains there original RF website at \url{https://www.stat.berkeley.edu/~breiman/RandomForests/}. This website is still one of the best references to understanding Breiman's original RF, and includes links to several relevant papers and the original Fortran source code. % Unfortunately, Leo Breiman passed away before completely finishing the regression and survival analysis aspects of his original RF.

To end this chapter, I'll leave you with a quote listed under the philosophy section of Breiman and Cutler's RF website, which applies more generally than just to RFs:

\begin{VF}
Random forest is an example of a tool that is useful in doing analyses of scientific data. But the cleverest algorithms are no substitute for human intelligence and knowledge of the data in the problem. Take the output of random forests not as absolute truth, but as smart computer generated guesses that may be helpful in leading to a deeper understanding of the problem.

\VA{Leo Breiman and Adele Cutler}{\url{http://bit.ly/2VXFHYP}}

\end{VF}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Exercises}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \begin{ExerciseList}
% 
%   \Exercise \label{ex:rf-bagging} Discuss how you could implement bagged decision trees (i.e., no subsampling of the features before each split) using random forest software.
%   
%   \Exercise Using a random forest implementation of your choosing, along with your answer from Exercise~\ref{ex:rf-bagging}, compare the results of bagging with a (default) random forest on the XYZ data set using $B = 1000$ trees. Does a random forest appear to do better in terms of reduction to OOB RMSE? Re-run the analysis several times and comment on the difference in computation time.
%   
%   \Exercise \label{ex:rf-subsampling} Modify the \code{crforest()} function from Section~\ref{sec:rf-conditional-rf} to include the option to subsample without replacement (be sure to include an argument to control the sample size of each subsample). Re-run the Ames housing example using subsamples of size $N / 2$ without replacement. How do the results compare in terms of accuracy? How do the results compare in terms of computation time?
%   
% <<rf-ex-subsample, echo=FALSE, eval=FALSE>>==
% crforest <- function(X, y, mtry = NULL, B = 5, oob = TRUE, replace = TRUE, sample.frac = 0.5) {
%   min.node.size <- if (is.factor(y)) 1 else 5
%   N <- nrow(X)  # number of observations
%   p <- ncol(X)  # number of features          
%   train <- cbind(X, "y" = y)  # training data frame
%   fo <- as.formula(paste("y ~ ", paste(names(X), collapse = "+")))
%   if (is.null(mtry)) {  # use default definition
%     mtry <- if (is.factor(y)) sqrt(p) else p / 3
%     mtry <- floor(mtry)  # round down to nearest integer
%   }
%   # CTree parameters; basically force the tree to have maximum depth
%   ctrl <- party::ctree_control(mtry = mtry, minbucket = min.node.size,
%                                minsplit = 10, mincriterion = 0)
%   forest <- vector("list", length = B)  # to store each tree
%   for (b in 1:B) {  # fit trees to bootstrap samples
%     samp <- if (isTRUE(replace)) {
%       sample(1:N, size = N, replace = TRUE)
%     } else {
%       sample(1:N, size = floor(sample.frac * N), replace = FALSE)
%     }
%     forest[[b]] <- party::ctree(fo, data = train[samp, ], control = ctrl)
%     if (isTRUE(oob)) {  # store row indices for OOB data?
%       attr(forest[[b]], which = "oob") <- setdiff(1:N, unique(samp))
%     }
%   }
%   forest  # return the forest object 
% }
% 
% # Construct forests
% set.seed(1408)  # for reproducibility
% rfo.sub <- crforest(X, y = ames.trn$Sale_Price, B = 300, replace = FALSE)  # default RF
% 
% rmse.sub <- computeRMSEs(rfo.sub, newdata = ames.tst)
% plot(rmse.sub / 1000, 
%      type = "l", las = 1, xlab = "Number of trees", 
%      ylab = "RMSE / 1000")
% @
%   
%   \Exercise Write a \code{predict()} method for the classification case of \code{confRF()}. Use it to obtain the misclassification error on the email spam test data.
%   
%   \Exercise Update the \code{crforest()} function to fit trees in parallel, rather than sequentially. While there are numerous solutions to parallelizing \code{for} loops in \R{}, you may find that the \pkg{foreach} package \citep{R-foreach}, along with \pkg{doParallel} \citep{R-doParallel} (or another supported parallel backend), to be the easiest solution. Compare the computation time of your updated version with the original function. Is it faster? How many cores did you use?
%   
%   \Exercise Compare the decision boundaries from a random forest, rotation forest, and random rotation forest on the Palmer Penguins example from Section~\ref{sec:guide-penguins}. \FIXME{TBD.}
%   
%   \Exercise \label{ex:qrf} \FIXME{Exercise about quantile regression forest by hand. Ask them to use already fitted regression forest (perhaps have them do Ames housing first). Point them to source code for randomForest/ranger for hints.}
%   
%   \Exercise \label{ex:rf-guide} \FIXME{Add exercise about using GUIDE RFs and maybe compare to example from chapter?}
%   
%   \Exercise Redo the credit card fraud example using different random seeds for both the train/test split and the isolation forest. Do the result seem consistent with what I did in the previous section?
% 
% \end{ExerciseList}
